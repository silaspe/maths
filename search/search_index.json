{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Silas Maths","text":"<p>All derivations here are painstakingly authored by the \\(11\\) yo Silas Pembroke, with a little help from his Dad. He was- I'm all ready tired of talking in the \\(3\\)rd person. I am a boy who loves mathematics and would like to share my math with the world.</p> <ul> <li>Calculus - derivative formulas from first principles: sum rule, chain rule, product rule, multiplication by constant, exponent rule, introduction to \\(e\\), logarithmic derivative, power rule</li> <li>Polynomial - solving for the roots of the quartic equation</li> <li>Calculus II - derivations of \\(e\\) and \\(e^x\\) from limits, a derivation of the quotient rule</li> <li>Complex - introduction to complex numbers and their relationship to sine and cosine using geogebra</li> <li>Jacobian - on the relation between complex numbers and linear algebra</li> <li>Gamma - explores the connections between factorials, calculus, and the Gamma function, extending factorials to non-integers</li> <li>Trigonometry - Desmos visualization of angle addition</li> <li>Harmonic - On the alternating and non-alternating harmonic seireis. I didn't want to remove the old description made by ChatGPT, even if someone might think \"more description equals longer page\" which is why I replaced it. You can read the old description below:</li> </ul> <p>The alternating harmonic series <code>1 - 1/2 + 1/3 - 1/4 + 1/5 - ...</code> is restructured by rearranging terms, leading to a new series that converges to a known value. By defining a function <code>f(n)</code> that represents the sum of a sequence of fractions, and then extending this to <code>f(\u221e)</code>, it's shown that this sum converges to the natural logarithm of 2 (<code>ln(2)</code>). Therefore, the infinite alternating harmonic series sums up to <code>ln(2)</code>. This conclusion is reached through a creative manipulation of series and leveraging properties of logarithms.</p> <ul> <li>Eigenstuff - the applications and derivations of eigenvectors and eigenvalues</li> <li>Binomial - a derivation of the binomial theorem, an introduction to discrete calculus, and a method for computing pi (not pie)</li> <li>Fractional calculus - an introduction to fractional calculus using cauchy's (other) integral formula</li> <li>Fibonacci - using the golden ratio phi and the less popular psi to derive binet's formula among other things</li> <li>Probability - a coin flip game leads you into a new job as a detective to test if someone is cheating or not (I haven't added anything to it in a while)</li> <li>Complex II - a derivation of the roots of unity, \"I'll write this down when I put something in here and surround this text in quotation marks\"</li> <li>The strand puzzle - a popular puzzle from the strand magazine a hundred years ago that ramanujan solved \"straight away\", whatever that means</li> <li>The infamous \"arctan puzzle\" - the other puzzle that I was working on in the vacation</li> <li>Modular arithmetic - if you restart counting after \\(10\\), \\(5\\), or \\(7\\)</li> <li>Brainstorming a new page - pretty self explanatory</li> <li>Geometric algebra - (secretly clifford algebra) Yes, you can multiply two vectors.</li> </ul> <p>PS you don't get a vector</p> <ul> <li>Set theory - definitions of the subset, empty set, power set, and so on.</li> <li>Code repo - Pretty self explanatory. Also, the idea for both this page and the one above it did not originate in the brainstorm page.</li> <li>My way to count - why YOU should use binary</li> <li>Geometric algebra 2 - the first part (this page will not make sense without it) crashed</li> <li>Lambda calculus - an exploration of lambda calculus, the smallest programing language where \"functional programing language\" is an understatement (based off of this and this)</li> <li>Thoughts - Contains non-math-related things that I still wanted to tell.</li> <li>Summer of Math Exposition 4 - my submission to SoME4 (he didn't have a video for SoME \\(2\\) through \\(4\\))</li> <li>Set theory: logic edition - the set theory page, but derived from the ground up using logic and extensions.</li> <li>Projective geometry - an introduction to projective geometry where parallel lines cross, and there are two points that lie on every circle.</li> <li>Lambda calculus revisited - (Lcr) I gave up on this one, but then I came back! It is still heavily based off of this, this, and even more heavily based off of this (Warning: the videos are ~ \\(1 \\text{H}\\) long, and the paper has (exactly) \\(120\\) pages).</li> <li>Linear algebra - A course about the more general mathematician's version of linear algebra, the study of vectors and matrices (like the fake AI generated simulated reality). But if you want some more intuition about how it works, each chapter will have a corresponding part in this playlist</li> <li>Group theory - A page about group theory, an attempt at a grand unified theory of mathematics. I'll be covering things like dihedral groups, symmetric groups, subgroups, cosets, isomorphism, and maybe even the \\(196,883\\) dimensional monster.</li> </ul>"},{"location":"actual_repo.html","title":"Code repo","text":"<p>This page is not on the website (yet), because it is entirely pseudo-code and python code.</p>"},{"location":"actual_repo.html#asterisk-operator-pseudo","title":"asterisk operator (pseudo)","text":"<pre><code>define Asterisk(x,y,n)\n  (comment) if n is 1, it is adding x and y\n  (comment) if n is 2, it is multiplying x and y\n  (comment) if n is 3, it is it is taking x to the power of y\n  (comment) and so on.\n\n  if n = 1\n    return x + y\n\n  (comment) because I said that earlier.\n\n  else if y = 1\n    return x\n\n  (coment) x times 1 = x, x to the power of 1 = x, ect\n\n  else \n    return Asterisk(x,Asterisk(x,y - 1,n),n - 1)\n\n  (comment) this makes sense if you think about it.\n</code></pre>"},{"location":"actual_repo.html#asterisk-operator-python","title":"asterisk operator (python)","text":"<pre><code>def Asterisk(a, b, n):\n  if n == 1:\n    return a + b\n  elif n == 2:\n    return a*b\n  elif b == 1:\n    return a\n  else:\n    return Asterisk(a,Asterisk(a,b - 1,n),n - 1)\n</code></pre>"},{"location":"actual_repo.html#alternitave-asterisk-operator-python","title":"alternitave asterisk operator (python)","text":"<pre><code>def (a, b, n):\n  if n == 1:\n    return a + b\n  if n == 2:\n    return a*b\n  k = a\n  for i in range(b - 1):\n    k = Asterisk(a, k, n - 1)\n</code></pre>"},{"location":"actual_repo.html#the-exact-digits-of-square-roots-pseudo","title":"the exact digits of square roots (pseudo)","text":"<p>404 Page not found.</p>"},{"location":"actual_repo.html#the-exact-digits-of-square-roots-python-i-have-been-working-on-this-since-this-page-was-made","title":"the exact digits of square roots (python) (I have been working on this since this page was made)","text":"<pre><code>def Root(x, y, n):\n  f = 0\n  while f**x &lt;= y:\n    f += 1\n  f -= 1\n  a = f\n  l = 0\n  for i in range(n):\n    a *= 10\n    l += 1\n    while a**x &lt;= y*(10**(l*x)):\n      a += 1\n    a -= 1\n  a -= f*(10**l)\n  if n &gt; 1:\n    return str(f) + '.' + str(a)\n  else:\n    return str(f)\n</code></pre>"},{"location":"actual_repo.html#the-exact-digits-of-logarithms-pseudo","title":"the exact digits of logarithms (pseudo)","text":"<pre><code>404 Page not found.\n</code></pre>"},{"location":"actual_repo.html#the-exact-digits-of-logarithms-python","title":"the exact digits of logarithms (python)","text":"<pre><code>def Log(x, y, n):\n  f = 0\n  while x**f &lt;= y:\n    f += 1\n  f -= 1\n  a = f\n  l = 0\n  for i in range(n):\n    a *= 10\n    l += 1\n    while x**a &lt;= y**(10**(l)):\n      a += 1\n    a -= 1\n  a -= f*(10**l)\n  if n &gt; 1:\n    return str(f) + '.' + str(a)\n  else:\n    return str(f)\n</code></pre>"},{"location":"actual_repo.html#as-a-cherry-on-top-the-exact-digits-of-the-roots-of-polynomials-pseudo","title":"as a cherry on top, the exact digits of the roots of polynomials (pseudo)","text":"<pre><code>def Roots(p(x), StartValue, n):\n  f = StartValue\n  while p(f) &lt;= y:\n    f += 1\n  f -= 1\n  a = f\n  l = 0\n  for i in range(n):\n    a *= 10\n    l += 1\n    while p(a*(10**(-l) &lt;= p(y):\n(coment) you will have to expand and simplify the equation so that it is a differance of integers\n      a += 1\n    a -= 1\n  a -= f*(10**l)\n  if n &gt; 1:\n    return str(f) + '.' + str(a)\n  else:\n    return str(f)\n</code></pre>"},{"location":"actual_repo.html#set-theory-numbers","title":"set theory numbers","text":"<pre><code>def SetTheoryNumbers(n):\n  string = \"{\"\n  k = n - 1\n  while k &gt;= 0:\n    string += SetTheoryNumbers(k)\n    k -= 1\n  string += \"}\"\n  return string\n</code></pre>"},{"location":"actual_repo.html#alternitave-set-theory-numbers","title":"alternitave set theory numbers","text":"<pre><code>def AlmostSetTheoryNumbers(n):\n  if n == 0:\n    return \"\"\n  return SetTheoryNumbers(n - 1) + AlmostSetTheoryNumbers(n - 1)\n</code></pre> <pre><code>def SetTheoryNumbers(n):\n  return \"{\" + AlmostSetTheoryNumbers(n) + \"}\"\n</code></pre>"},{"location":"actual_repo.html#base-names","title":"base names","text":"<pre><code>def SmallestPrimeDivisor(n):\n  k = 2\n  while n % k != 0:\n    k += 1\n  return k\n</code></pre> <pre><code>def AlmostBase(n):\n  if n == 2:\n    return \"bi\"\n  if SmallestPrimeDivisor(n) == n:\n    return \"un\" + AlmostBase(n - 1) + \"sna\"\n  string = \"\"\n  k = n\n  while k != 1:\n    spdk = SmallestPrimeDivisor(k)\n    string += AlmostBase(spdk)\n    k /= spdk\n  return string\n</code></pre> <pre><code>def base(n):\n  if n &lt; 2:\n    return \"n must be a positave integer!\"\n  return AlmostBase(n) + \"nary\"\n</code></pre>"},{"location":"actual_repo.html#base-names-first-n-values","title":"base names (first \\(n\\) values)","text":"<pre><code>def SmallestPrimeDivisor(n):\n  k = 2\n  while n % k != 0:\n    k += 1\n  return k\n</code></pre> <pre><code>for n in range(101):\n  L = [None, None]\n  if n == 2:\n    L += [\"bi\"]\n    print(\"binary\")\n  if SmallestPrimeDivisor(n) == n:\n    L.append(\"un\" + L[n - 1] + \"sna\")\n    print(\"un\" + L[n - 1] + \"snanary\")\n  string = \"\"\n  k = n\n  while k != 1:\n    spdk = SmallestPrimeDivisor(k)\n    string += L[spdk]\n    k /= spdk\n  L.append(string)\n  print(string + \"nary\")\n</code></pre>"},{"location":"actual_repo.html#the-exact-digits-of-reciprocals-in-any-base-using-my-home-made-method","title":"the exact digits of reciprocals (in any base) (using my home made method)","text":"<pre><code>def frac(n, b):\n  if n == 1:\n    return \"this code can not compute one over one!\"\n  if b == 1:\n    return \"the base must not equal one!\"\n  if n &lt; b:\n    nb = n\n  else:\n    nb = b\n  for K in range(nb - 1):\n    k = K + 2\n    if n % k == 0:\n      if b % k == 0:\n        return \"this code will not work if you give it two numbers that are coprime\"\n  rem = b % n\n  string = \"0. repeating \" + str(b // n)\n  for i in range(n - 2):\n    rem *= b\n    string += \" \" + str(rem // n)\n    rem %= n\n    if rem == 1:\n      return string\n</code></pre> <p>also, this is the \\(400\\)'th commit to this branch.</p>"},{"location":"actual_repo.html#crash-do-not-run-this-code","title":"crash (DO NOT RUN THIS CODE)","text":"<pre><code>def crash(exponent):\n  result = \"1\"\n  for i in range(exponent):\n    result += result\n  return result\n</code></pre> <pre><code>print(crash(crash(crash(crash(crash(crash(crash(crash(crash(crash(10000)))))))))))\n</code></pre>"},{"location":"actual_repo.html#crash-2-it-goes-without-saying-at-this-point","title":"crash #2 (it goes without saying at this point)","text":"<pre><code>result = \"1\"\nwhile true:\n  result += result\n  print result\n</code></pre>"},{"location":"actual_repo.html#group-theory-ive-kinda-been-doing-a-lot-of-group-theory-today-and-yesterday","title":"group theory (I've kinda been doing a lot of group theory today and yesterday)","text":"<pre><code>def GroupTheory(TheNumberOfFaces, TransformationNumberOne, TransformationNumberOnesIndex, TransformationNumberTwo, TransformationNumberTwosIndex):\n  if (TheNumberOfFaces &lt; 2) or (TransformationNumberOne != \"f\" and TransformationNumberOne != \"r\") or (TransformationNumberOnesIndex &lt; 0 or TransformationNumberOnesIndex &gt;= TheNumberOfFaces) or (TransformationNumberTwo != \"f\" and TransformationNumberTwo != \"r\") or (TransformationNumberTwosIndex &lt; 0 or TransformationNumberTwosIndex &gt;= TheNumberOfFaces):\n    return \"that isn't group theory!\"\n  if TransformationNumberOne == \"f\":\n    if TransformationNumberTwo == \"f\":\n      return \"r\" + str((TransformationNumberTwosIndex - TransformationNumberOnesIndex) % TheNumberOfFaces)\n    return \"f\" + str((TransformationNumberOnesIndex - TransformationNumberTwosIndex) % TheNumberOfFaces)\n  if TransformationNumberTwo == \"f\":\n    return \"f\" + str((TransformationNumberTwosIndex - TransformationNumberOnesIndex) % TheNumberOfFaces)\n  return \"r\" + str((TransformationNumberOnesIndex + TransformationNumberTwosIndex) % TheNumberOfFaces)\n</code></pre>"},{"location":"actual_repo.html#binary","title":"binary","text":"<pre><code>def replace(string, n):\n  result = \"\"\n  for i in range(n):\n    result += string[i]\n  if string[n] == \"0\":\n    result += \"1\"\n  else:\n    result += \"0\"\n  for i in range(n + 1, len(string)):\n    result += string[i]\n  return result\n</code></pre> <pre><code>def BinarySuccessor(string):\n  for i in range(len(string)):\n    if string[i] != \"0\" and string[i] != \"1\":\n      return \"this code will not work if you don't give it a binary string!\"\n  k = 0\n  result = string\n  while result[k] == \"1\":\n    result = replace(result, k)\n    k += 1\n  result = replace(result, k)\n  return result\n</code></pre> <pre><code>def binaryfy(n):\n  result = \"0\"\n  for i in range(n):\n    result = BinarySuccessor(result)\n  return result\n</code></pre>"},{"location":"actual_repo.html#binary-names-i-have-been-procrastinating-on-this-for-a-month","title":"binary names (I have been procrastinating on this for a month!)","text":"<pre><code>def IsZero(string):\n  for i in range(len(string)):\n    if string[i] != \"0\":\n      return \"false\"\n  return \"true\"\n</code></pre> <pre><code>def IsOne(string):\n  for i in range(len(string) - 1):\n    if string[i] != \"0\":\n      return \"false\"\n  if string[len(string)] != \"1\":\n    return \"false\"\n  return \"true\"\n</code></pre> <pre><code>def BinaryNames(n):\n  if n &gt;= 65536\n    return \"false\"\n  string = binaryfy(n)\n  if IsZero(string) == \"true\":\n    return \"\"\n  if IsOne(string) == \"true\":\n    return \"one \"\n  split = 1\n  length = len(string)\n  while split &lt; length:\n    split *= 2\n  split /= 2\n  split = length - split\n  string_one = \"\"\n  for i in range(split):\n    string_one += string[i]\n  string_two = \"\"\n  for i in range(split + 1, length):\n    string_two += string[i]\n  if IsZero(string_one) == \"true\":\n    return string_two\n  if length - split == 1\n    strpow = \"two \"\n  if length - split == 2\n    strpow = \"four \"\n  if length - split == 4\n    strpow = \"hex \"\n  if length - split == 8\n    strpow = \"byte \"\n  if IsOne(string_one) == \"true\":\n    return strpow + string_two\n  return string_one + strpow + string_two\n</code></pre>"},{"location":"actual_repo.html#alternitave-numbers","title":"alternitave numbers","text":"<pre><code>def SmallestPrimeDivisor(n):\n  k = 2\n  while n % k != 0:\n    k += 1\n  return k\n</code></pre> <pre><code>def AlmostNumber(n, threshold):\n  if n &lt;= threshold and n &gt; 0:\n    isPrime = \"false\"\n    return str(n)\n  if SmallestPrimeDivisor(n) == n:\n    isPrime = \"true\"\n    return \"((\" + AlmostNumber(n - 1) + \")+1)\"\n  else:\n    isPrime = \"false\"\n  string = \"\"\n  k = n\n  while k != 1:\n    spdk = SmallestPrimeDivisor(k)\n    string += AlmostBase(spdk)\n    k /= spdk\n  return string\n</code></pre> <pre><code>def number(n, threshold):\n  if isPrime == \"false\":\n    return AlmostNumber(n, threshold)\n  result = \"\"\n  for i in range(1, len(AlmostNumber(n, threshold)) - 1):\n    result += AlmostNumber(n, threshold)[i]\n  return result\n</code></pre>"},{"location":"actual_repo.html#magic-sequences","title":"magic sequences","text":"<p>magic sequences</p> <pre><code>def MagiSeq(n):\n  MagicSequence = [1]\n  MagicNext = 1\n  for i in range(n - 1):\n    MagicNext *= 2\n    MagicNext %= n\n    MagicSequence += [MagicNext]\n    if MagicNext in MagicSequence[:-1]:\n      return MagicSequence\n</code></pre>"},{"location":"actual_repo.html#the-apocalyptic-numbers","title":"The Apocalyptic Numbers","text":"<p>The Apocalyptic Numbers</p> <pre><code>for i in range(1000):\n  power = str(2 ** i)\n  for j in range(len(power)):\n    if power[j] == \"6\":\n      if len(power) &gt;= j + 2:\n        if power[j + 1] == \"6\":\n          if power[j + 2] == \"6\":\n            if power[j + 3] != \"6\":\n              result = \"\"\n              for k in range(j):\n                result += power[k]\n              result += \"  666  \"\n              for k in range(j + 3, len(power)):\n                result += power[k]\n              if i != 157:\n                print()\n              print(f\"{i}, 2^{i} = {result}\")\n</code></pre>"},{"location":"actual_repo.html#nsuemtb-etrh-etohreyory","title":"nsuemtb etrh etohreyo#r#y","text":"<pre><code>def SmallestPrimeDivisor(n):\n  k = 2\n  while n % k != 0:\n    k += 1\n  return k\n</code></pre> <pre><code>N = 100\n</code></pre> <pre><code>for n in range(N):\n  L = [\"\", \"{}\"]\n  if SmallestPrimeDivisor(n) == n:\n    L.append(\"{\" + L[n - 1] + \"}\")\n    print(\"{\" + L[n - 1] + \"}\")\n  string = \"{\"\n  k = n\n  while k != 1:\n    spdk = SmallestPrimeDivisor(k)\n    string += L[spdk]\n    k /= spdk\n  L.append(string + \"}\")\n  print(string + \"}\")\n</code></pre>"},{"location":"actual_repo.html#big-numbers-and-transfinite-ordinals","title":"Big numbers and Transfinite ordinals","text":""},{"location":"actual_repo.html#fast-growing-hierachies","title":"fast-growing hierachies","text":"<pre><code>def f(x, n):\n  if n == 0:\n    return x + 1\n  if n == 1:\n    return 2 * x\n  if n == 2:\n    return x * 2 ** x\n  k = x\n  for i in range(x):\n    k = f(n - 1, k)\n  return k\n</code></pre>"},{"location":"actual_repo.html#omega","title":"omega","text":"<pre><code>def omega(x):\n  return f(x, x)\n</code></pre>"},{"location":"actual_repo.html#epsilon","title":"epsilon","text":"<pre><code>def epsilon_subscript(x, n):\n  if n == 0:\n    result = x\n    for i in range(x - 1):\n      result = x ** result\n    return result\n  result = epsilon_subscript(x, n - 1)\n  for i in range(x - 1):\n    result = epsilon_subscript(x, n - 1) ** result\n  return result\n</code></pre> <pre><code>def epsilon(x, n):\n  return f(x, epsilon_subscript(x, n))\n</code></pre>"},{"location":"actual_repo.html#phi","title":"phi","text":"<pre><code>def veblen_subscript(n, x, y):\n  if n == 0:\n    return x ** y\n  if n == 1:\n    return epsilon_subscript(x, y)\n  if y == 0:\n    k = 0\n    for i in range(x):\n      k = veblen_subscript(n - 1, x, k)\n    return k\n  k = veblen_subscript(n, x, y - 1)\n  m = k\n  for i in range(x - 1):\n    k = m ** k\n  for j in range(1, n):\n    for l in range(x - 1):\n      k = veblen_subscript(j, x, k)\n  return k\n</code></pre> <pre><code>def veblen(n, x, y):\n  return f(x, veblen_subscript(n, x, y))\n</code></pre>"},{"location":"actual_repo.html#gamma","title":"gamma","text":"<pre><code>def gamma(x):\n  k = 0\n  for i in range(x):\n    k = veblen_subscript(k, x, 0)\n  return k\n</code></pre>"},{"location":"actual_repo.html#essentially-crash-3-it-goes-without-saying-so-im-not-going-to","title":"essentially crash #3 (it goes without saying, so I'm not going to)","text":"<pre><code>x = f(x, gamma(x) + 1729)\n</code></pre>"},{"location":"actual_repo.html#computer-within-a-computer","title":"computer within a computer","text":"<pre><code>def AND(b1, b2):\n  if b1 == 0:\n    return 0\n  return b2\n</code></pre> <pre><code>def OR(b1, b2):\n  if b1 == 1:\n    return 1\n  return b2\n</code></pre> <pre><code>def NOT(b1):\n  if b1 == 0:\n    return 1\n  return 0\n</code></pre> <pre><code>def bnot(b1, b2):\n  return AND(b1, NOT(b2))\n</code></pre> <pre><code>def XOR(b1, b2):\n  return OR(bnot(b1, b2), bnot(b2, b1))\n</code></pre> <pre><code>def half(b1, b2):\n  return AND(b1, b2),XOR(b1, b2)\n</code></pre> <pre><code>def full(b1, b2, c):\n  i1 = half(b1, b2)\n  i2 = half(i1[1], c)\n  return OR(i1[0], i2[0]),i2[1]\n</code></pre> <pre><code>def cadd(B1, B2, c):\n  i7 = full(B1[7], B2[7], c)\n  i6 = full(B1[6], B2[6], i7[0])\n  i5 = full(B1[5], B2[5], i6[0])\n  i4 = full(B1[4], B2[4], i5[0])\n  i3 = full(B1[3], B2[3], i4[0])\n  i2 = full(B1[2], B2[2], i3[0])\n  i1 = full(B1[1], B2[1], i2[0])\n  i0 = full(B1[0], B2[0], i1[0])\n  return i0[1],i1[1],i2[1],i3[1],i4[1],i5[1],i6[1],i7[1]\n</code></pre> <pre><code>def add(B1, B2):\n  return cadd((B1, B2, 0))\n</code></pre> <pre><code>def sub(B1, B2):\n  return cadd(B1, (NOT(B2[0]),NOT(B2[1]),NOT(B2[2]),NOT(B2[3]),NOT(B2[4]),NOT(B2[5]),NOT(B2[6]),NOT(B2[7])), 1)\n</code></pre> <pre><code>def mini_MUX(b1, b2, b3):\n  return OR(bnot(b1, b3), AND(b2, b3))\n</code></pre> <pre><code>def MUX(B1, B2, b1):\n  return mini_MUX(B1[0], B2[0], b3),mini_MUX(B1[1], B2[1], b3),mini_MUX(B1[2], B2[2], b3),mini_MUX(B1[3], B2[3], b3),mini_MUX(B1[4], B2[4], b3),mini_MUX(B1[5], B2[5], b3),mini_MUX(B1[6], B2[6], b3),mini_MUX(B1[7], B2[7], b3)\n</code></pre> <pre><code>def AU(B1, B2, b1):\n  return MUX(add(B1, B2), sub(B1, B2), b1)\n</code></pre> <pre><code>def LU(B1, B2, b1):\n  i1 = AU(B1, B2, b1)\n  i2 = NOT(OR(OR(OR(i1[0], i1[1]), OR(i1[2], i1[3])), OR(OR(i1[4], i1[5]), OR(i1[6], i1[7]))))\n  return i1[0],OR(i1[0], i2),i2\n</code></pre> <pre><code>import numpy as np\n</code></pre> \\[ \\text{Here's some code that my dad (and ChatGPT) wrote:} \\] <pre><code>class BitArray(np.ndarray):\n  def __new__(cls, shape):\n    # Create an array of the given shape, initialized to False (0)\n    obj = np.zeros(shape, dtype=np.bool_).view(cls)\n    return obj\n\n  def __array_finalize__(self, obj):\n    if obj is None: return\n\n  def __repr__(self):\n    # Convert the boolean array to an integer array for display\n    int_array = self.astype(int)\n    # Use numpy's array2string function to format the output\n    try:\n      return '\\n'.join(f\"{i:3d}  \"  + ' '.join([str(v) for v in val]) for i, val in enumerate(int_array))\n    except:\n      return ' '.join([str(v) for v in int_array])\n    #return np.array2string(int_array, separator=' ')\n\n  def __str__(self):\n    return self.__repr__()\n</code></pre> <p>\\(700+\\) (So, 702) lines, this is the new longest page on the website.</p> \\[ \\text{Back to my code.} \\] <pre><code>code_lines = BitArray((256, 20))\n</code></pre> <pre><code>RAM = BitArray((256, 8))\n</code></pre> <pre><code>def numfy(B1):\n  return B1[0] * 128 + B1[1] * 64 + B1[2] * 32 + B1[3] * 16 + B1[4] * 8 + B1[5] * 4 + B1[6] * 2 + B1[7] * 1\n</code></pre> <p>So, how you use the code is this (entire next line):</p> <p>if you want to input some data (which has to be a number (which has to be encoded as the binary equivilant with a comma between every bit)) (call it \\(D\\)) at some address (call it \\(a\\)), run the code <code>RAM[numfy(a)] = D</code>. However, if you want to run code, then store the corrasponding opcode (that you can get from the table below) \\(O\\) with corrasponding numbers that go with the opcode (e.g. ssu requires an address to store the subtraction) \\(n1\\) and \\(n2\\) into line \\(n\\) with <code>code_lines[n] = O + n1 + n2</code> (that's an o, not a zero).</p> Instruction Stands for Description Opcode halt halt stops the program 0000 ldi load immediate stores directly into RAM 0001 rtr (optional) RAM to RAM stores from RAM into RAM 0010 lia load immediate a stores directly into register a 0011 lib load immediate b stores directly into register b 0100 ria RAM into a stores from RAM into register a 0101 rib RAM into b stores from RAM into register b 0110 sad store addition stores addition of a and b into RAM 0111 ssu store subtraction stores subtraction of a and b into RAM 1000 jump jump jump to a different line of code 1001 jin jump if negative jump if the negative flag (of the LU) is on 1010 jil jump if low jump if the NOZ (negaitve or zero) flag (of the LU) is on 1011 biz branch (aka jump) if zero branch if the zero flag (of the LU) is on 1100 <pre><code>halt = 0,0,0,0\nldi = 0,0,0,1\nrtr = 0,0,1,0\nlia = 0,0,1,1\nlib = 0,1,0,0\nria = 0,1,0,1\nrib = 0,1,1,0\nsad = 0,1,1,1\nssu = 1,0,0,0\njump = 1,0,0,1\njin = 1,0,1,0\njil = 1,0,1,1\nbiz = 1,1,0,0\n</code></pre> <pre><code>t0 = 0,0,0,0,0,0,0,0\n</code></pre> <pre><code>def yfmun(n):\n  i7 = n % 2\n  i6 = (n % 4) // 2\n  i5 = (n % 8) // 4\n  i4 = (n % 16) // 8\n  i3 = (n % 32) // 16\n  i2 = (n % 64) // 32\n  i1 = (n % 128) // 64\n  i0 = n // 128\n  return i0,i1,i2,i3,i4,i5,i6,i7\n</code></pre> \\[ \\text{Now, this is where the code is run, so, if you want to store a program, or some data, put it here.} \\] <pre><code>instr_addr_reg = t0\nreg_a = t0\nreg_b = t0\nwhile True:\n  program = code_lines[numfy(instr_addr_reg)]\n  opcode = program[0],program[1],program[2],program[3]\n  n1 = program[4],program[5],program[6],program[7],program[8],program[9],program[10],program[11]\n  n2 = program[12],program[13],program[14],program[15],program[16],program[17],program[18],program[19]\n  if opcode == halt:\n    break\n  elif opcode == ldi:\n    RAM[numfy(n2)] = n1\n  elif opcode == rtr:\n    RAM[numfy(n2)] = RAM[numfy(n1)]\n  elif opcode == lia:\n    reg_a = n1\n  elif opcode == lib:\n    reg_b = n1\n  elif opcode == ria:\n    reg_a = RAM[numfy(n1)]\n  elif opcode == rib:\n    reg_b = RAM[numfy(n1)]\n  elif opcode == sad:\n    RAM[numfy(n1)] = add(reg_a, reg_b)\n  elif opcode == ssu:\n    RAM[numfy(n1)] = sub(reg_a, reg_b)\n  elif opcode == jump:\n    instr_addr_reg = n1\n  elif opcode == jin:\n    if LU(reg_a, reg_b, 1)[0] == 1:\n      instr_addr_reg = n1\n  elif opcode == jil:\n    if LU(reg_a, reg_b, 1)[1] == 1:\n      instr_addr_reg = n1\n  elif opcode == biz:\n    if LU(reg_a, reg_b, 1)[2] == 1:\n      instr_addr_reg = n1\n  else:\n    break\n  instr_addr_reg = add(instr_addr_reg, yfmun(1))\n</code></pre> \\[ \\text{here's an example instruction:} \\] <pre><code>code_lines[0] = ldi + yfmun(0) + yfmun(2)\ncode_lines[1] = rtr + yfmun(0) + yfmun(3)\ncode_lines[2] = ria + yfmun(3) + t0\ncode_lines[3] = rib + yfmun(1) + t0\ncode_lines[4] = ssu + yfmun(3) + t0\ncode_lines[5] = ria + yfmun(2) + t0\ncode_lines[6] = lib + yfmun(1) + t0\ncode_lines[7] = sad + yfmun(2) + t0\ncode_lines[8] = ria + yfmun(3) + t0\ncode_lines[9] = lib + yfmun(0) + t0\ncode_lines[10] = jin + yfmun(12) + t0\ncode_lines[11] = jump + yfmun(2) + t0\ncode_lines[12] = ria + yfmun(3) + t0\ncode_lines[13] = rib + yfmun(1) + t0\ncode_lines[14] = sad + yfmun(3) + t0\ncode_lines[15] = ria + yfmun(2) + t0\ncode_lines[16] = lib + yfmun(1) + t0\ncode_lines[17] = ssu + yfmun(2) + t0\ncode_lines[18] = halt + t0 + t0\n</code></pre>"},{"location":"actual_repo.html#turing-machine","title":"Turing machine","text":"<pre><code>import numpy as np\n</code></pre> <pre><code>tape = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n</code></pre> <pre><code>h = 0\n</code></pre> <pre><code>states = np.array([[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]],[[\"H\"],[\"H\"]]]\n</code></pre> <pre><code>state = 0\n</code></pre> <pre><code>while True:\n  instructions = states[state][tape[h]]\n  for i in range(len(instructions)):\n    instruction = instructions[i]\n    if instruction == \"H\":\n      break\n    elif instruction[0] == \"t\":\n      h = int(instruction[2])\n    elif instruction[0] == \"s\":\n      state = int(instruction[2])\n    elif instruction[0] == \"w\":\n      tape = int(instruction[2])\n    else:\n      print(f\"error: {instruction} is not defined.\")\n      break\n</code></pre>"},{"location":"actual_repo.html#all-the-groups-run-at-your-own-computers-risk","title":"all the groups (run at your own (computer's) risk)","text":"<pre><code>import itertools\n</code></pre> <pre><code>import numpy as np\n</code></pre> <pre><code>def permutation(n):\n  return [list(_) for _ in itertools.permutations(np.arange(n))]\n</code></pre> <pre><code>from itertools import chain, combinations\n</code></pre> <pre><code>def powerlist(L):\n  \"\"\"generate power set from iterable input\"\"\"\n  (list(subset) for subset in chain.from_iterable(combinations(L, r) for r in range(len(L) + 1)))\n</code></pre> <pre><code>def mult(p, L):\n  return L[p]\n</code></pre> <p>Do not run this next code that my dad wrote for an input more than \\(4\\).</p> <pre><code>for _ in powerlist(permutation(4)):\n  if len(_) == 0:\n    continue\n  b = True\n  for i in _:\n    for j in _:\n      k = list(mult(np.array(i), np.array(j)))\n      if not k in _:\n        b = False\n#         print(f'found {k} not in {_}')\n        break\n    if not b:\n      break\n  if b:\n    print(f'{_}')\n    print()\n</code></pre>"},{"location":"actual_repo.html#project-mu","title":"project mu","text":"<p>Mine is project mu (\\(\\mu\\)), my dad's is project nu (\\(\\nu\\)) (he's been working on is pretty much all weekend), and together, it is project kappa (\\(\\kappa\\)), a project that should be able to do lambda calculus. I know what you're thinking: why can't you just use the built-in words <code>lambda</code> and <code>:</code>? 'Cause that's boring! (Also, get it? \\(\\kappa \\lambda \\mu \\nu\\)?)</p> <p>Syntax: \\(f(x)\\) is denoted as <code>[f x]</code></p> <pre><code>def reduce(string):\n  for i in range(len(string)):\n    if string[i] == \"[\" and string[i + 1] == \"\u03bb\":\n      bound_varable = string[i + 2]\n      j = i + 4\n      runc = 0\n      s1 = \"\"\n      s2 = \"\"\n      split = False\n      while True:\n        term = string[j]\n        if term == \"[\":\n          runc += 1\n        if term == \"]\":\n          runc -= 1\n          if runc == -1:\n            break\n        if runc == 0 and term == \" \":\n          split = True\n          si = j\n        else:\n          if not split:\n            s1 += term\n          else:\n            s2 += term\n        j += 1\n      result = \"\"\n      for k in range(0, i):\n        result += string[k]\n      for k in range(i + 4, si):\n        if string[k] == bound_varable:\n          result += s2\n        else:\n          result += string[k]\n      for k in range(j + 1, len(string)):\n        result += string[k]\n      return [result, \"needs another!\"]\n  return [string, \"done!\"]\n</code></pre> <pre><code>def fullreduce(string):\n  prev_result = string\n  result = reduce(string)\n  for i in range(100):\n    if result == prev_result:\n       return result\n    prev_result = result\n    result = reduce(result)\n  print(f\"recursion depth limit exceeded, the furthest I got was {result}\")\n  return result\n</code></pre> <p>But there's a problem: capture of free variables. I'll explain with an example:</p> \\[ \\text{[} \\lambda f. \\lambda g. \\lambda x. \\text{[} f \\text{ } \\text{[} g \\text{ } x \\text{]} \\text{]} \\text{ } g \\text{]} = \\lambda g. \\lambda x. \\text{[} g \\text{ } \\text{[} g \\text{ } x \\text{]} \\text{]} \\]"},{"location":"actual_repo.html#project-xi","title":"project xi","text":"<p>Apparently, I just found another method for doing lambda calculus is python, so I subdivided again into project mu (\\(\\mu\\)) and project xi (\\(\\xi\\)).</p> <p>\\(1000\\) Lines, wow.</p> <p>(Get it? \\(\\kappa \\lambda \\mu \\nu \\xi\\)?)</p> <p>The method supports functions as arguments, currying, and even (I think) capture of free variables. I'll explain with some examples:</p> <pre><code>def mult(x):\n  def mult1(y):\n    return x * y\n  return mult1\n</code></pre> <pre><code>mult(1)(2)\n</code></pre> <pre><code>def B(f):\n  def B1(g):\n    def B2(x):\n      return f(g(x))\n    return B2\n  return B1\n</code></pre> <pre><code>def inc(x):\n  return x + 1\n</code></pre> <pre><code>def dec(x):\n  return x - 1\n</code></pre> <pre><code>B(inc)(dec)(5)\n</code></pre>"},{"location":"actual_repo.html#xi","title":"xi","text":"<pre><code>def B(f):\n  def B1(g):\n    def B2(x):\n      return f(g(x))\n    return B2\n  return B1\n</code></pre> <pre><code>comp = B\n</code></pre> <pre><code>def C(f):\n  def C1(x):\n    def C2(y):\n      return f(y)(x)\n    return C2\n  return C1\n</code></pre> <pre><code>swap = C\n</code></pre> <pre><code>def K(x):\n  def K1(y):\n    return x\n  return K1\n</code></pre> <pre><code>fstin = K\n</code></pre> <pre><code>const = K\n</code></pre> <pre><code>def W(f):\n  def W1(x):\n    return f(x)(x)\n  return W1\n</code></pre> <pre><code>I = B(C)(C)\n</code></pre> <pre><code>Th = c(I)\n</code></pre> <pre><code>Ki = K(I)\n</code></pre> <pre><code>sndin = Ki\n</code></pre> <pre><code>T = K\n</code></pre> <pre><code>F = Ki\n</code></pre> <pre><code>xibool = C(Th(\"T / K\"))(\"F / Ki\")\n</code></pre> <pre><code>AND = W(C)\n</code></pre> <pre><code>OR = W(I)\n</code></pre> <pre><code>NOT = C\n</code></pre> <pre><code>beq = B(W)(B(C(B)(C))(C))\n</code></pre> <pre><code>n0 = Ki\n</code></pre> <pre><code>n1 = I\n</code></pre> <pre><code>n2 = W(B)\n</code></pre> <pre><code>n3 = W(B(B)(W(B)))\n</code></pre> <pre><code>n4 = W(B(B)(W(B(B)(W(B)))))\n</code></pre> <pre><code>n5 = W(B(B)(W(B(B)(W(B(B)(W(B)))))))\n</code></pre> <pre><code>n6 = W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B)))))))))\n</code></pre> <pre><code>n7 = W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B)))))))))))\n</code></pre> <pre><code>n8 = W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B)))))))))))))\n</code></pre> <pre><code>n9 = W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B)))))))))))))))\n</code></pre> <pre><code>n10 = W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B(B)(W(B)))))))))))))))))\n</code></pre> <pre><code>xinum = C(Th(lambda x: x + 1))(0)\n</code></pre> <pre><code>inc = B(W)(B(B))\n</code></pre> <pre><code>add = B(B(W))(B(C)(B(B(B))(B(B))))\n</code></pre> <pre><code>mult = B\n</code></pre> <pre><code>exp = Th\n</code></pre> <pre><code>is0 = C(Th(K(F)))(T)\n</code></pre> <pre><code>pair = B(C)(Th)\n</code></pre> <pre><code>fst = Th(K)\n</code></pre> <pre><code>snd = Th(Ki)\n</code></pre> <pre><code>def xipair(p):\n  return f\"({fst(p)}, {snd(p)})\"\n</code></pre> <pre><code>Phi = W(C(B(B)(B(pair)(snd)))(B(inc)(snd))\n</code></pre> <pre><code>dec = B(fst)(C(Th(Phi))(pair(n0)(n0)))\n</code></pre> <pre><code>sub = Th(dec)\n</code></pre> <pre><code>dcomp = B(B)(B)\n</code></pre> <pre><code>leq = dcomp(is0)(sub)\n</code></pre> <pre><code>geq = C(leq)\n</code></pre> <pre><code>gt = dcomp(NOT)(leq)\n</code></pre> <pre><code>lt = C(gt)\n</code></pre> <pre><code>def eq(n):\n  def eq1(k):\n    return AND(leq(n)(k))(geq(n)(k))\n  return eq1\n</code></pre> <pre><code>neq = dcomp(NOT)(eq)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"arctan.html","title":"The infamous \"arctan puzzle\"","text":""},{"location":"arctan.html#first-some-history","title":"First, some history","text":"<p>Before this website was even created, I was on a vacation for family, and I solved at last \\(2\\) puzzles. After that, I said that I solved the strand puzzle and the arctan puzzle. First, I solved the strand puzzle in praha, but I only solved fir \\(x_n\\) and not \\(y_n\\), the time that I truly solved the strand puzzle was while I was writing it down on the page. I solved the strand puzzle first by the way. I know what you might be thinking,</p>"},{"location":"arctan.html#what-even-is-the-arctan-puzzle","title":"what even is the arctan puzzle?","text":"\\[ z = a + bi = re^{i\\theta} = r(cos(\\theta) + i \\text{ } sin(\\theta)) = rcos(\\theta) + irsin(\\theta) \\] \\[ a = rcos(\\theta) \\] \\[ b = rsin(\\theta) \\] <p>Good, you can convert from polar to cartesian coordinates, but what about back from cartesian to polar? (\\(r\\) Is the distance from the origin at \\(0\\), and \\(\\theta\\) is the angle counerclockwise from the positave \\(x\\) axis. I'm saying this, because either I didn't say it earlier, or as a reminder) You would probably use the pythagorian therom to prove this, but this website doesn't heve illustrations, so I will prove the pythagorian therom without geometry. First, take the complex conjegate.</p> \\[ a - bi = rcos(\\theta) - irsin(\\theta) \\] \\[ cos(\\theta) = cos(-\\theta) \\] \\[ -sin(\\theta) = sin(-\\theta) \\] \\[ a - bi = rcos(-\\theta) + irsin(-\\theta) \\] \\[ a - bi = re^{-i\\theta} \\] \\[ \\text{Side note!} \\] \\[ \\text{ccong} (e^{i\\theta}) = e^{-i\\theta} = \\frac{1}{e^{i\\theta}} \\] \\[ \\text{End of side note.} \\] \\[ (a + bi)(a - bi) = a^2 - abi + bia + b^2 = a^2 + b^2 = re^{i\\theta} re^{-i\\theta} = r^2 \\] \\[ \\text{Side note!.. Again.} \\] \\[ r = |z| \\text{ The distance from } 0 \\text{, get it?} \\] \\[ z \\text{ ccong} (z) = |z|^2 \\] \\[ \\text{ccong} (z) = \\frac{|z|^2}{z} \\] \\[ \\frac{1}{z} = \\frac{|z|^2}{\\text{ccong} (z)} \\] \\[ \\text{End of side note... Again.} \\] \\[ a^2 + b^2 = r^2 \\] <p>Okay, that is the pythagonian therom done, bou I was looking for \\(r\\).</p> \\[ r \\text{ (Insert is greater than or equal to symbol here.) } 0 \\] \\[ r = \\sqrt{a^2 + b^2} \\] <p>That's \\(r\\), but what about \\(\\theta\\)?</p> \\[ \\frac{b}{a} = \\frac{rsin(\\theta)}{rcos(\\theta)} = \\frac{sin(\\theta)}{cos(\\theta)} = tan(\\theta) \\] \\[ \\text{Good, } r \\text{ doesn't change } \\theta! \\] \\[ \\theta = arctan(\\frac{b}{a}) = ? \\] <p>There's the arctan. By the way, this was when I used \\(h\\) instead of \\(dx\\). If \\(arctan = tan^{-1}\\), than I tried to find the derivitves of the inverse for the taylor seiries. I accedentally proved the chain rule with the proof that is now on the calculus page as a bonus. I thought \"different method for finding \\(\\theta\\). First, normalise \\(z\\).\"</p> \\[ ln(z) = ln(e^{i\\theta}) = i\\theta \\] \\[ i^2 = -1 \\] \\[ (i) (-i) = 1 \\] \\[ \\frac{1}{i} = -i \\] \\[ \\theta = -i \\text{ } ln(z) \\] \\[ \\text{But what about } ln(z) \\text{?} \\] \\[ \\text{You might remember the formula } \\frac{a^{dx} - 1}{dx} \\text{ for } ln(a) \\text{, so let's try that!} \\] \\[ \\theta = -i \\frac{z^{dx} - 1}{dx} \\] \\[ \\text{But then, what about }z^{dx} \\text{?} \\] \\[ \\theta = -i \\text{ } \\frac{((1 + i dx)^{\\frac{\\theta}{dx}})^{dx} - 1}{dx} = \\frac{1}{i} \\text{ } \\frac{(1 + i dx)^\\theta - 1}{dx} = \\text{ } \\frac{(1 + i\\theta dx) - 1}{i dx} \\] \\[ \\theta = \\theta \\] <p>So, all that we have proved is that \\(\\theta = \\theta\\). Yay!</p> <p>Ok, so that's the end of that train of thought. I googled \"derivitave of arctan\" for the result. When I found the awnser \\(\\frac{1}{1 + x^2}\\), I diden't find an awnser why. So, of course I asked ChatGPT, the proof went something like this:</p> <p>Actually, I forgot it and this is my best guess, with \\(\\overline{f}\\) as the inverse of \\(f\\):</p>"},{"location":"arctan.html#derivitave-of-arctan","title":"derivitave of arctan","text":"\\[ f(\\overline{f} (x)) = : x \\] \\[ \\frac{d}{dx} f(\\overline{{}^f} (x)) = f\\prime (\\overline{f} (x)) \\overline{f}\\prime (x) = 1 \\] \\[ \\overline{f}\\prime (x) = \\frac{1}{f\\prime (\\overline{f} (x))} \\] \\[ f(x) = tan(x) \\] \\[ \\overline{f} = arctan(x) \\] \\[ arctan \\prime (x) = \\frac{1}{tan \\prime (arctan(x))} \\] \\[ \\text{Oh! I forgot to tell you about (well, for one, it's pi day today) the other trigonometric functions other than } \\text{sin} \\text{ pronounced \"sine\", and } cos \\text{ pronounced \"cosine\" or \"cos\", but I prefer cosine, theres } tan = \\frac{sin}{cos} \\text{ pronounced \"tan\" or \"tangent\", which I use interchangibly in real life, and it's inverse } arctan \\text{ pronounced execly how it is spelled, but theres also } sec = \\frac{1}{cos} \\text{ pronounced \"secant\" or \"sec\", but I prefer secant, but theres also } csc = \\frac{1}{sin} \\text{ counterintuitivly, but it's pronounced \"cosec\" or \"cosecant\", but I prefer cosec. You can also square any of these for a } 2 \\text{ superscript in front of the function instead of the parenthasies} \\] \\[ tan \\prime (x) = (\\frac{sin(x)}{cos(x)}) \\prime = \\frac{sin\\prime (x) cos(x) - sin(x) cos\\prime (x)}{cos^2(x)} = \\frac{cos(x) cos(x) + sin(x) sin(x)}{cos^2(x)} \\] \\[ \\text{Okay, we have } cos^2 + sin^2 \\text{. Eccept, I still have to derive that.} \\] \\[ z = e^{i\\theta} = r(cos(\\theta) + i \\text{ } sin(\\theta)) = rcos(\\theta) + irsin(\\theta) \\] \\[ r = 1 \\] \\[ z = cos(\\theta) + isin(\\theta) = a + bi \\] \\[ a = cos(\\theta) \\] \\[ b = sin(\\theta) \\] \\[ r = \\sqrt{a^2 + b^2} \\] \\[ cos^2 (\\theta) + sin^2 (\\theta) = 1 \\] \\[ \\text{Okay, time to keep going.} \\] \\[ tan \\prime (x) = \\frac{1}{cos^2(x)} \\] \\[ tan \\prime (x) = sec^2 (x) \\] \\[ tan^2 (x) = \\frac{sin^2 (x)}{cos^2 (x)} = \\frac{sin^2 (x) + cos^2 (x) - cos^2 (x)}{cos^2 (x)} = \\frac{sin^2 (x) + cos^2 (x)}{cos^2 (x)} - \\frac{cos^2 (x)}{cos^2 (x)} \\] \\[ tan^2 (x) = sec^2 (x) - 1 \\] \\[ arctan\\prime (x) = \\frac{1}{sec^2 (arctan(x))} = \\frac{1}{1 + sec^2 (arctan(x)) - 1} = \\frac{1}{1 + tan^2 (arctan(x))} = \\frac{1}{1 + tan(arctan(x))tan(arctan(x))} \\] \\[ arctan\\prime (x) = \\frac{1}{1 + x^2} \\]"},{"location":"arctan.html#many-weeks-later","title":"Many weeks later...","text":"<p>I asked ChatGPT for awnsers, and it solved it immediety. This is why I called the arctan puzzle the \"tricky puzzle that took \\(1\\) ChatGPT search to solve\". Warning: This counts as a taylor seiries (404 page not found) for \\(arctan(x)\\) in the interval \\(-1 &lt; x &lt; 1\\) (or equal to \\(1\\)!)</p> \\[ arctan(0) = 0 \\] \\[ arctan(x) = \\int_{0}^{x} \\frac{1}{1 + t^2} dt \\]"},{"location":"arctan.html#quick-tangent-no-pun-intended-plus-this-is-more-about-arctan-than-tan","title":"Quick tangent (no pun intended, plus this is more about arctan than tan)","text":"\\[ \\sum\\limits_{n=0}^{\\infty}x^n = ? \\] \\[ \\text{By the convention in the harmonic page below, if } -1 &lt; x &lt; 1 \\text{, than the powers of } x \\text{ approach } 0 \\] <p>.</p> \\[ ? x = x \\sum\\limits_{n=0}^{\\infty}x^n = \\sum\\limits_{n=0}^{\\infty}x^{n + 1} = \\sum\\limits_{n=1}^{\\infty + 1}x^n = \\sum\\limits_{n=0}^{\\infty}x^n - x^0 + x^{\\infty + 1} \\] \\[ x^0 = 1 \\] \\[ x^{\\infty + 1} \\to 0 \\] \\[ ? x = ? - 1 \\] \\[ ? (x - 1) = -1 \\] \\[ ? = \\frac{-1}{x - 1} \\] \\[ \\sum\\limits_{n=0}^{\\infty}x^n = \\frac{1}{1 - x} \\]"},{"location":"arctan.html#back-on-track","title":"back on track","text":"\\[ \\frac{1}{1 + x^2} = \\frac{1}{1 - (-x^2)} = \\sum\\limits_{n=0}^{\\infty} (-x^2)^n = \\sum\\limits_{n=0}^{\\infty} (-1)^n x^{2n} \\] \\[ arctan(x) = \\int_{0}^{x} \\sum\\limits_{n=0}^{\\infty} (-1)^n t^{2n} dt = \\sum\\limits_{n=0}^{\\infty} \\int_{0}^{x} (-1)^n t^{2n} dt \\] \\[ \\int_{0}^{x} (-1)^n t^{2n} dt = (-1)^n \\frac{x^{2n + 1}}{2n + 1} \\] \\[ arctan(x) = \\sum\\limits_{n=0}^{\\infty} (-1)^n \\frac{x^{2n + 1}}{2n + 1} = x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7} \\dots \\]"},{"location":"binary.html","title":"The best way to count","text":"<p>First, the hindu-arabic numeral system: it is a way to write numbers. So, for example, six would be wrtten as \" \\(6\\) \". The arabic numerals are just: \\(0, 1, 2, 3, 4, 5, 6, 7, 8,\\) and \\(9\\). As opposed to the hindu-arabic numeral system which uses mostly decimal. For example, five hundred and sixty-seven in decimal is \"five six seven\", and then, if you use the arabic numerals to go from decimal to the hindu-arabic numeral system, you get \" \\(567\\) \". WARNING! (Kinda.) \\(567\\) is NOT a number, it is just a popular way to write the corrasponding number.</p> <p>The hindu-arabic numerals are a positional numeral system, which means that the value of each digit is affected by the position. Positional numeral system must've been a pretty revolutionary idea, at least, in comaprison to non-positional numeral systems, such as the roman numeral system. Positional numeral systems probably got into the top \\(10\\) most revolutionay mathematical ideas, 'cause you can write all infinately many numbers using just \\(10\\) digits (\\(10\\) arabic numerals), without getting out of hand quickly (just see \\(100,000\\) vs \\(MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM\\) to prove that). Also, the hindu-arabic numeral system is injective, every number has just one representation (e.g. nine as \\(9\\) vs \\(IX\\) or \\(VIV\\) or \\(VIIII\\) or \\(IIIIIIIIII\\)). All of these should make sense, and you probably take at least one if not all of these properties for granted.</p> <p>But what if there was a bug with the hindu-arabic numerals? Something that makes them NOT the best way to write numbers? There is such a bug, filed as bug report number \\(10\\): why count up to \\(10\\) before using the next digit? (Side note! the first two-digit number in any base is known as the base of the number, so the hindu-arabic numeral system has a base of ten) The choice of \\(10\\) is comepletely arbitrary, (side note! Again, the base of any system would always be written as \" \\(10\\) \" in that base.) the only reason that it is a good base is because it is a positional numeral system, the same system would work for any integer base greater then \\(1\\), and if the only reason that \\(10\\) is a good base is because it is a positional numeral system, at least on my YouTube homepage, I see a lot of proposals for a dosenal system (base \\(12\\)), but if you can bother using a different base, then the question becomes: what is the best base?</p> <p>Among (\u0d9e) all the proposals for a different base, one stands out: a better way to count by jan misali, proposing a seximal (base \\(6\\)) system. I was going to summerize it, but I'm doing this in incognito mode, too many ads. This is the best time to say it, this page is based off of the best way to count, you can either watch that video, or read this page, or both! (#NotSponsored, It's just a really good video.) But, just to be on the same page (no pun intended), jan misali is wrong, seximal is not the best base out there, so fasten your mathematical seatbelts (this page is going to be really, really long), because we're going down a deep rabbit hole, to discover</p>"},{"location":"binary.html#binary","title":"Binary","text":"<p>The actual best way to count.</p> <p>Binary is an extreme case, because any smaller base would just break, using only two symbols (say, \\(a\\) and \\(b\\)) to represent all the natural numbers. Binary also decomposes numbers into powers of \\(2\\), doing a similar role for addition as primes do for multiplication, it's also a maximally efficient game of twenty questions, chopping the space of possibilities in half every digit. Binary has appeared in ancient Egyptian mathematics, in the \\(16\\)'th century works of leibniz, and in the Arecibo message, and, of coarse, in nearly every modern computer (which I know a LOT about). But binary is also the best base for human use. By nearly any metric for comparing bases, binary preforms best, and the best part is, it's all because \\(2\\) is the smallest integer greater than \\(1\\). In contrast, a base like seximal, misali's favorite, is still a very good base, but almost entirely because of a mathematical coincidence: six contains factors of the first \\(2\\) primes, and is adjacent to the next \\(2\\) primes. seximal is a cheater in the world of bases, and a more thorough analysis exposes the shallowness of it's tricks and reveals a gaping emptiness underneath. Meanwhile, binary shines without the need for coincidences, it plays fair and square, and that fairness pays off in the end.</p> <p>Because this is a reply to a better way to count, let's start out with misali's think of binary (click here to hear the actual audio clip): Base \\(2\\), binary! Binary is the smallest base that actually works, at all. Doing simple arithmetic in binary is super easy, and only having \\(2\\) digits means storing binary information on a computer is maximally efficient. That's about it for positives! Every other part about binary is a downside, numbers get real long real fast, and the only terminating ratios are the ones where the denominator is a power of \\(2\\).</p>"},{"location":"binary.html#chapter-0-binary-numbers-are-long","title":"Chapter \\(0\\): binary numbers are long","text":""},{"location":"binomial.html","title":"Binomial","text":""},{"location":"binomial.html#1-xn","title":"\\((1 + x)^n\\)","text":"\\[ (1 + x)^0 = 1 \\] \\[ (1 + x)^1 = 1 + x \\] \\[ (1 + x)^2 = 1 + 2x + x^2 \\] \\[ (1 + x)^3 = 1 + 3x + 3x^2 + x^3 \\] \\[ (1 + x)^n = \\sum\\limits_{k = 0}^{n} f(n, k) x^k = \\sum\\limits_{k = -\\infty}^{\\infty} f(n, k) x^k \\] \\[ f(n, 0) = 1 \\] \\[ f(n, k) = 0, \\] <p>for \\(k\\) bigger than \\(n\\) or smaller than \\(0\\)</p> \\[ (1 + x)^n = (1 + x) (1 + x)^{n - 1} = (1 + x) \\sum\\limits_{k = -\\infty}^{\\infty} f(n - 1, k) x^k = \\sum\\limits_{k = -\\infty}^{\\infty} f(n - 1, k) x^k + \\sum\\limits_{k = -\\infty}^{\\infty} f(n - 1, k) x^{k + 1} = \\sum\\limits_{k = -\\infty}^{\\infty} f(n - 1, k) x^k +\\sum\\limits_{k = -\\infty}^{\\infty} f(n - 1, k - 1) x^k \\] \\[ \\sum\\limits_{k = -\\infty}^{\\infty} f(n, k) x^k = \\sum\\limits_{k = -\\infty}^{\\infty} (f(n - 1, k) + f(n - 1, k - 1)) x^k \\] \\[ f(n, k) = f(n - 1, k) + f(n - 1, k - 1) \\]"},{"location":"binomial.html#discrete-calculus","title":"discrete calculus","text":"\\[ f(n, k) - f(n - 1, k) = f(n - 1, k - 1) \\] \\[ \\Delta f(x) = : f(x + 1) - f(x) \\] \\[ x^{\\frac{n}{}} = : \\prod\\limits_{k = x - n + 1}^{x} k = x \\cdot (x - 1) \\cdot (x - 2) ... \\text{n times} = \\frac{x!}{(x - n)!} \\] \\[ \\Delta x^{\\frac{n}{}} = \\frac{(x + 1)!}{(x - n + 1)!} - \\frac{x!}{(x - n)!} = (x + 1) \\frac{x!}{(x - n + 1)!} - \\frac{x!}{\\frac{(x - n + 1)!}{x - n + 1}} = (x + 1) x^{\\frac{n - 1}{}} - (x - n + 1) x^{\\frac{n - 1}{}} = x^{\\frac{n - 1}{}} ((x + 1) - (x - n + 1)) \\] \\[ \\Delta x^{\\frac{n}{}} = n x^{\\frac{n - 1}{}} \\] \\[ \\Delta f(x, k) = f(x + 1, k) - f(x, k) = f((x + 1), k) - f((x + 1) - 1, k) = f((x + 1) - 1, k - 1) \\] \\[ \\Delta f(x, k) = f(x, k - 1) \\] \\[ \\Delta \\frac{x^{\\frac{n}{}}}{n!} = \\frac{x^{\\frac{n - 1}{}}}{(n - 1)!} \\] \\[ f(x, k) = \\frac{x^{\\frac{k}{}}}{k!} = \\frac{x!}{k!(x - k)!} = \\begin{pmatrix} x \\\\ k \\\\ \\end{pmatrix} \\] \\[ (1 + x)^n = \\sum\\limits_{k = 0}^{\\infty} \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} x^k = \\sum\\limits_{k = 0}^{\\infty} \\frac{n! x^k}{k!(n - k)!} \\] <p>if \\(x\\) is closer to \\(0\\), than this will converge faster!</p>"},{"location":"binomial.html#pi","title":"\\(\\pi\\)","text":"\\[ \\pi = 2 \\int_{-1}^{1} \\sqrt{1 - x^2} dx = 4 \\int_{0}^{1} \\sqrt{1 - x^2} dx \\] <p>but at \\(0\\), this is \\(1\\), and at \\(\\frac{1}{2}\\), this will converge really fast!</p> <p>But</p> \\[ \\int_{0}^{\\frac{1}{2}} \\sqrt{1 - x^2} dx = \\frac{\\sqrt{3}}{8} + \\frac{\\pi}{12} \\]"},{"location":"binomial.html#sqrt3","title":"\\(\\sqrt{3}\\)","text":"\\[ \\sqrt{3} = \\sqrt{4 - 1} = \\sqrt{4(1 - \\frac{1}{4})} = 2 \\sqrt{1 - \\frac{1}{4}} = 2 (1 - \\frac{1}{4})^{\\frac{1}{2}} \\] <p>which converges really fast!</p> <p>thus...</p> \\[ \\pi = 12 ((\\sum\\limits_{k = 0}^{\\infty} \\frac{\\begin{pmatrix} \\frac{1}{2} \\\\ k \\\\ \\end{pmatrix} (-\\frac{1}{4})^{k + 1}}{k + 1}) - \\frac{\\sum\\limits_{k = 0}^{\\infty} \\begin{pmatrix} \\frac{1}{2} \\\\ k \\\\ \\end{pmatrix} (-\\frac{1}{4})^k}{4}) \\] <p>where</p> \\[ \\begin{pmatrix} \\frac{1}{2} \\\\ k \\\\ \\end{pmatrix} = \\frac{\\prod\\limits_{m = \\frac{1}{2} - k + 1}^{\\frac{1}{2}} m}{k!} = \\frac{\\prod\\limits_{m = 1 - k}^{0} m + \\frac{1}{2}}{k!} \\]"},{"location":"brainstorm.html","title":"Brainstorming a new page","text":"<p>The equations below are the beginning of pages that may or may not get added to my website. All pages after Modular arithmetic and this one started here in the brainstorm page. I got (almost none of) these from copying things from a piece of paper (actually, just the one below) that had potential to become a digital page. And finally, the titles have a question mark if they are my best guess for the titles of the page, no question mark if I made the page and know the title, a question mark if I never published the page, but still want the world to see it, and extra small if is just one puzzle, but didn't fit anywhere and/or was worth making a dedicated page. Also, happy eclipse day! The eclipse was worth the \\(10\\) (total) hours of driving.</p>"},{"location":"brainstorm.html#galois-theorygroup-theoryring-theory","title":"galois theory/group theory/ring theory?","text":"<p>A field of numbers is a collection of numbers where you can add, subtract, multiply, and even sometimes divide two numbers (as long as you don't divide by zero) in that field to get another number in that field. For example, the rationals, the reals and the complex numbers are all fields that are infinite and you can divide them. The complex lattice points (which are complex numbers of the form \\(integer + integer \\cdot i\\) ), the matrices (insert joke here), and the integers, are all infinite but can not always be divided. I know what you might be thinking: \"what the stand-in-for-a-curse-word is a non-infinite field?\". A non-infinite field (or finite field) is something like the modulo numbers. You might not be able to divide in a non-prime base, but you can do it in a prime base. So that completes the venn diagram!</p> <p>Today, I want to show you an infinite field where you can not divide, I'm talking (well, typing) about numbers of the form</p> \\[ a + b \\sqrt{7} \\] <p>for integers \\(a\\) and \\(b\\).</p> <p>Side note! A number of this form can only be written in one way. Not because \\(\\sqrt{7}\\) is an imaginary number, but because \\(\\sqrt{7}\\) is irrational. End of side note.</p> <p>You can probably take my word for it that you can add, subtract, and multiply these numbers to get another one, but here's a proof (which was the only thing on that was on the paper that inspired this (digital) page. Other than division and square roots).</p> \\[ (a + b \\sqrt{7}) + (c + d \\sqrt{7}) = a + b \\sqrt{7} + c + d \\sqrt{7} \\] \\[ (a + b \\sqrt{7}) - (c + d \\sqrt{7}) = a + b \\sqrt{7} - c - d \\sqrt{7} \\] \\[ (a + b \\sqrt{7}) \\cdot (c + d \\sqrt{7}) = ac + ad \\sqrt{7} + b \\sqrt{7} c + b \\sqrt{7} d \\sqrt{7} \\] <p>.</p> \\[ (a + b \\sqrt{7}) + (c + d \\sqrt{7}) = (a + c) + (b + d) \\sqrt{7} \\] \\[ (a + b \\sqrt{7}) - (c + d \\sqrt{7}) = (a - c) + (b - d) \\sqrt{7} \\] \\[ (a + b \\sqrt{7}) \\cdot (c + d \\sqrt{7}) = (ac + 7bd) + (ad + bc) \\sqrt{7} \\] \\[ \\text{And all of these are a number of this form. The end! Yeah, that was my whole* idea.} \\]"},{"location":"brainstorm.html#why-cant-you-multiply-two-vectors-all-endings","title":"why can't you multiply two vectors? (all endings)?","text":"\\[ \\vec{u} = \\begin{bmatrix} u_x \\\\ u_y \\\\ u_z \\\\ \\end{bmatrix} = u_x \\hat{i} + u_y \\hat{j} + u_z \\hat{k} \\] \\[ \\vec{v} = \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\\\ \\end{bmatrix} = v_x \\hat{i} + v_y \\hat{j} + v_z \\hat{k} \\] \\[ \\vec{u} \\vec{v} = (u_x \\hat{i} + u_y \\hat{j} + u_z \\hat{k}) (v_x \\hat{i} + v_y \\hat{j} + v_z \\hat{k}) = u_x \\hat{i} v_x \\hat{i} + u_x \\hat{i} v_y \\hat{j} + u_x \\hat{i} v_z \\hat{k} + u_y \\hat{j} v_x \\hat{i} + u_y \\hat{j} v_y \\hat{j} + u_y \\hat{j} v_z \\hat{k} + u_z \\hat{k} v_x \\hat{i} + u_z \\hat{k} v_y \\hat{j} + u_z \\hat{k} v_z \\hat{k} \\] \\[ \\text{For lack of a better way to display this, } \\vec{u} \\vec{v} \\text{ equals the thing below:} \\] \\[ (\\hat{i} \\hat{i})(u_x v_x) + (\\hat{i} \\hat{j})(u_x v_y) + (\\hat{i} \\hat{k})(u_x v_z) \\] \\[ (\\hat{j} \\hat{i})(u_y v_x) + (\\hat{j} \\hat{j})(u_y v_y) + (\\hat{j} \\hat{k})(u_y v_z) \\] \\[ (\\hat{k} \\hat{i})(u_z v_x) + (\\hat{k} \\hat{j})(u_z v_y) + (\\hat{k} \\hat{k})(u_z v_z) \\] \\[ \\text{Ending } 1 \\text{, I give up.} \\] \\[ \\text{Because there is no way that this is a vector. Anyways, ending 2, } \\]"},{"location":"brainstorm.html#geometric-algebra","title":"geometric algebra","text":"<p>Yeah, I forgot all the endings to multiplying vectors, so I came up with the previous one. Well, I guess I had this one, but I would rather make it into its own page. After watching A Swift Introduction to Geometric Algebra (literally, that was the name), I thought that (if it is a scalar plus a bivector), than it is just a scalar plus a vector times \\(i\\) (or \\(-i\\), I am not sure yet), but I will call it \\(U\\) instead. But first, here's the definition of multiplying two basis vectors (all the alternative endings probably just had alternative definitions for this. I think that I remember the definition that multiplication was anticommutative, which would lead me to the cross product): The product of a basis vector \\(e_1\\) and it self is \\(0\\), and the product of two basis vectors \\(e_1\\) and \\(e_2\\) equals \\(-e_2 e_1\\) equals \\(e_3\\). This means that you can do this at any point in the product of basis vectors.</p> \\[ \\hat{i} = x \\] \\[ \\hat{j} = y \\] \\[ \\hat{k} = z \\] \\[ \\text{Now is about as good of a time as any to simplify the product.} \\] \\[ \\vec{u} \\vec{v} = \\begin{pmatrix} (xx)(u_x v_x) + (xy)(u_x v_y) + (xz)(u_x v_z) +  \\\\ (yx)(u_y v_x) + (yy)(u_y v_y) + (yz)(u_y v_z) + \\\\ (zx)(u_z v_x) + (zy)(u_z v_y) + (zz)(u_z v_z) \\\\ \\end{pmatrix} = \\begin{pmatrix} (xx)(u_x v_x) + (xy)(u_x v_y) - (zx)(u_x v_z) -  \\\\ (xy)(u_y v_x) + (yy)(u_y v_y) + (yz)(u_y v_z) + \\\\ (zx)(u_z v_x) - (yz)(u_z v_y) + (zz)(u_z v_z) \\\\ \\end{pmatrix} = \\begin{pmatrix} u_x v_x + (xy)(u_x v_y) - (zx)(u_x v_z) -  \\\\ (xy)(u_y v_x) + u_y v_y + (yz)(u_y v_z) + \\\\ (zx)(u_z v_x) - (yz)(u_z v_y) + u_z v_z \\\\ \\end{pmatrix} = u_x v_x +  u_y v_y + u_z v_z + \\begin{pmatrix} (xy)(u_x v_y - u_y v_x) +  \\\\ (yz)(u_y v_z - u_z v_y) + \\\\ (zx)(u_z v_x - u_x v_z) \\\\ \\end{pmatrix} = \\vec{u} \\cdot \\vec{v} + \\begin{pmatrix} (xy)(u_x v_y - u_y v_x) +  \\\\ (yz)(u_y v_z - u_z v_y) + \\\\ (zx)(u_z v_x - u_x v_z) \\\\ \\end{pmatrix} \\] \\[ \\text{But now, I need to turn a bivector into a vector, I'll use that } U \\text{ thing for that.} \\] <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>\\(100\\) Lines btw.</p> \\[ U = xyz \\] <p>Puzzle time! Prove that \\(U \\vec{v} = \\vec{v} U\\).</p> \\[ U^2 = xyzxyz = -xyzxzy = xyzzxy = xyxy = -xxyy \\] \\[ U^2 = -1 \\] \\[ \\text{Yes, your suspicions are confirmed, it was called } i \\text{ for that reason.} \\] \\[ U^3 = xyzxyzxyz = -xyxzyzxyz = xxyzyzxyz = yzyzxyz = -yyzzxyz = -xyz = -U = xzy = -zxy = zyx \\] \\[ U^4 = U^2 U^2 = (-1) (-1) = 1 \\] \\[ U^4 = U^3 U = (-U) U = -U^2 = -(-1) = 1 \\] <p>.</p> \\[ U^1 = U = xyz \\] \\[ U^2 = 1 \\] \\[ U^3 = -U = zyx = -xyz \\] \\[ U^4 = (-U) U  = 1 \\] <p>\\(2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2\\) lines!</p> <p>.</p> \\[ xy = xyzz = Uz \\] \\[ yz = yzxx = -yxzx = xyzx = Ux \\] \\[ zx = zxyy = -zyxy = -(-U)y = Uy \\] \\[ \\text{Yes! Now I can finally solve the puzzle.} \\] \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\begin{pmatrix} (Uz)(u_x v_y - u_y v_x) +  \\\\ (Ux)(u_y v_z - u_z v_y) + \\\\ (Uy)(u_z v_x - u_x v_z) \\\\ \\end{pmatrix} = \\vec{u} \\cdot \\vec{v} + U \\begin{pmatrix} x(u_y v_z - u_z v_y) +  \\\\ y(u_z v_x - u_x v_z) + \\\\ z(u_x v_y - u_y v_x) \\\\ \\end{pmatrix} = \\vec{u} \\cdot \\vec{v} + U \\begin{bmatrix} u_y v_z - u_z v_y \\\\ u_z v_x - u_x v_z \\\\ u_x v_y - u_y v_x \\\\ \\end{bmatrix} \\] <p>.</p> \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + U \\text{ } \\vec{u} \\times \\vec{v} \\] \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\hat{i} \\hat{j} \\hat{k} \\text{ } \\vec{u} \\times \\vec{v} \\] \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\vec{u} \\times \\vec{v} \\text{ } i \\] <p>Also, the cross product only works in \\(3d\\) while this \\(\\hat{i} \\hat{j} \\hat{k} \\text{ } \\vec{u} \\times \\vec{v}\\) thing works in any dimension. This operator actually has a name (well, two names), the outer product (as opposed to the dot product sometimes referred to as the inner product) or wedge product for its appearance as a wedge unicode character. This more general cross product is written \\(\\vec{u} \u2227 \\vec{v}\\)</p> \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\vec{u} \u2227 \\vec{v} \\]"},{"location":"brainstorm.html#maxwells-equation-singular","title":"Maxwell's equation (singular)","text":"\\[ \\nabla F = J \\] \\[ \\text{Actually,} \\] \\[ \\nabla F = \\frac{J}{c \\epsilon_0} \\] <p>.</p>"},{"location":"brainstorm.html#frac1vecv","title":"\\(\\frac{1}{\\vec{v}}\\)","text":"\\[ \\vec{v} \\frac{1}{\\vec{v}} = \\vec{v} \\cdot \\frac{1}{\\vec{v}} + \\hat{i} \\hat{j} \\hat{k} \\text{ } \\vec{v} \\times \\frac{1}{\\vec{v}} = 1 + \\hat{i} \\hat{j} \\hat{k} \\text{ } 0 \\] \\[ \\vec{v} \\cdot \\frac{1}{\\vec{v}} = 1 \\] \\[ \\vec{v} \\times \\frac{1}{\\vec{v}} = 0 \\] \\[ \\text{if } \\vec{v} \\times \\frac{1}{\\vec{v}} = 0 \\text{, than they are on the same axis, so } \\frac{1}{\\vec{v}} = c \\vec{v} \\text{, and the puzzle now is to solve for } c \\text{ in terms of } \\vec{v} \\] \\[ \\vec{v} \\cdot c \\vec{v} = || \\vec{v} || \\cdot || c \\vec{v} || \\cdot cos(\\text{The angle between them}) = 1 \\] \\[ \\text{But the angle between them is } 0 \\text{, so} \\] \\[ \\vec{v} \\cdot c \\vec{v} = || \\vec{v} || \\cdot || c \\vec{v} || =  c || \\vec{v} || \\cdot || \\vec{v} || = 1 \\] \\[ c = \\frac{1}{|| \\vec{v} ||^2} \\] <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> \\[ \\frac{1}{\\vec{v}} = \\frac{\\vec{v}}{|| \\vec{v} ||^2} \\] <p>And perfect timing, it is exactly \\(200\\) lines.</p> \\[ \\frac{1}{\\frac{1}{\\vec{v}}} = \\frac{\\frac{1}{\\vec{v}}}{|| \\frac{1}{\\vec{v}} ||^2} = \\frac{\\frac{1}{\\vec{v}}}{|| \\vec{v} \\frac{1}{|| \\vec{v} ||^2} ||^2} = \\frac{\\frac{1}{\\vec{v}}}{\\frac{1}{|| \\vec{v} ||^4} || \\vec{v}||^2} = \\frac{\\frac{\\vec{v}}{|| \\vec{v} ||^2}}{\\frac{1}{|| \\vec{v}||^2}} = \\vec{v} \\] \\[ \\text{But that is only in } 3d \\text{, what about } 4d \\text{ and beyond? I remember telling my sister the other day \"what happens when you multiply two vectors? In } 4 \\text{ dimensions!\" (Both gasp).} \\] \\[ \\text{Let's say that we are in dimension } d \\text{. First, basis vectors} \\] \\[ e_1 = \\hat{i} = x \\] \\[ e_2 = \\hat{j} = y \\] \\[ e_3 = \\hat{k} = z \\] \\[ e_4 = \\hat{l} = w \\] \\[ \\vdots \\] \\[ \\vec{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_d\\\\ \\end{bmatrix} = \\sum\\limits_{n = 1}^{d} u_n e_n \\] \\[ \\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_d \\\\ \\end{bmatrix} = \\sum\\limits_{n = 1}^{d} v_n e_n \\] \\[ \\vec{u} \\vec{v} = \\begin{bmatrix} (xx) u_1 v_1 &amp; (xy) u_1 v_2 &amp; (xz) u_1 v_3 &amp; (xw) u_1 v_4 &amp; \\dots &amp; (x e_d) u_1 v_d  \\\\ (yx) u_2 v_1 &amp; (yy) u_2 v_2 &amp; (yz) u_2 v_3 &amp; (yw) u_2 v_4 &amp; \\dots &amp; (y e_d) u_2 v_d \\\\ (zx) u_3 v_1 &amp; (zy) u_3 v_2 &amp; (zz) u_3 v_3 &amp; (zw) u_3 v_4 &amp; \\dots &amp; (z e_d) u_3 v_d \\\\ (wx) u_4 v_1 &amp; (wy) u_4 v_2 &amp; (wz) u_4 v_3 &amp; (ww) u_4 v_4 &amp; \\dots &amp; (w e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (e_d x) u_d v_1 &amp; (e_d y) u_d v_2 &amp; (e_d z) u_d v_3 &amp; (e_d w) u_d v_4 &amp; \\dots &amp; (e_d e_d) u_d v_d \\\\ \\end{bmatrix} = \\begin{bmatrix} u_1 v_1 &amp; (xy) u_1 v_2 &amp; (xz) u_1 v_3 &amp; (xw) u_1 v_4 &amp; \\dots &amp; (x e_d) u_1 v_d  \\\\ (yx) u_2 v_1 &amp; u_2 v_2 &amp; (yz) u_2 v_3 &amp; (yw) u_2 v_4 &amp; \\dots &amp; (y e_d) u_2 v_d \\\\ (zx) u_3 v_1 &amp; (zy) u_3 v_2 &amp; u_3 v_3 &amp; (zw) u_3 v_4 &amp; \\dots &amp; (z e_d) u_3 v_d \\\\ (wx) u_4 v_1 &amp; (wy) u_4 v_2 &amp; (wz) u_4 v_3 &amp; u_4 v_4 &amp; \\dots &amp; (w e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (e_d x) u_d v_1 &amp; (e_d y) u_d v_2 &amp; (e_d z) u_d v_3 &amp; (e_d w) u_d v_4 &amp; \\dots &amp; u_d v_d \\\\ \\end{bmatrix} = (u_1 v_1 + u_2 v_2 + u_3 v_3 + u_4 v_4 + \\dots + u_d v_d) + \\begin{bmatrix}  &amp; (xy) u_1 v_2 &amp; (xz) u_1 v_3 &amp; (xw) u_1 v_4 &amp; \\dots &amp; (x e_d) u_1 v_d  \\\\ (yx) u_2 v_1 &amp;  &amp; (yz) u_2 v_3 &amp; (yw) u_2 v_4 &amp; \\dots &amp; (y e_d) u_2 v_d \\\\ (zx) u_3 v_1 &amp; (zy) u_3 v_2 &amp;  &amp; (zw) u_3 v_4 &amp; \\dots &amp; (z e_d) u_3 v_d \\\\ (wx) u_4 v_1 &amp; (wy) u_4 v_2 &amp; (wz) u_4 v_3 &amp;  &amp; \\dots &amp; (w e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ (e_d x) u_d v_1 &amp; (e_d y) u_d v_2 &amp; (e_d z) u_d v_3 &amp; (e_d w) u_d v_4 &amp; \\dots &amp;  \\\\ \\end{bmatrix} \\] \\[ \\text{It was really frustrating to write a plus in between, so I just gave up. Just assume that the result above is summed together.} \\] \\[ \\vdots \\] \\[ \\text{I actually have no idea how to prove this, but} \\] \\[ \\frac{1}{\\vec{v}} = \\frac{\\vec{v}}{|| \\vec{v} ||^2} \\]"},{"location":"brainstorm.html#mini-quadratic","title":"mini quadratic","text":"<p>How this page works is that I count up in binary such that if the last digit is one, it has a constant term, if the second to last digit is one, it has a linear term, if the third to last digit is one, it has a quadratic term. Starting at \\(0\\) with the number above.</p> <p>.</p> <p>000</p> \\[  = 0 \\] \\[ x = \\frac{0}{0} \\] <p>001</p> \\[ a = 0 \\] \\[ x = \\frac{a}{0} \\] <p>010</p> \\[ ax = 0 \\] \\[ x = \\frac{0}{a} \\] <p>011</p> \\[ ax + b = 0 \\] \\[ x + \\frac{b}{a} = 0 \\] \\[ x = -\\frac{b}{a} \\] <p>100</p> \\[ ax^2 = 0 \\] \\[ x^2 = \\frac{0}{a} \\] \\[ x = \\sqrt{\\frac{0}{a}} \\] \\[ \\text{if } a = 0 \\text{, then the square root of } \\frac{0}{0} \\text{ is still } \\frac{0}{0} \\text{, and if } a \\ne 0 \\text{ then the square root of } 0 \\text{ is still } 0 \\text{! (Not } 0 \\text{ factorial)} \\] \\[ x = \\frac{0}{a} \\] <p>\\(300\\) Lines.</p> <p>101</p> \\[ ax^2 + b = 0 \\] \\[ x^2 = -\\frac{b}{a} \\] \\[ x = \\sqrt{-\\frac{b}{a}} \\] <p>110</p> \\[ ax^2 + bx = 0 \\] \\[ x(ax + b) = 0 \\] \\[ x = 0, x = -\\frac{b}{a} \\] \\[ x = -\\frac{b}{2a} \\mp \\frac{b}{2a} \\] <p>111</p> \\[ ax^2 + bx + c = 0 \\] \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]"},{"location":"brainstorm.html#line-through-two-points","title":"line through two points?","text":"\\[ f(a_x) = a_y \\] \\[ f(b_x) = b_y \\] \\[ f(x) \\text{ Is a linear function.} \\] \\[ \\text{So, } f(x) \\text{ is the line that passes thru } (a_x, a_y) \\text{ and } (b_x, b_y) \\] \\[ \\text{there are many ways to solve this puzzle, but I only have time to show you one.} \\] \\[ f(x) = cx + d \\] \\[ c a_x + d = a_y \\] \\[ c b_x + d = b_y \\] \\[ c a_x + d - c b_x - d = a_y - b_y \\] \\[ c(a_x - b_x) = a_y - b_y \\] \\[ c = \\frac{a_y - b_y}{a_x - b_x} \\] \\[ \\frac{a_y - b_y}{a_x - b_x} a_x + d = a_y \\] \\[ \\frac{a_y - b_y}{a_x - b_x} a_x - a_y = d \\] \\[ (a_y - b_y) a_x - a_y (a_x - b_x) = d (a_x - b_x) \\] \\[ a_x a_y - a_x b_y - a_y a_x + a_y b_x = d (a_x - b_x) \\] \\[ a_x a_y - a_x a_y + a_y b_x - a_x b_y = d (a_x - b_x) \\] \\[ d = \\frac{a_y b_x - a_x b_y}{a_x - b_x} \\] \\[ f(x) = \\frac{a_y - b_y}{a_x - b_x} x + \\frac{a_y b_x - a_x b_y}{a_x - b_x} = \\frac{b_y - a_y}{b_x - a_x} x + \\frac{a_x b_y - a_y b_x}{b_x - a_x} = \\frac{(a_y - b_y) x + a_y b_x - a_x b_y}{a_x - b_x} = \\frac{(b_y - a_y) x + a_x b_y - a_y b_x}{b_x - a_x} \\] \\[ \\text{Personally, my favorite is:} \\] \\[ f(x) = \\frac{a_y - b_y}{a_x - b_x} x + \\frac{a_x b_y - a_y b_x}{b_x - a_x} \\]"},{"location":"brainstorm.html#multivector-times-tables","title":"multivector times tables","text":"<p>\\(0\\) d</p> \\[ (u_1)(v_1) = (u_1 v_1) = (u_1 v_1) \\] <p>\\(1\\) d</p> \\[ (u_1 + u_2 x)(v_1 + v_2 x) = (u_1 v_1 + u_2 v_2) + (u_1 v_2 + u_2 v_1)x \\] <p>\\(2\\) d</p> \\[ (u_1 + u_2 x + u_3 y + u_4 xy)(v_1 + v_2 x + v_3 y + v_4 xy) = (u_1 v_1 + u_2 v_2 + u_3 v_3 - u_4 v_4) + (u_1 v_2 + u_2 v_1 - u_3 v_4 + u_4 v_3)x + (u_1 v_3 + u_2 v_4 + u_3 v_1 - u_4 v_2)y + (u_1 v_4 + u_2 v_3 - u_3 v_2 + u_4 v_1)xy \\] <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>\\(400\\) lines.</p> <p>\\(3\\) d</p> \\[ (u_1 + u_2 x + u_3 y + u_4 xy + u_5 z + u_6 xz + u_7 yz + u_8 xyz)(v_1 + v_2 x + v_3 y + v_4 xy + v_5 z + v_6 xz + v_7 yz + v_8 xyz) = u_1 v_1 + u_1 v_2 x + u_1 v_3 y + u_1 v_4 xy + u_1 v_5 z + u_1 v_6 xz + u_1 v_7 yz + u_1 v_8 xyz + u_2 v_1 x + u_2 v_2 + u_2 v_3 xy + u_2 v_4 y + u_2 v_5 xz + u_2 v_6 z + u_2 v_7 xyz + u_2 v_8 yz + u_3 v_1 y - u_3 v_2 xy + u_3 v_3 - u_3 v_4 x + u_3 v_5 yz - u_3 v_6 xyz + u_3 v_7 z - u_3 v_8 xz + u_4 v_1 xy - u_4 v_2 y + u_4 v_3 x - u_4 v_4 + u_4 v_5 xyz - u_4 v_6 yz + u_4 v_7 xz - u_4 v_8 z + u_5 v_1 z - u_5 v_2 xz - u_5 v_3 yz + u_5 v_4 xyz + u_5 v_5 - u_5 v_6 x - u_5 v_7 y + u_5 v_8 xy \\] \\[ \\text{I think I'm too lazy to finish this.} \\]"},{"location":"brainstorm.html#set-theorylogic-definitions","title":"set theory/logic (definitions)","text":"\\[ \\text{A set is a well defined collection of objects, a set could contain the two shoes on you feet, or the } 5 \\text{ pieces of cheese on this cutting board (that I'm going to pretend exists), sets can even contain other sets, but sets can not contain themselves, because it would lead to a paradox: would the set that contains every set that doesn't contain itself contain itself?\" this also means that there isn't a set that contains everything.} \\] \\[ \\text{But the thing is, using some symbols, you can describe almost all of math. These symbols can just be pronounced as words, and it would make a sentence, such as \"} \u00ac \\exists (x): |x| &lt; 0 \\text{\" as \"there does not exist } x \\text{ such that the absolute value of } x \\text{ is strictly less than } 0 \\text{\". Time to rapidfire through each one's pronunciation and meaning.} \\] <p>.</p> \\[ \u2200 \\text{ Is pronounced \"for any\" or \"for all\" (but I prefer \"for any\") and means what it says. It than has an open parentheses, a thing (} x, y, z, \\text{ or a set) that I will call } x \\text{ for now, a closed parentheses (parenthese is not a word), a } \\cdot \\text{, a statement that implies something about } x \\text{, a colon, and finish it off with a statement including } x. \\] \\[ ( \\text{ and } ) \\text{ are not pronounced.} \\] \\[ \\cdot \\text{ Is pronounced \"such that\" and it's only used in two contexts: \"for any } x \\text{ such that...\" and \"there exists } x \\text{ such that...\".} \\] \\[ : \\text{ Is pronounced however a colon is pronounced.} \\] \\[ \\exists \\text{ Is pronounced \"there exists\" and I don't think I need to explain that.} \\] \\[ \u00ac \\text{ Is pronounced \"is not\" or \"does not\" as in \"there does not exist } x \\text{\".} \\] \\[ \\in \\text{ Is pronounced \"is an element of\" where an element of a set is a singular object that is contained in that set.} \\] \\[ \u00d8 \\text{ Is pronounced \"the empty set\" and means \"the set of which is empty inside\".} \\] \\[ x, y, \\text{ And } z \\text{ are pronounced \"} x, y, \\text{ And } z \\text{\" and they all mean \"a thing that could  be an element of a set\".} \\] \\[ \u2286 \\text{ Is pronounced \"is a subset of\" and I'll get to the meaning of that in the next chapter.} \\] \\[ \\text{capital letters are sets.} \\] \\[ \\iff \\text{ Is pronounced \"if and only if\" as in \"if statement } a \\text{ is true, statement } b \\text{ is true, and if statement } a \\text{ is false, statement } b \\text{ is false\".} \\] \\[ pow \\text{ Is pronounced \"the power set of\" as in \"} pow(S) \\text{\" and I'll get to the meaning in the next chapter.} \\] \\[ \u2229 \\text{ Is pronounced \"and\" and means \"} a \u2229 b \\text{ is true if and only if statement } a \\text{ is true and } b \\text{ is true\".} \\] \\[ = : \\text{ Is pronounced \"equals by definition\" and means \"define the thing on the left as the thing on the right\", or was it the other way around?} \\] \\[ = \\text{ Is pronounced \"is the same as\" and I'll get to its formal meaning in the next chapter.} \\] \\[ \\in^S \\text{ Is pronounced \"is a super element of\" (} S \\text{ for super) and I'll get to its meaning in the next chapter.} \\] \\[ \u2228 \\text{ Is pronounced \"or\" and means \"} a \u2228 b \\text{ is true if statement } a \\text{ is true or } b \\text{ is true... Or both!\", it can also mean the union of two sets, in that case, it is pronounced \"unioned with\", but I'll get to its formal meaning in the next chapter.} \\] \\[ \\text{succ Is pronounced \"the immediate successor of\" and means \"that number } + 1 \\text{\".} \\] \\[ set \\text{ Is pronounced \"the set containing\" as in \"} set(S) \\text{\" and I'll get to its formal meaning in the next chapter.} \\]"},{"location":"brainstorm.html#set-theory-definitions-from-those-definitions","title":"set theory (definitions from those definitions)?","text":"\\[ \u00ac \\exists (x) \\cdot x \\in \u00d8 \\] \\[ A \u2286 B \\iff \u2200(x) \\cdot x \\in A: x \\in B \\] \\[ \u2200(P) \\cdot \u2200(U) \\cdot U \u2286 S: U \\in P \u2229 \u2200(T) \\cdot T \u00ac \u2286 S: T \u00ac \\in P: P = : pow(S) \\] \\[ A = B \\iff A \u2286 B \u2229 B \u2286 A \\] \\[ \u00ac \\exists (S) \\cdot S \\in S \\] \\[ x \\in \\in S \\iff \\exists (U) \\cdot U \\in S \u2229 x \\in U \\] \\[ x \\in \\in S \\text{ can also be written as } \\in^2 \\] \\[ x \\in \\in \\in S \\iff \\exists (U) \\cdot U \\in S \u2229 x \\in^2 U \\] \\[ x \\in \\in \\in S \\text{ can also be written as } \\in^3 \\] \\[ x \\in \\in \\in \\in S \\iff \\exists (U) \\cdot U \\in S \u2229 x \\in^3 U \\] \\[ x \\in \\in \\in \\in S \\text{ can also be written as } \\in^4 \\] \\[ \\vdots \\] \\[ x \\in^{a + b} S \\iff \\exists (U) \\cdot U \\in^a S \u2229 x \\in^b U \\] \\[ x \\in^{a + b} S \\text{ can also be written as } x \\in^a \\in^b S \\] \\[ x \\in^S S \\iff x \\in S \u2228 \\exists (U) \\cdot U \\in S \u2229 x \\in^S U \\]"},{"location":"brainstorm.html#_1","title":".","text":"<p>Was recursion in the rule book? I guess so.</p>"},{"location":"brainstorm.html#set-theory-numbers","title":"set theory (numbers)","text":"\\[ 0 = \u00d8 \\] \\[ \\text{succ} (n) \\text{ (Which mathematically equals } n + 1 \\text{) Is how you would usually define numbers, so I'll define numbers that way, I'll say that succ} (n) \\text{ is the set that contains all numbers } 0 \\text{-} n \\text{. But first: the union of two sets, denoted as an } \u2228 \\text{ sign.} \\] \\[ x \\in A \u2228 B \\iff x \\in A \u2228 x \\in B \\] \\[ \\text{Around } 500 \\text{ lines?? (I might add another definition, but at the time of typing this, this is on } 500 \\text{ lines.)} \\] \\[ \u2200(S) \\cdot E \\in S \u2229 \u2200(T) \\cdot T \u00ac = E: T \u00ac \\in S: S = : set(E) \\] \\[ \\text{succ} (n) = : set(n) \u2228 n \\]"},{"location":"brainstorm.html#set-theory-proofs","title":"set theory proofs?","text":"<p>No.</p>"},{"location":"brainstorm.html#bayes","title":"bayes'?","text":"<p>.</p>"},{"location":"brainstorm.html#counting-in-binary","title":"counting in binary","text":"<p>Binary is a way to count where instead of the places: \\(1\\), \\(10\\), \\(100\\), \\(1,000\\), and so on, you use the \\(1\\), \\(2\\), \\(4\\), \\(8\\), \\(16\\), \\(32\\), \\(64\\), \\(128\\), \\(256\\), \\(512\\), \\(1,024\\), \\(2,048\\), \\(4,096\\), \\(8,192\\), \\(16,384\\), \\(32,768\\), \\(65,536\\), \\(131,072\\),  \\(262,144\\), and so on for place names. (Also, if you were using binary to begin with, those would be \\(1\\), \\(10\\), \\(100\\) and so on.) So the numbers \\(1-10\\) would be \\(|\\), \\(|.\\), \\(||\\), \\(|..\\), \\(|.|\\), \\(||.\\), \\(|||\\), \\(|...\\), \\(|..|\\), \\(|.|.\\) (Period.) But how would you pronounce this?</p> \\[ | = \\text{ One} \\] \\[ |. = \\text{ Two} \\] \\[ |.. = \\text{ Four} \\] \\[ |.... = \\text{ Hex} \\] \\[ |.^{|...} = \\text{ Byte} \\] \\[ |.^{|....} = \\text{ Short} \\] \\[ |.^{|.....} = \\text{ Int} \\] \\[ |.^{|......} = \\text{ Long} \\] \\[ |.^{|.......} = \\text{ Overlong} \\] \\[ \\text{And finally,} \\] \\[ |.^{|........} = |.^{Byte} = \\text{ Byteplex or sha (for sha } 256 \\text{)} \\] \\[ \\text{And then, for the other powers of two, combine the names, so } |... = |.. \\text{ } \\cdot \\text{ } |. = \\text{ Four } + \\text{ Two } = \\text{ Four Two. Then, to get things that are other than powers of two, combine the names of all of the powers of two that sum to it, largest to smallest. (Also, } . = \\text{Zero.)} \\] \\[ \\text{So, } |..| = |... + | = \\text{ Four two } + \\text{ One } = \\text{ Four Two One. But the thing is, } ||| \\text{ is also pronounced \"Four Two One\". Okay, so, if something like that happens where the first part of the next digit is smaller than the last part of the previous digit, then... Um...} \\]"},{"location":"brainstorm.html#heres-a-thing-that-i-worked-on-for-hours-but-did-not-want-to-delete","title":"here's a thing that I worked on for hours, but did not want to delete","text":"\\[ (\\frac{D}{Dt} + \\vec{\\nabla}) \\text{ } (\\vec{E} + B) = \\frac{\\rho + \\vec{J}}{c \\epsilon_0} \\] \\[ \\frac{D \\vec{E}}{Dt} + \\vec{\\nabla} \\vec{E} + \\frac{DB}{Dt} + \\vec{\\nabla} B = \\frac{\\rho + \\vec{J}}{c \\epsilon_0} \\] \\[ \\frac{D \\vec{E}}{Dt} + \\vec{\\nabla} \\cdot \\vec{E} + \\vec{\\nabla} \u2227 \\vec{E} + \\frac{DB}{Dt} + \\vec{\\nabla} \\cdot B + \\vec{\\nabla} \u2227 B = \\frac{\\rho + \\vec{J}}{c \\epsilon_0} \\] \\[ \\vec{\\nabla} \\cdot \\vec{E} + \\frac{D \\vec{E}}{Dt} + \\vec{\\nabla} \u2227 B + \\vec{\\nabla} \u2227 \\vec{E} + \\frac{DB}{Dt} + \\vec{\\nabla} \\cdot B = \\frac{\\rho}{c \\epsilon_0} + \\frac{\\vec{J}}{c \\epsilon_0} \\] \\[ \\vec{\\nabla} \\cdot \\vec{E} = \\text{A scalar} \\] \\[ \\frac{D \\vec{E}}{Dt} + \\vec{\\nabla} \u2227 B = \\text{a vector} \\] \\[ \\vec{\\nabla} \u2227 \\vec{E} + \\frac{DB}{Dt} = \\text{a bivector} \\] \\[ \\vec{\\nabla} \\cdot B = \\text{a trivector} \\] \\[ \\text{And on the right hand side:} \\] \\[ \\frac{\\rho}{c \\epsilon_0} = \\text{A scalar} \\] \\[ \\frac{\\vec{J}}{c \\epsilon_0} = \\text{a vector.} \\] \\[ \\vec{\\nabla} \\cdot \\vec{E} = \\frac{\\rho}{c \\epsilon_0} \\] \\[ \\frac{D \\vec{E}}{Dt} + \\vec{\\nabla} \u2227 B = \\frac{\\vec{J}}{c \\epsilon_0} \\] \\[ \\vec{\\nabla} \u2227 \\vec{E} + \\frac{DB}{Dt} = 0 \\] <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>(insert text here)</p> <p>\\(600\\) lines.</p> \\[ \\vec{\\nabla} \\cdot B = 0 \\]"},{"location":"brainstorm.html#ncuomdbee-rr-etphoeory","title":"ncuomdbee rr etphoe#o#r#y","text":"\\[ \\text{This chapter is about writing numbers as infinite dimensional vectors (if the vector components are } 3 \\text{, } 4 \\text{, } 5 \\text{ and then infinite } 0 \\text{'s, than it would be written as } [3 \\text{, } 4 \\text{, } 5 \\text{, } 0 \\text{, } 0 \\text{, } 0...] \\text{), this is how:} \\] \\[ 2025000 = 2^3 \\cdot 3^4 \\cdot 5^5 \\cdot 7^0 \\cdot 11^0 \\cdot 13^0 \\cdot ... \\rightarrow \\vec{2025000} = [3 \\text{, } 4 \\text{, } 5 \\text{, } 0 \\text{, } 0 \\text{, } 0...] \\] \\[ \\text{Before we do vector addition and scalar multiplication, here's some code:} \\] <pre><code>def SmallestPrimeDivisor(n):\n  k = 2\n  while n % k != 0:\n    k += 1\n  return k\n</code></pre> <pre><code>def vec(n):\n  primes = []\n  for i in range(2, n + 1):\n    if SmallestPrimeDivisor(i) = i:\n      primes.append(i)\n  AlmostResult = []\n  for i in range(len(L)):\n    p = l[i]\n    power = 0\n    k = n\n    while k % p = 0:\n      k /= p\n      power += 1\n    AlmostResult.append(power)\n  k = 1\n  while AlmostResult[len(AlmostResult) - k] = [0]:\n    k += 1\n  result = []\n  for i in range(len(AlmostResult) - k + 1):\n    result += AlmostResult[i]\n  return result\n</code></pre> \\[ \\vec{u} + \\vec{v} = \\vec{u \\cdot v} \\] \\[ c \\cdot \\vec{v} = \\vec{v^c} \\]"},{"location":"brainstorm.html#lambda-simplification","title":"lambda simplification","text":"\\[ f(x) = g(x) \u2192 f = g \\] \\[ (f \u2218 g)(x) = f(g(x)) \\] \\[ C(f)(a)(b) = f(b)(a) \\] \\[ T_h (x)(f) = f(x) \\] \\[ T_h = C(I) \\] \\[ I = C \u2218 C \\] \\[ V(a)(b)(c) = c(a)(b) = T_h (b)(c(a)) = T_h (b)(T_h (a)(c)) = T_h (b)(T_h (a)(c)) = B(T_h (b))(T_h (a))(c) \\] \\[ V(a)(b)(c) = B(T_h (b))(T_h (a))(c) \\] \\[ V(a)(b) = B(T_h (b))(T_h (a)) = C(B)(T_h (a))(T_h (b)) = B(C(B)(T_h (a)))(T_h)(b) \\] \\[ V(a)(b) = B(C(B)(T_h (a)))(T_h)(b) \\] \\[ V(a) = B(C(B)(T_h (a)))(T_h) = C(B)(T_h)(C(B)(T_h (a))) = (C(B)(T_h) \u2218 C(B) \u2218 T_h)(a) \\] \\[ V(a) = (C(B)(T_h) \u2218 C(B) \u2218 T_h)(a) \\] \\[ V = (C(B)(T_h) \u2218 C(B) \u2218 T_h) \\] \\[ V = (C(B)(C(I)) \u2218 C(B) \u2218 C(I)) \\] \\[ V = (C(B)(C(C \u2218 C)) \u2218 C(B) \u2218 C(C \u2218 C)) \\] \\[ e(a)(b) = a(b(a)) = B(a)(b)(a) = C(B(a))(a)(b) \\] \\[ e(a)(b) = C(B(a))(a)(b) \\] \\[ e(a) = C(B(a))(a) = (C \u2218 B)(a)(a) \\] \\[ W(f)(x) = f(x)(x) \\] \\[ e(a) = W(C \u2218 B)(a) \\] \\[ e = W(C \u2218 B) \\] \\[ \\text{Different approach!} \\] \\[ e(a)(b) = a(b(a)) = a(T_h (a)(b)) = B(a)(T_h (a))(b) \\] \\[ e(a)(b) = B(a)(T_h (a))(b) \\] \\[ e(a) = B(a)(T_h (a)) = B(B(a))(T_h)(a) = C(B)(T_h)(B(a))(a) = (C(B)(T_h) \u2218 B)(a)(a) = W(C(B)(T_h) \u2218 B)(a) \\] <p>\\(700\\) lines, this page might beat the code repo page and become the new longest page on the website.</p> \\[ e(a) = W(C(B)(T_h) \u2218 B)(a) \\] \\[ e = W(C(B)(T_h) \u2218 B) \\] \\[ e = W(C(B)(C(C \u2218 C)) \u2218 B) \\]"},{"location":"brainstorm.html#unfinished-cogputer","title":"(unfinished) cogputer","text":"<p>finished cogputer</p> $0 \\to $ \\(1 \\to 0\\) \\(2 \\to 1\\) \\(3 \\to 7\\) \\(4 \\to 2\\) $5 \\to $ \\(6 \\to 8\\) $7 \\to $ \\(8 \\to 3\\) \\(9 \\to 14\\) 00 01 02 03 04 05 06 07 08 00 01 02 04 08 16 32 64 03 06 10 24 00 00 00 09 00 00 00 00 20 00 27 00 00 00 00 00 00 81 30 00 00 00 00 00 00 00 00 00 40 00 00 00 00 00 00 00 00 00 50 00 00 00 00 00 00 00 00 00 60 00 00 00 00 00 00 00 00 00 70 00 00 00 00 00 00 00 00 00 80 00 00 00 00 00 00 00 00 00 90 00 00 00 00 00 00 00 00 00"},{"location":"brainstorm.html#logic-to-set-theory","title":"logic to set theory","text":"\\[ \\text{T} \\] \\[ \\text{F} \\] \\[ \\text{F} \u2229 \\text{F} = \\text{F} \\] \\[ \\text{F} \u2229 \\text{T} = \\text{F} \\] \\[ \\text{T} \u2229 \\text{F} = \\text{F} \\] \\[ \\text{T} \u2229 \\text{T} = \\text{T} \\] \\[ p \u2229 q \u2229 r = p \u2229 (q \u2229 r) \\] \\[ \\text{F} \u2228 \\text{F} = \\text{F} \\] \\[ \\text{F} \u2228 \\text{T} = \\text{T} \\] \\[ \\text{T} \u2228 \\text{F} = \\text{T} \\] \\[ \\text{T} \u2228 \\text{T} = \\text{T} \\] \\[ p \u2228 q \u2228 r = p \u2228 (q \u2228 r) \\] \\[ \u00ac \\text{F} = \\text{T} \\] \\[ \u00ac \\text{T} = \\text{F} \\] \\[ \\text{F} \u2192 \\text{F} = \\text{T} \\] \\[ \\text{F} \u2192 \\text{T} = \\text{T} \\] \\[ \\text{T} \u2192 \\text{F} = \\text{F} \\] \\[ \\text{T} \u2192 \\text{T} = \\text{T} \\] \\[ (p = q = r) = (p = q) \u2229 (q = r) \\] \\[ (p \\iff q) = (p = q) = (p \u2192 q) \u2229 (q \u2192 p) \\] \\[ \\text{Order of operations: } \u00ac &gt; \u2229 &gt; \u2228 &gt; = &gt; \u2192 &gt; \\iff \\text{.} \\] \\[ \\text{E} (x; p(x)) = \\text{ The extension of } p(x) \\text{, the set of all } x \\text{ such that } p(x) \\text{ (is true) (the function } p(x) \\text{ inputs, well, anything, and outputs a boolean (true or false) (e.g. is } x \\text{ an odd number?)), and this is how I'm going to define sets.} \\] \\[ \\text{The extension of that particular function is the set of all odd numbers.} \\] \\[ x \\in \\text{E} (y; p(y)) \\iff p(x) \\text{ (} = \\text{T)} \\] <p>\\(777\\) Lines.</p> \\[ \u2200(x) \\cdot p(x) \\text{ (} = \\text{T)}: q(x) \\text{ (} = \\text{T)} \\iff p(x) \u2192 q(x) \\] \\[ \\exists (x) \\cdot p(x) \\text{ (} = \\text{T)} \\iff \u00ac(p(x) = \\text{F}) \\] \\[ 0 = \u00d8 = \\text{E} (x; \\text{F}) \\] \\[ \\text{succ} (x) = \\text{E} (y; y = x) \\] \\[ \\text{succ} (0) = 1 \\] \\[ \\text{succ} (1) = 2 \\] \\[ \\text{succ} (2) = 3 \\] \\[ 0 \\in \u2115 \\] \\[ x \\in \u2115 \u2192 \\text{succ} (x) \\in \u2115 \\] \\[ \u2115 = \\text{E} (x; (x = 0) \u2228 \\exists (y) \\cdot (\\text{succ} (y) = x) \u2229 y \\in \u2115) = \\text{E} (x; (x = 0) \u2228 \u00ac(((\\text{succ} (y) = x) \u2229 y \\in \u2115) = \\text{F})) \\] <p>\\(800\\) Lines.</p> \\[ R = \\text{E} (x; \u00ac(x \\in x)) \\] \\[ \\text{Now, the question is, is } R \\in R \\text{? Because if } \u00ac(R \\in R) \\text{, than } \u00ac(x \\in x) \\text{ would be true (for } x \\text{ equal to } R \\text{), but then, } R \\text{ would be an element of } R \\text{, but if } R \\in R \\text{, than } \u00ac(x \\in x) \\text{ would be false (for } x \\text{ equal to } R \\text{), but then, } R \\text{ wouldn't be an element of } R \\text{, paradox! (Actually, one of the most popular paradoxes, russell's paradox.)} \\] \\[ x \\in \\text{E} (y; p(y)) = \\text{N (} \\ne \u2115 \\text{)} \\iff (\u00ac(x \\in \\text{E} (y; p(y))) \u2192 p(x) = \\text{T}) \u2229 ((x \\in \\text{E} (y; p(y))) \u2192 p(x) = \\text{F}) \\] \\[ R \\in R = \\text{N} \\] \\[ p(a, b, c,...) = p(a) \u2229 (p(b) \u2229 (p(c) \u2229 (... \\] \\[ x1, x2, x3, x4, x5,... \\in \\text{E} (x; \\text{T}) \\] \\[ x1 = x \\] \\[ x2 = y \\] \\[ x3 = z \\] \\[ a, b \\in \u2115 \\] \\[ \\overline{d} \\]"},{"location":"brainstorm.html#projective-geometry","title":"projective geometry","text":"<p>Credit (even if it is very small): The two points that lie on every circle (???) #SoME3, Putting Algebraic Curves in Perspective, and Extraordinary Conics: The Most Difficult Math Problem I Ever Solved.</p> <p>Yes, I know, the last one was added literally \\(4\\) days ago, but I thougt of something else to talk about.</p> <p>Let's say that a point \\((a: b)\\) (as opposed to \\((a, b)\\)) is equal to \\((ca: cb)\\) \\((c \\ne 0)\\), so every* (and that's a big asterisk) point \\((a: b)\\) can be scaled onto \\((\\frac{a}{b}: 1)\\), a kind of number line.</p> <p>*unless \\(b = 0\\), then we add this kind of \"point at infinity\" to our number line (it's a single point because the point \\((a: 0)\\) can be scaled to \\((1: 0)\\) (aka the point at infinity), that is, of coarse, unless \\(a = 0\\), but that point isn't really aloud for the same reason as \\(\\frac{0}{0}\\)) making it the real projective line or \\(\u211d \\text{P}^1\\).</p> <p>The reason why it's at infinity is because, if you consider the point \\((1: 1)\\), it falls onto \\(1\\) on the number line, the point \\((1: \\frac{1}{2})\\) falls onto \\(2\\), the point \\((1: \\frac{1}{4})\\) falls onto \\(4\\), the point \\((1: \\frac{1}{8})\\) falls onto \\(8\\), and as the second number gets smaller, the point on the number line gets bigger approaching infinity, hence the name \"point at infinity\". But, if you instead do this from the other direction, it approaches negative infinity. You can imagine a number line that curves down as it goes along, consecutive integers getting closer and closer, and an unsigned infinity at the bottom where the line meets itself.</p> <p>Stepping a dimension up, you get the real projective plane or \\(\u211d \\text{P}^2\\), \\((a: b: c) = (da: db: dc)\\), most numbers going to \\((\\frac{a}{c}: \\frac{b}{c}: 1)\\), some becoming \\((\\frac{a}{b}: 1: 0)\\), less becoming \\((1: 0: 0)\\), the point at infinity becomes a line at infinity (more of a circle, but \\(1\\) degree of freedom, so it's a line), and the number line becomes a space of all points.</p> <p>There is a problem though (that is big enough to be explained on a line by itself), you could imagine the same process that I used to prove the unsigned infinity thing but in \\(\u211d \\text{P}^2\\) to get the unsurprising result of \\((a: b: 0) = (-a: -b: 0)\\). This does mean that, when drawing the regular or affine plane, and drawing a circle around it (to represent the line at infinity of coarse), if you wanted to draw, say, the point \\((1: 1: 0)\\), it would need to be at both the very top right, and the very bottom left of the circle.</p> <p>To see why this double counting thing makes sense, I'll project onto a unit sphere, so, if \\(r = \\sqrt{a^2 + b^2 + c^2}\\), the point \\((a: b: c)\\) maps to \\((\\frac{a}{r}: \\frac{b}{r}: \\frac{c}{r})\\). You might see the problem though, it also maps to \\((-\\frac{a}{r}: -\\frac{b}{r}: -\\frac{c}{r})\\), because it's also on the unit sphere. So, if you just consider the top half of the sphere (including the equator so that points at infinity are accounted for), it counts almost every point once, and points at infinity twice, kinda like the one where we projected onto the plane parallel to and one unit above the \\(xy\\) plane. So, to fix this problem, and give every point the same treatment, you (counterintuitively) count every point twice by using the entire sphere, kinda like giving every line in \\(2d\\) an angle instead of a slope to fix the vertical lines problem, at the cost of there being two angles for every line.</p> <p>Here's a desmos graph.</p> <p>Yes, I know, the plane is placed one unit below the sphere instead of one above, but it's only like that for the sake of demonstration.</p>"},{"location":"brainstorm.html#duality","title":"duality","text":"<p>It's hard to explain how points are dual to lines, but an example would be the origin and the line at infinity, or on the sphere, the equator and the north and south poles (remember, two solutions). The more general definition would be something like this: the two points on a sphere, a point on the dual line, and the point \\(90\u00b0\\) away but still on the dual line are all mutually perpendicular. By the way, points on the plane project to antipodal points on the sphere, and lines on the plane project to great circles on the sphere.</p> <p>Also fun fact: the duals of every point on a line would all pass through the dual point, and the duals of every line that passes through a point would all lie on the dual line.</p>"},{"location":"brainstorm.html#linear-algebra-complex-numbers-and-higher-dimensional-complex-numbers","title":"linear algebra, complex numbers, and higher dimensional complex numbers?","text":"\\[ \\text{Let's say that a vector } \\vec{v} \\text{ is an ordered set of numbers } \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix} \\text{, or } v_x \\hat{x} + v_y \\hat{y} \\text{, or a point } (v_x, v_y) \\text{, or an arrow with it's tip at } (v_x, v_y) \\text{, and tail at } (0, 0) \\text{. They can be scaled (that is, multiplied by a scalar or real number) (} c \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix} = c(v_x \\hat{x} + v_y \\hat{y} = c v_x \\hat{x} + c v_y \\hat{y}) = \\begin{bmatrix} c v_x \\\\ c v_y \\\\ \\end{bmatrix} \\text{), added (} \\begin{bmatrix} u_x \\\\ u_y \\\\ \\end{bmatrix} + \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix} = u_x \\hat{x} + u_y \\hat{y} + v_x \\hat{x} + v_y \\hat{y} = (u_x + v_x) \\hat{x} + (u_y + v_y) \\hat{y} = \\begin{bmatrix} u_x + v_x \\\\ u_y + v_y \\\\ \\end{bmatrix} \\text{), and with just those two, any vector can be made from } \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\text{ and } \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} \\text{ (that is, } 1 \\hat{x} + 0 \\hat{y} \\text{ and } 0 \\hat{x} + 1 \\hat{y} \\text{ (that is, } \\hat{x} \\text{ and } \\hat{y} \\text{)), and that's pretty much it.} \\] <p>You can also multiply vectors, but that's a story for another day.</p> \\[ \\text{Let's say that a matrix } A \\text{ is also an ordered set of numbers arranged in a square: } \\begin{bmatrix} a_{11} &amp; a_{21} \\\\ a_{12} &amp; a_{22} \\\\ \\end{bmatrix} \\text{ that denotes a linear transformation (every linear transformation has a corrasponding matrix, and every matrix has a corrasponding linear transformation), a type of function that, among other things, acts on vectors. ( A matrix } A \\text{ applied to a vector } \\vec{v} \\text{ is denoted as }  A \\vec{v} \\text{ btw.) Matrix operations are linear (the linear part of linear algebra), that is, } A(c \\vec{v}) = c(A \\vec{v}) \\text{, and } A(\\vec{u} + \\vec{v}) = A \\vec{u} + A \\vec{v} \\text{. So all you need to describe a linear transformation is how it affects the basis vectors (} \\hat{x} \\text{ and } \\hat{y} \\text{), thankfully, it's right there in the columns, } \\begin{bmatrix} a_{11} &amp; a_{21} \\\\ a_{12} &amp; a_{22} \\\\ \\end{bmatrix} \\hat{x} = \\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ \\end{bmatrix} \\text{, and } \\begin{bmatrix} a_{11} &amp; a_{21} \\\\ a_{12} &amp; a_{22} \\\\ \\end{bmatrix} \\hat{y} = \\begin{bmatrix} a_{21} \\\\ a_{22} \\\\ \\end{bmatrix} \\text{.} \\] \\[ \\text{Now, I can derive a formula for matrix vector multiplication!} \\] \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} v_x \\\\ v_x \\\\ \\end{bmatrix} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} (v_x \\hat{x} + v_y \\hat{y}) = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} (v_x \\hat{x}) + \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} (v_y \\hat{y}) = v_x \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\hat{x} + v_y \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\hat{y} = v_x \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + v_y \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix} \\hat{y} = \\begin{bmatrix} v_x a \\\\ v_x c \\\\ \\end{bmatrix} + \\begin{bmatrix} v_y b \\\\ v_y d \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix} = \\begin{bmatrix} v_x a + v_y b \\\\ v_x c + v_y d \\\\ \\end{bmatrix} \\] \\[ \\text{Next, matrix multiplication!} \\] \\[ \\text{I'll define } AB \\text{ like this: } (AB) \\vec{v} = A(B \\vec{v}) \\text{, and I'll derive the formula like this:} \\] \\[ \\text{Actually, too hard.} \\] <p>\\(900\\) Lines.</p> \\[ \\text{And if you were wondering, this also works in } 3d \\text{ or higher, with } \\hat{z} \\text{, } \\hat{w} \\text{, and so on.} \\] \\[ \\text{Next, complex numbers!} \\] <p>Complex numbers are, if I'm gonna quote Morphocular in this video, the language of \\(2d\\) rotation. I'll describe them in an unusual way:</p> \\[ \\text{Let's say that I want to rotate a vector } \\vec{v} \\text{ or } \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix} \\text{ by an angle } \\theta \\text{ in the plane, well, there's only one plane, and from } \\hat{x} \\text{ to } \\hat{y} \\text{, the result is } \\begin{bmatrix} v_x cos(\\theta) - v_y sin(\\theta) \\\\ v_x sin(\\theta) + v_y cos(\\theta) \\\\ \\end{bmatrix} \\text{, which happens to equal } \\begin{bmatrix} cos(\\theta) &amp; -sin(\\theta) \\\\ sin(\\theta) &amp; cos(\\theta) \\\\ \\end{bmatrix} \\vec{v} \\text{.} \\] \\[ \\text{I'll take a simple rotation like } \\theta = 90\u00b0 \\text{, and give it a name: } i \\text{ (yes, complex numbers } i \\text{), so } i = \\begin{bmatrix} cos(90\u00b0) &amp; - sin(90\u00b0) \\\\ sin(90\u00b0) &amp; cos(90\u00b0) \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\text{, let's see what happens if we square } i \\] \\[ i^2 \\vec{v} = (ii) \\vec{v} = i(i \\vec{v}) = i(i \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix}) = i \\begin{bmatrix} -v_y \\\\ v_x \\\\ \\end{bmatrix} = \\begin{bmatrix} -v_x \\\\ -v_y \\\\ \\end{bmatrix} \\] \\[ i^2 \\vec{v} = -\\vec{v} \\] \\[ i^2 = -1 ?!?!?! \\] \\[ \\text{Ok, you might have seen that one coming.} \\] \\[ \\text{Time for euler's identity!} \\] \\[ e^{i \\theta} = cos(\\theta) + isin(\\theta) \\] \\[ \\text{I know what you're thinking: Why? And how? Well, first, if you have two matrices } A \\text{ and } B \\text{, } A + B \\text{ is equal to the termwise sum of } A \\text{ and } B \\text{. And, you'll have to trust me on this, but } (A + B) \\vec{v} = A \\vec{v} + B \\vec{v} \\text{. And second, the matrix } \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\text{ (aka } I \\text{) is equal to } 1 \\text{ (} 1 \\text{ times anything is that thing, and } I \\text{ times anything is that thing, hence, } I = 1 \\text{). Now time to figure out what } e^{i \\theta} \\text{ is.} \\] \\[ cos(\\theta) + isin(\\theta) = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} cos(\\theta) + \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} sin(\\theta) = \\begin{bmatrix} cos(\\theta) &amp; 0 \\\\ 0 &amp; cos(\\theta) \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 &amp; -sin(\\theta) \\\\ sin(\\theta) &amp; 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} cos(\\theta) &amp; -sin(\\theta) \\\\ sin(\\theta) &amp; cos(\\theta) \\\\ \\end{bmatrix} \\] \\[ e^{i \\theta} = \\begin{bmatrix} cos(\\theta) &amp; -sin(\\theta) \\\\ sin(\\theta) &amp; cos(\\theta) \\\\ \\end{bmatrix} \\] \\[ \\text{So, if you want to rotate a vector } \\vec{v} \\text{ with an angle } \\theta \\text{ from } \\hat{x} \\text{ to } \\hat{y} \\text{, it's just } e^{i \\theta} \\vec{v} \\text{!} \\] \\[ \\text{And this would technically also work in } 3d \\text{ or higher. You could say that } i = \\begin{bmatrix} 0 &amp; -1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\text{, } j = \\begin{bmatrix} 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\text{, } k = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\text{, then you would have } e^{i \\theta} \\text{, } e^{j \\theta} \\text{, and } e^{k \\theta} \\text{ for the } xy \\text{, } xz \\text{, and } yz \\text{ planes respectively.} \\] <p>If you want a better rotation formula, here it is!</p>"},{"location":"brainstorm.html#homogenization","title":"homogenization","text":"<p>Homogenization is a method of interpolation from equations on the affine plane (non-projective plane) to equations on the projective plane (so, adding the line at infinity), but I think it would be better if I just showed how to do it.</p> <p>Let's say that I have these equations for describing my line:</p> \\[ y = mx + b \\] \\[ z = 1 \\] <p>so, we have this equation:</p> \\[ (x: y: z) = (x: mx + b: 1) \\] \\[ (x, y, z) = (cx, cmx + cb, c) \\] <p>and from those, I have this new equation for describing my line:</p> \\[ y = mx + bz \\] <p>Now, the equation is homogeneous \\(^1\\).</p> <p>In (this \\((x, y, z) = (cx, cmx + cb, c)\\)) equation for a line, \\(z\\) could not equal \\(0\\), but now, \\(z\\) can equal \\(0\\), and if \\(z = 0\\), then it's at the line at infinity, so these \\(z = 0\\) solutions snuck in as a result of homogenization, mission success!</p> <p>\\(^1\\) That is, a polynomial where each term has the same degree. There is a much easier way of doing this called homogenization: you take each term whose degree is not the max, and add factors of \\(z\\) to bring the degree up to the max.</p> <p>But what are these solutions at the line at infinity?</p> \\[ z = 0 \\] \\[ y = mx + bz \\] \\[ y = mx \\] \\[ (x: y: z) = (x: mx: 0) \\] \\[ (1: m: 0) \\] <p>This has some pretty cool implications, but I'll do that tomorrow.</p> <p>Oh, look, it's tomorrow, time to tell you the implications.</p> <p>\\(1000\\) Lines, wow.</p> <p>Y'know how any two distinct points on the affine plane have a line through them? And how (almost) any two distinct lines on the affine plane have a point on both? That is, of course, unless the lines are parallel. Solution: homogenization. A homogenized line with slope \\(m\\) has the point \\((1: m: 0)\\) (and \\((0: 1: 0)\\) if the line is vertical). So, if two lines have the same slope \\(m\\) (and are distinct), then they don't meet normally, and they intersect at \\((1: m: 0)\\). If they have different slopes, then they do meet normally, and they don't intersect at the line at infinity. But what about the \"any two distinct points have a line through them\" rule? If you have a normal point and a point at infinity \\((1: m: 0)\\), they have the line with slope \\(m\\) going through the first one. But what if you have two points on the line at infinity? This (among other things) is why it's called the line at infinity, a line that all points at infinity lie on.</p>"},{"location":"brainstorm.html#greek-letters","title":"greek letters","text":"Alpha Beta Gamma Delta Epsilon Zeta Eta Theta Iota Kappa Lambda Mu Nu Xi Omicron Pi Rho Sigma (yes, actualy) Tau Upsilon Phi Chi Psi Omega \\(\\alpha\\) \\(\\beta\\) \\(\\gamma\\) \\(\\delta\\) \\(\\epsilon\\) \\(\\zeta\\) \\(\\eta\\) \\(\\theta\\) \\(\\iota\\) \\(\\kappa\\) \\(\\lambda\\) \\(\\mu\\) \\(\\nu\\) \\(\\xi\\) \\(\\omicron\\) \\(\\pi\\) \\(\\rho\\) \\(\\sigma\\) \\(\\tau\\) \\(\\upsilon\\) \\(\\phi\\) \\(\\chi\\) \\(\\psi\\) \\(\\omega\\) \\(\\Alpha\\) \\(\\Beta\\) \\(\\Gamma\\) \\(\\Delta\\) \\(\\Epsilon\\) \\(\\Zeta\\) \\(\\Eta\\) \\(\\Theta\\) \\(\\Iota\\) \\(\\Kappa\\) \\(\\Lambda\\) \\(\\Mu\\) \\(\\Nu\\) \\(\\Xi\\) \\(\\Omicron\\) \\(\\Pi\\) \\(\\Rho\\) \\(\\Sigma\\) \\(\\Tau\\) \\(\\Upsilon\\) \\(\\Phi\\) \\(\\Chi\\) \\(\\Psi\\) \\(\\Omega\\) A B E Z H I K M N O P T X"},{"location":"brainstorm.html#cursed-math","title":"cursed math","text":"\\[ \\int i \\text{ } dt = \\iota \\]"},{"location":"brainstorm.html#fixed-point-combinators","title":"fixed point combinators","text":"<p>When I say \"fixed point combinator\", what I really mean is a combinator \\(p\\) such that \\(f(p(f)) = p(f)\\). The term \"fixed point\" just means: \\(x\\) is a fixed point of \\(f\\) if and only if \\(f(x) = x\\). The puzzle of constructing your own fixed point combinator is a puzzle found in the lambda paper after showing you the \\(\\text{Y}\\) combinator and the older turing fixed point combinator \\(\\Theta\\). In both of them, \\(p(f)\\) reduces to \\(f(p(f))\\). Here's a proof:</p> \\[ \\text{Y} = \\lambda f. (\\lambda x. f(x(x)))(\\lambda x. f(x(x))) \\] \\[ \\text{Y} (f) = (\\lambda f. (\\lambda x. f(x(x)))(\\lambda x. f(x(x))))(f) = (\\lambda x. f(x(x)))(\\lambda x. f(x(x))) = f((\\lambda x. f(x(x)))(\\lambda x. f(x(x)))) = f((\\lambda f. (\\lambda x. f(x(x)))(\\lambda x. f(x(x))))(f)) = f(\\text{Y} (f)) \\] \\[ \\text{Y} (f) = f(\\text{Y} (f)) \\] \\[ \\Theta = A(A) \\] \\[ A = \\lambda xy. y(x(x)(y)) \\] \\[ \\Theta (f) = A(A)(f) = (\\lambda xy. y(x(x)(y)))(A)(f) = f(A(A)(f)) = f(\\Theta (f)) \\] \\[ \\Theta (f) = f(\\Theta (f)) \\] <p>But this challenge of making your own fixed point combinator is really easy (I'll use the Theta combinator as an example): first, we need a combinator that reduces to itself, a self referential combinator (such as \\(\\Omega\\) or \\(\\text{M} (\\text{M})\\) or \\((\\lambda x. x(x))(\\lambda x. x(x))\\), they're all the same thing. Actually, I'm gonna re derive The Omega). And for that, we need a form, where a form has some \\(f\\)s, maybe \\(x\\)s and \\(y\\)s where it is \\(f\\) of single things (so no \\(f(x(y))\\)s), one of which is another \\(f\\). The one that I'm gonna use (and the simplest one) is \\(f(f)\\). To make a self referential combinator out of this, we're gonna need to make a combinator \\(A\\) where \\(A(A)\\) reduces to itself. That is, \\(A\\) of all the given inputs (just \\(A\\)) returns \\(A(A)\\). So \\(A\\), if you exaluate it on \\(A\\), you get \\(A(A)\\). So \\(A\\) must be the self application combinator \\(\\lambda x. x(x)\\). To turn this self referential combinator into a fixed point combinator, you just need to make \\(A(A)(f)\\) equal to \\(A(A)(f)\\). But, to avoid confusion with the original \\(A\\) (not the one in the turing fixed point combinator), I'll call it \\(B\\). First, as a starting point, \\(B\\) should equal \\(A\\) but with one more input (so \\(B = \\lambda xy. x(x)(y)\\)). And, now that \\(B\\) can factor in \\(f\\), we can make \\(B\\) of \\(B\\) and \\(f\\) output \\(f(B(B)(f))\\). But this is easy, just change the definition of \\(B\\) to \\(\\lambda xy. y(x(x)(y))\\). And we now have \\(\\Theta\\).</p>"},{"location":"brainstorm.html#linear-systems-of-equations","title":"linear systems of equations","text":"\\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\dots &amp; a_{2n} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\dots &amp; a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\dots &amp; a_{nn} \\\\ \\end{bmatrix} \\] \\[ \\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} \\] \\[ \\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\\\ \\end{bmatrix} \\] \\[ A \\vec{x} = \\vec{v} \\] \\[ \\vec{x} = ? \\] \\[ A \\vec{x} = A \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} = A (x_1 \\hat{x} + x_2 \\hat{y} + x_3 \\hat{z} + \\dots + x_n \\hat{\\omega}) = A x_1 \\hat{x} + A x_2 \\hat{y} + A x_3 \\hat{z} + \\dots + A x_n \\hat{\\omega} = x_1 A \\hat{x} + x_2 A \\hat{y} + x_3 A \\hat{z} + \\dots + x_n A \\hat{\\omega} : = x_1 \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ \\vdots \\\\ a_{n1} \\\\ \\end{bmatrix} + x_2 \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32} \\\\ \\vdots \\\\ a_{n2} \\\\ \\end{bmatrix} + x_3 \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33} \\\\ \\vdots \\\\ a_{n3} \\\\ \\end{bmatrix} + \\dots + x_n \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ a_{3n} \\\\ \\vdots \\\\ a_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 a_{11} \\\\ x_1 a_{21} \\\\ x_1 a_{31} \\\\ \\vdots \\\\ x_1 a_{n1} \\\\ \\end{bmatrix} + \\begin{bmatrix} x_2 a_{12} \\\\ x_2 a_{22} \\\\ x_2 a_{32} \\\\ \\vdots \\\\ x_2 a_{n2} \\\\ \\end{bmatrix} + \\begin{bmatrix} x_3 a_{13} \\\\ x_3 a_{23} \\\\ x_3 a_{33} \\\\ \\vdots \\\\ x_3 a_{n3} \\\\ \\end{bmatrix} + \\dots + \\begin{bmatrix} x_n a_{1n} \\\\ x_n a_{2n} \\\\ x_n a_{3n} \\\\ \\vdots \\\\ x_n a_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 a_{11} + x_2 a_{12} + x_3 a_{13} + \\dots + x_n a_{1n} \\\\ x_1 a_{21} + x_2 a_{22} + x_3 a_{23} + \\dots + x_n a_{2n} \\\\ x_1 a_{31} + x_2 a_{32} + x_3 a_{33} + \\dots + x_n a_{3n} \\\\ \\vdots \\\\ x_1 a_{n1} + x_2 a_{n2} + x_3 a_{n3} + \\dots + x_n a_{nn} \\\\ \\end{bmatrix} \\] <p>\\(1102\\) Lines.</p> \\[ \\begin{bmatrix} x_1 a_{11} + x_2 a_{12} + x_3 a_{13} + \\dots + x_n a_{1n} \\\\ x_1 a_{21} + x_2 a_{22} + x_3 a_{23} + \\dots + x_n a_{2n} \\\\ x_1 a_{31} + x_2 a_{32} + x_3 a_{33} + \\dots + x_n a_{3n} \\\\ \\vdots \\\\ x_1 a_{n1} + x_2 a_{n2} + x_3 a_{n3} + \\dots + x_n a_{nn} \\\\ \\end{bmatrix} = \\vec{v} \\] \\[ \\begin{bmatrix} x_1 a_{11} + x_2 a_{12} + x_3 a_{13} + \\dots + x_n a_{1n} \\\\ x_1 a_{21} + x_2 a_{22} + x_3 a_{23} + \\dots + x_n a_{2n} \\\\ x_1 a_{31} + x_2 a_{32} + x_3 a_{33} + \\dots + x_n a_{3n} \\\\ \\vdots \\\\ x_1 a_{n1} + x_2 a_{n2} + x_3 a_{n3} + \\dots + x_n a_{nn} \\\\ \\end{bmatrix} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\\\ \\end{bmatrix} \\] \\[ x_1 a_{11} + x_2 a_{12} + x_3 a_{13} + \\dots + x_n a_{1n} = v_1 \\] \\[ x_1 a_{21} + x_2 a_{22} + x_3 a_{23} + \\dots + x_n a_{2n} = v_2 \\] \\[ x_1 a_{31} + x_2 a_{32} + x_3 a_{33} + \\dots + x_n a_{3n} = v_3 \\] \\[ \\vdots \\] \\[ x_1 a_{n1} + x_2 a_{n2} + x_3 a_{n3} + \\dots + x_n a_{nn} = v_n \\]"},{"location":"brainstorm.html#linear-algebra","title":"linear algebra","text":"<p>This will be a series of subchapters about linear algebra. In particular, the more general mathmetician's version. But if you want some more intuition about how it works, each subchapter will have a corrasponding part in this playlist. The first thing to do in linear algebra is to...</p>"},{"location":"brainstorm.html#part-1-choose-your-fighte-vector-space","title":"part \\(1\\): choose your fighte- vector space!","text":"<p>As you might know, the main thing in linear algebra is the vector. so, to make this as general as possible, I'm gonna let you make your own vector space (space in which vectors live). Something important that defines a vector is that there's a sense of vector \\(\\vec{u} + \\vec{v}\\) (for vectors \\(\\vec{u}\\) and \\(\\vec{v}\\)) and there's a sense of vector \\(c \\vec{v}\\) (for vector \\(\\vec{v}\\) and scalar (real number) \\(c\\)). But, you cannot (necessarily) multiply vectors or add vectors and scalars (unless you use geometric algebra). But, for something to quallify as a vector space, there are some more rules/axioms it has to follow: (assume that your vector space is denoted as \\(\\text{V}\\) with vectors \\(\\vec{u}\\), \\(\\vec{v}\\), \\(\\vec{w}\\) and scalars \\(x\\), \\(y\\), and \\(z\\))</p> <p>Rule #\\(1\\):</p> <p>\\(\\vec{u} + (\\vec{v} + \\vec{w}) = (\\vec{u} + \\vec{v}) + \\vec{w}\\)</p> <p>Rule #\\(2\\):</p> <p>\\(\\vec{u} + \\vec{v} = \\vec{v} + \\vec{u}\\)</p> <p>Rule #\\(3\\) with words:</p> <p>There is a vector \\(\\vec{0}\\) aka \"the zero vector\" such that \\(\\vec{v} + \\vec{0} = \\vec{v}\\) for all \\(\\vec{v}\\)</p> <p>Rule #\\(3\\) with set theory:</p> <p>\\(\\exists \\vec{0} \\in \\text{V}. \u2200 \\vec{v} \\in \\text{V}. \\vec{v} + \\vec{0} = \\vec{v}\\)</p> <p>Rule #\\(4\\) with words:</p> <p>For any \\(\\vec{v}\\) there is a \\(-\\vec{v}\\) such that \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\) for all \\(\\vec{v}\\)</p> <p>Rule #\\(4\\) with set theory:</p> <p>\\(\u2200 \\vec{v} \\in \\text{V}. \\exists -\\vec{v} \\in \\text{V}. \\vec{v} + (-\\vec{v}) = \\vec{0}\\)</p> <p>Rule #\\(5\\):</p> <p>\\(x(y \\vec{v}) = (xy) \\vec{v}\\)</p> <p>Rule #\\(6\\):</p> <p>\\(1 \\vec{v} = \\vec{v}\\)</p> <p>Rule #\\(7\\):</p> <p>\\(x(\\vec{u} + \\vec{v}) = x \\vec{u} + x \\vec{v}\\)</p> <p>Rule #\\(8\\):</p> <p>\\((x + y) \\vec{v} = x \\vec{v} + y \\vec{v}\\)</p> <p>The vector space that (at least to me) makes all of the intuition click is arrows in space where it's the same if it has the same length and direction (hence the little arrow over every vector). The result of adding two of them is putting the base of the second on the tip of the first and drawing a new arrow from the base of the first to the tip of the second. The result of multiplying one of these by a number is scaling the length by a factor of the number (hence the name) and flipping the vector and scaling the length by a factor of the absolute value of the number if it is negative. You can convince yourself that this is a vector space. Also, these sorts of vectors are usually rooted at the origin.</p> <p>another commonly used definition of a vector is that of lists of numbers. The result of adding two of them is adding them term by term and the result of multiplying one of these by a number is multiplying each term by said number. You can convince yourself that this is a vector space.</p> <p>You can convert from the first definition to the second by making a list of the vector's coordinates and doing the opposite to convert from a list of numbers to an arrow.</p> <p>Now that you have chosen a vector space, we can now move on to...</p>"},{"location":"brainstorm.html#part-2-linear-combinations-span-and-basis-vectors","title":"part \\(2\\): linear combinations, span, and basis vectors","text":"<p>In \\(2d\\) (arrows of length \\(1\\) or list of two numbers) there are vectors that will prove to be very important. The first being called \\(\\hat{x}\\) (x hat), the unit vector pointing to the right (in the direction in the \\(x\\) axis) or the list of numbers \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix}\\) (vectors that are of length \\(1\\) are denoted with a hat) and \\(\\hat{y}\\) (y hat) the unit vector pointing up (in the direction in the \\(y\\) axis). AKA the list of numbers \\(\\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}\\).</p> <p>If you think about it, any \\(2d\\) vector \\(\\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix}\\) can be written in terms of \\(\\hat{x}\\) and \\(\\hat{y}\\) (i.e. \\(x \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} x \\\\ 0 \\\\ \\end{bmatrix}\\), \\(y \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ y \\\\ \\end{bmatrix}\\), \\(\\begin{bmatrix} x \\\\ 0 \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ y \\\\ \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix}\\)).</p> <p>\\(1200\\) Lines.</p> <p>Alternatively (and you might need a grid for this), you can take the unit vector in the x direction and scale it until it's tip is on the same vertical line as the tip of your vector and the same thing with the y direction. And, when you add them up, (you can deduce that) you get your original vector. This can be used as another way to go back and forth between the two definitions of a vector.</p> <p>By the way, this is called a linear combination of \\(\\hat{x}\\) and \\(\\hat{y}\\) (linear because if you fix one and vary the other, it traces out a line if you look at the tip of the result).</p> <p>For this reason that every \\(2d\\) vector can be made out of \\(\\hat{x}\\) and \\(\\hat{y}\\), they are called the basis vectors.</p> <p>Also, Every Vector that can be formed by adding and scaling \\(\\hat{x}\\), $\\hat{y}, and any other vector formed in this way is of the form \\(a \\hat{x} + b \\hat{y}\\), and the reason why is because \\((a \\hat{x} + b \\hat{y}) + c \\hat{x} + d \\hat{y} = (a + c) \\hat{x} + (b + d) \\hat{y}\\) and \\(c(a \\hat{x} + b \\hat{y}) = (a) \\hat{x} + (cb) \\hat{y}\\).</p> <p>Also by the way, \\(a \\vec{u} + b \\vec{v}\\) is called a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\).</p> <p>But this begs the question: we could've used any other two basis vectors and we would've gotten another completely sensible way of going back and forth between the two definitions of a vector. That is, of course, unless the two vectors that are aligned with each other (or are both the zero vector).</p> <p>By the way, the set of all the vectors that can be made with a linear combination of two vectors is called the span of those two vectors. This idea of using different basis vectors, aka a different basis, is something that I'll go much more in detail about later.</p> <p>Also, if you have just one vector, think of it as an arrow, but if you have many vectors, think of each of them as a point where the point lies at the tip of the vector.</p> <p>But, things get more interesting in \\(3d\\), now it's \\(a \\vec{u} + b \\vec{v} + c \\vec{w}\\) for scalars \\(a\\), \\(b\\), \\(c\\). And if the third is in the span of the other two, it doesn't change the span and it's still a flat sheet cutting through the origin.</p> <p>You can imagine the first two forming a plane and then the third one moving the plane around sweeping it through space. Another intuition is that you're using all three scalars to your advantage, you can't replace one of them with the other two.</p> <p>Whenever you can remove a vector without changing its span it is also known as linearly dependent, but \\(\\hat{z}\\) signed the declaration of independence ~\\(250\\) years ago, so they span all of \\(3d\\) space.</p> <p>So, the more formal definition of a basis is a set of linearly independent vectors that span all of space.</p>"},{"location":"brainstorm.html#part-3-matrices-and-linear-transformations","title":"part \\(3\\): matrices and linear transformations","text":"<p>Let's start off this part with a quote:</p> <p>\\(1234\\) lines.</p> <p>No one really understands The Matrix, you just have to see for yourself</p> <p>-Morpheus</p> <p>Jokes Aside, for this part I'm going to be talking about linear transformations. Transformation is just a fancy word for function (In this context, it's a function that inputs and outputs vectors), but what makes it linear is that it preserves the two operations of vector addition and scalar multiplication, that is, \\(L(\\vec{u} + \\vec{v}) = L(\\vec{u}) + L(\\vec{v})\\) and \\(L(c \\vec{v}) = c L(\\vec{v})\\) (I'll explain why the word linear is used later).</p> <p>But, if you were given one of these guys, how would you describe it numerically? What is \\(L(\\vec{v})\\)?</p> <p>Well, describe \\(\\vec{v}\\) as a linear combination of \\(\\hat{x}\\) and \\(\\hat{y}\\), so \\(v_x \\hat{x} + v_y \\hat{y}\\)</p> \\[ L(\\vec{v}) = L(v_x \\hat{x} + v_y \\hat{y}) = L(v_x \\hat{x}) + L(v_y \\hat{y}) = v_x L(\\hat{x}) + v_y L(\\hat{y}) \\] <p>This is why it's called a linear transformation, \\(L(\\vec{v})\\) is a linear combination of \\(L(\\hat{x})\\) and \\(L(\\hat{y})\\)</p> <p>So, literally all you need to define a (\\(2d\\)) linear transformation is where \\(\\hat{x}\\) and \\(\\hat{y}\\) each go.</p> <p>Here's a concrete example: let's say that the transformation applied to \\(\\hat{x}\\) is \\(\\begin{bmatrix} 1 \\\\ -2 \\\\ \\end{bmatrix}\\) and the transformation applied to \\(\\hat{y}\\) is \\(\\begin{bmatrix} 3 \\\\ 0 \\\\ \\end{bmatrix}\\), then the transformation applied to \\(-1 \\hat{x} + 2 \\hat{y}\\) should be \\(-1 \\begin{bmatrix} 1 \\\\ -2 \\\\ \\end{bmatrix} + 2 \\begin{bmatrix} 3 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} (-1)(1) + (2)(3) \\\\ (-1)(-2) + (2)(0) \\\\ \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 2 \\\\ \\end{bmatrix}\\)</p> <p>Ok, got all that?</p> <p>In general, this transformation applied to \\(\\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix}\\) is \\(\\begin{bmatrix} 1x + 3y \\\\ -2x + 0y\\). You give me any vector and I tell you the output vector.</p> <p>What I'm saying is that the linear transformation \\(L\\) is completely determined by four numbers: the \\(x\\) coordinate of the transformed \\(\\hat{x}\\), the y coordinate of the transformed \\(\\hat{x}\\), the \\(x\\) coordinate of the transformed \\(\\hat{y}\\), and the y coordinate of the transformed \\(\\hat{y}\\).</p> <p>Usually how you write a linear transformation is with a \\(2x2\\) group of numbers, also called a called a \\(2x2\\) matrix. You can read off the first column as where \\(\\hat{x}\\) goes and the second as where \\(\\hat{y}\\) goes.</p> <p>By the way, a matrix \\(A\\) times a vector \\(\\vec{v}\\)</p> <p>If you're given a matrix describing a linear transformation and you're also given some specific vector and you want to compute the linear transformation evaluated on said vector, you multiply the coordinates of the vector by the columns of the matrix and adding up the results.</p> <p>Here's a concrete example:</p> \\[ \\begin{bmatrix} 3 &amp; 2 \\\\ -2 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 7 \\\\ \\end{bmatrix} = 5 \\begin{bmatrix} 3 \\\\ -2 \\\\ \\end{bmatrix} + 7 \\begin{bmatrix} 2 \\\\ 1 \\\\ \\end{bmatrix} = 5 \\begin{bmatrix} 3 \\\\ -2 \\\\ \\end{bmatrix} + 7 \\begin{bmatrix} 2 \\\\ 1 \\\\ \\end{bmatrix} = \\dots = \\begin{bmatrix} 29 \\\\ -3 \\\\ \\end{bmatrix} \\] <p>What about the most general possible example of matrix vector multiplication:</p> \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} = x \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + y \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix} = \\begin{bmatrix} ax \\\\ cx \\\\ \\end{bmatrix} + \\begin{bmatrix} by \\\\ dy \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} = \\begin{bmatrix} ax + by \\\\ cx + dy \\\\ \\end{bmatrix} \\] <p>\\(1300\\) Lines.</p> <p>You could even use this formula as a definition. And then you could teach it to high schoolers worldwide and not teach them the key intuition that makes it intuitive (\\(x \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + y \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix}\\))</p> <p>Isn't it better to think of the columns of the matrix as where \\(\\hat{x}\\) and \\(\\hat{y}\\) each go and the result of multiplying a matrix by a vector as the appropriate linear combination?</p> <p>How would you describe a linear transformation like a 90\u00b0 counterclockwise rotation? (Yes, that is a linear transformation.) Well, \\(\\hat{x}\\) gets shifted up towards \\(\\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}\\) (\\(\\hat{y}\\)) and \\(\\hat{y}\\) gets rotated down towards \\(\\begin{bmatrix} -1 \\\\ 0 \\\\ \\end{bmatrix}\\) (\\(-\\hat{x}\\)). So the result should be the matrix \\(\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0\\), and if you want to rotate any vector clockwise by 90 degrees, just multiply it by the matrix \\(\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0\\).</p> <p>On the other hand, if the two columns are linearly dependent, the transformation squishes all of space onto one line, the span of the two linearly dependent columns.</p> <p>Summary:</p> <p>linear transformations are those that preserve the operations of vector addition and scalar multiplication, of which you can think of as transformations of space that keep the grid lines parallel and evenly spaced with the origin remaining fixed. But to describe your linear transformation, you only need a handful of numbers: the coordinates of where the basis vectors land. matrices give us a language for linear transformations: just read off the columns and you'll know where the basis vectors land. And matrix vector multiplication just tells you what the linear transformation does to a given vector.</p>"},{"location":"brainstorm.html#linear-systems-of-equations-but-with-sum-notation","title":"linear systems of equations (but with sum notation)","text":"\\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\dots &amp; a_{1k} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\dots &amp; a_{2k} &amp; \\dots &amp; a_{2n} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\dots &amp; a_{3k} &amp; \\dots &amp; a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp;  &amp; \\vdots \\\\ a_{k1} &amp; a_{k2} &amp; a_{k3} &amp; \\dots &amp; a_{kk} &amp; \\dots &amp; a_{kn} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\dots &amp; a_{nk} &amp; \\dots &amp; a_{nn} \\\\ \\end{bmatrix} \\] \\[ \\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_k \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} \\] \\[ \\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_k \\\\ \\vdots \\\\ v_n \\\\ \\end{bmatrix} \\] \\[ A \\vec{x} = \\vec{v} \\] \\[ \\vec{x} = ? \\] \\[ A \\vec{x} = A \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_k \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} = A(\\sum\\limits_{k = 1}^{n} x_k e_k) = \\sum\\limits_{k = 1}^{n} A x_k e_k = \\sum\\limits_{k = 1}^{n} x_k A e_k : = \\sum\\limits_{k = 1}^{n} x_k \\begin{bmatrix} a_{1k} \\\\ a_{2k} \\\\ a_{3k} \\\\ \\vdots \\\\ a_{mk} \\\\ \\vdots \\\\ a_{nk} \\\\ \\end{bmatrix} = \\sum\\limits_{k = 1}^{n} \\begin{bmatrix} x_k a_{1k} \\\\ x_k a_{2k} \\\\ x_k a_{3k} \\\\ \\vdots \\\\ x_k a_{mk} \\\\ \\vdots \\\\ x_k a_{nk} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\sum\\limits_{k = 1}^{n} x_k a_{1k} \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{2k} \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{3k} \\\\ \\vdots \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{mk} \\\\ \\vdots \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{nk} \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} \\sum\\limits_{k = 1}^{n} x_k a_{1k} \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{2k} \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{3k} \\\\ \\vdots \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{mk} \\\\ \\vdots \\\\ \\sum\\limits_{k = 1}^{n} x_k a_{nk} \\\\ \\end{bmatrix} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_k \\\\ \\vdots \\\\ v_n \\\\ \\end{bmatrix} \\] \\[ \\sum\\limits_{k = 1}^{n} a_{1k} x_k = v_1 \\] \\[ \\sum\\limits_{k = 1}^{n} a_{2k} x_k = v_2 \\] \\[ \\sum\\limits_{k = 1}^{n} a_{3k} x_k = v_3 \\] \\[ \\vdots \\] \\[ \\sum\\limits_{k = 1}^{n} a_{mk} x_k = v_m \\] <p>\\(1400\\) lines.</p> \\[ \\vdots \\] \\[ \\sum\\limits_{k = 1}^{n} a_{nk} x_k = v_n \\]"},{"location":"brainstorm.html#solutions-to-said-linear-systems-of-equations","title":"solutions to said linear systems of equations","text":"\\[ \\sum\\limits_{k = 1}^{n} a_{1k} x_k = v_1 \\] \\[ x_1 a_{11} + \\sum\\limits_{k = 2}^{n} a_{1k} x_k = v_1 \\] \\[ x_1 a_{11} = v_1 - \\sum\\limits_{k = 2}^{n} a_{1k} x_k \\] \\[ x_1 = \\frac{v_1 - \\sum\\limits_{k = 2}^{n} a_{1k} x_k}{a_{11}} = \\frac{v_1}{a_{11}} - \\frac{\\sum\\limits_{k = 2}^{n} a_{1k} x_k}{a_{11}} = \\frac{v_1}{a_{11}} - \\sum\\limits_{k = 2}^{n} \\frac{a_{1k} x_k}{a_{11}} \\] \\[ x_1 = \\frac{v_1}{a_{11}} - \\sum\\limits_{k = 2}^{n} \\frac{a_{1k}}{a_{11}} x_k \\] \\[ \\sum\\limits_{k = 1}^{n} a_{mk} x_k = v_m \\] \\[ a_{m1} x_1 + \\sum\\limits_{k = 2}^{n} a_{mk} x_k = v_m \\] \\[ a_{m1} (\\frac{v_1}{a_{11}} - \\sum\\limits_{k = 2}^{n} \\frac{a_{1k}}{a_{11}} x_k) + \\sum\\limits_{k = 2}^{n} a_{mk} x_k = v_m \\] \\[ a_{m1} \\frac{v_1}{a_{11}} - a_{m1} \\sum\\limits_{k = 2}^{n} \\frac{a_{1k}}{a_{11}} x_k + \\sum\\limits_{k = 2}^{n} a_{mk} x_k = v_m \\] \\[ \\frac{a_{m1} v_1}{a_{11}} - \\sum\\limits_{k = 2}^{n} \\frac{a_{m1} a_{1k}}{a_{11}} x_k + \\sum\\limits_{k = 2}^{n} a_{mk} x_k = v_m \\] \\[ -\\sum\\limits_{k = 2}^{n} \\frac{a_{m1} a_{1k}}{a_{11}} x_k + \\sum\\limits_{k = 2}^{n} a_{mk} x_k = v_m - \\frac{a_{m1} v_1}{a_{11}} \\] \\[ \\sum\\limits_{k = 2}^{n} \\frac{a_{m1} a_{1k}}{a_{11}} x_k + \\sum\\limits_{k = 2}^{n} a_{mk} x_k = \\frac{a_{m1} v_1}{a_{11}} - v_m \\] \\[ \\sum\\limits_{k = 2}^{n} \\frac{a_{m1} a_{1k}}{a_{11}} x_k + a_{mk} x_k = \\frac{a_{m1} v_1}{a_{11}} - \\frac{a_{11} v_m}{a_{11}} \\] \\[ \\sum\\limits_{k = 2}^{n} (\\frac{a_{m1} a_{1k}}{a_{11}} + a_{mk}) x_k = \\frac{a_{m1} v_1 - a_{11} v_m}{a_{11}} \\] \\[ \\sum\\limits_{k = 2}^{n} (\\frac{a_{m1} a_{1k}}{a_{11}} + \\frac{a_{11} a_{mk}}{a_{11}}) x_k = \\frac{a_{m1} v_1 - a_{11} v_m}{a_{11}} \\] \\[ \\sum\\limits_{k = 2}^{n} (\\frac{a_{m1} a_{1k} + a_{11} a_{mk}}{a_{11}}) x_k = \\frac{a_{m1} v_1 - a_{11} v_m}{a_{11}} \\] \\[ \\sum\\limits_{k = 2}^{n} (a_{m1} a_{1k} + a_{11} a_{mk}) x_k = a_{m1} v_1 - a_{11} v_m \\] \\[ \\sum\\limits_{k = 2}^{n} (a_{21} a_{1k} + a_{11} a_{2k}) x_k = a_{21} v_1 - a_{11} v_2 \\] \\[ \\sum\\limits_{k = 2}^{n} (a_{31} a_{1k} + a_{11} a_{3k}) x_k = a_{31} v_1 - a_{11} v_3 \\] \\[ \\vdots \\] \\[ \\sum\\limits_{k = 2}^{n} (a_{m1} a_{1k} + a_{11} a_{mk}) x_k = a_{m1} v_1 - a_{11} v_m \\] \\[ \\vdots \\] \\[ \\sum\\limits_{k = 2}^{n} (a_{n1} a_{1k} + a_{11} a_{nk}) x_k = a_{n1} v_1 - a_{11} v_n \\] \\[ \\begin{bmatrix} a_{21} a_{12} + a_{11} a_{22} &amp; a_{21} a_{12} + a_{11} a_{22} &amp; \\dots &amp; a_{21} a_{1k} + a_{11} a_{2k} &amp; \\dots &amp; a_{21} a_{1n} + a_{11} a_{2n} \\\\ a_{31} a_{12} + a_{11} a_{32} &amp; a_{31} a_{13} + a_{11} a_{33} &amp; \\dots &amp; a_{31} a_{1k} + a_{11} a_{3k} &amp; \\dots &amp; a_{31} a_{1n} + a_{11} a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp;  &amp; \\vdots \\\\ a_{21} a_{1k} + a_{11} a_{k2} &amp; a_{31} a_{1k} + a_{11} a_{k3} &amp; \\dots &amp; a_{k1} a_{1k} + a_{11} a_{kk} &amp; \\dots &amp; a_{n1} a_{1k} + a_{11} a_{kn} \\\\ \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} a_{12} + a_{11} a_{n2} &amp; a_{n1} a_{13} + a_{11} a_{n3} &amp; \\dots &amp; a_{n1} a_{1k} + a_{11} a_{nk} &amp; \\dots &amp; a_{n1} a_{1n} + a_{11} a_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_k \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{21} v_1 - a_{11} v_2 \\\\ a_{31} v_1 - a_{11} v_3 \\\\ \\vdots \\\\ a_{k1} v_1 - a_{11} v_k \\\\ \\vdots \\\\ a_{n1} v_1 - a_{11} v_n \\\\ \\end{bmatrix} \\]"},{"location":"brainstorm.html#conclutions","title":"conclutions","text":"<p>conclution #1:</p> \\[ \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\dots &amp; a_{1k} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\dots &amp; a_{2k} &amp; \\dots &amp; a_{2n} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\dots &amp; a_{3k} &amp; \\dots &amp; a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp;  &amp; \\vdots \\\\ a_{k1} &amp; a_{k2} &amp; a_{k3} &amp; \\dots &amp; a_{kk} &amp; \\dots &amp; a_{kn} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\dots &amp; a_{nk} &amp; \\dots &amp; a_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_k \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_k \\\\ \\vdots \\\\ v_n \\\\ \\end{bmatrix} \\] \\[ \\Downarrow \\] \\[ x_1 = \\frac{v_1}{a_{11}} - \\sum\\limits_{k = 2}^{n} \\frac{a_{1k}}{a_{11}} x_k \\] \\[ \\begin{bmatrix} a_{21} a_{12} + a_{11} a_{22} &amp; a_{21} a_{12} + a_{11} a_{22} &amp; \\dots &amp; a_{21} a_{1k} + a_{11} a_{2k} &amp; \\dots &amp; a_{21} a_{1n} + a_{11} a_{2n} \\\\ a_{31} a_{12} + a_{11} a_{32} &amp; a_{31} a_{13} + a_{11} a_{33} &amp; \\dots &amp; a_{31} a_{1k} + a_{11} a_{3k} &amp; \\dots &amp; a_{31} a_{1n} + a_{11} a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp;  &amp; \\vdots \\\\ a_{21} a_{1k} + a_{11} a_{k2} &amp; a_{31} a_{1k} + a_{11} a_{k3} &amp; \\dots &amp; a_{k1} a_{1k} + a_{11} a_{kk} &amp; \\dots &amp; a_{n1} a_{1k} + a_{11} a_{kn} \\\\ \\vdots &amp; \\vdots &amp;  &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} a_{12} + a_{11} a_{n2} &amp; a_{n1} a_{13} + a_{11} a_{n3} &amp; \\dots &amp; a_{n1} a_{1k} + a_{11} a_{nk} &amp; \\dots &amp; a_{n1} a_{1n} + a_{11} a_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_k \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{21} v_1 - a_{11} v_2 \\\\ a_{31} v_1 - a_{11} v_3 \\\\ \\vdots \\\\ a_{k1} v_1 - a_{11} v_k \\\\ \\vdots \\\\ a_{n1} v_1 - a_{11} v_n \\\\ \\end{bmatrix} \\] <p>\\(1516\\) Lines</p> <p>conclution #2:</p> \\[ \\begin{bmatrix} a_{11} \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\end{bmatrix} = \\begin{bmatrix} v_1 \\\\ \\end{bmatrix} \\] \\[ \\Downarrow \\] \\[ x_1 = \\frac{v_1}{a_{11}} \\]"},{"location":"brainstorm.html#general-matrix-multiplication","title":"general matrix multiplication","text":"\\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\dots &amp; a_{2n} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\dots &amp; a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\dots &amp; a_{nn} \\\\ \\end{bmatrix} \\] \\[ B = \\begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} &amp; \\dots &amp; b_{1n} \\\\ b_{21} &amp; b_{22} &amp; b_{23} &amp; \\dots &amp; b_{2n} \\\\ b_{31} &amp; b_{32} &amp; b_{33} &amp; \\dots &amp; b_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{n1} &amp; b_{n2} &amp; b_{n3} &amp; \\dots &amp; b_{nn} \\\\ \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} ? &amp; ? &amp; ? &amp; \\dots &amp; ? \\\\ ? &amp; ? &amp; ? &amp; \\dots &amp; ? \\\\ ? &amp; ? &amp; ? &amp; \\dots &amp; ? \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ ? &amp; ? &amp; ? &amp; \\dots &amp; ? \\\\ \\end{bmatrix} \\] \\[ (AB) e_x = A(B e_x) \\] \\[ B e_x : = \\begin{bmatrix} b_{1x} \\\\ b_{2x} \\\\ b_{3x} \\\\ \\vdots \\\\ b_{nx} \\\\ \\end{bmatrix} \\] \\[ A(B e_x) = A \\begin{bmatrix} b_{1x} \\\\ b_{2x} \\\\ b_{3x} \\\\ \\vdots \\\\ b_{nx} \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\dots &amp; a_{2n} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; \\dots &amp; a_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; a_{n3} &amp; \\dots &amp; a_{nn} \\\\ \\end{bmatrix} \\begin{bmatrix} b_{1x} \\\\ b_{2x} \\\\ b_{3x} \\\\ \\vdots \\\\ b_{nx} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\sum\\limits_{i = 1}^{n} b_{ix} a_{1i} \\\\ \\sum\\limits_{i = 1}^{n} b_{ix} a_{2i} \\\\ \\sum\\limits_{i = 1}^{n} b_{ix} a_{3i} \\\\ \\vdots \\\\ \\sum\\limits_{i = 1}^{n} b_{ix} a_{ni} \\\\ \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} \\sum\\limits_{i = 1}^{n} b_{i1} a_{1i} &amp; \\sum\\limits_{i = 1}^{n} b_{i2} a_{1i} &amp; \\sum\\limits_{i = 1}^{n} b_{i3} a_{1i} &amp; \\dots &amp; \\sum\\limits_{i = 1}^{n} b_{in} a_{1i} \\\\ \\sum\\limits_{i = 1}^{n} b_{i1} a_{2i} &amp; \\sum\\limits_{i = 1}^{n} b_{i2} a_{2i} &amp; \\sum\\limits_{i = 1}^{n} b_{i3} a_{2i} &amp; \\dots &amp; \\sum\\limits_{i = 1}^{n} b_{in} a_{2i} \\\\ \\sum\\limits_{i = 1}^{n} b_{i1} a_{3i} &amp; \\sum\\limits_{i = 1}^{n} b_{i2} a_{3i} &amp; \\sum\\limits_{i = 1}^{n} b_{i3} a_{3i} &amp; \\dots &amp; \\sum\\limits_{i = 1}^{n} b_{in} a_{3i} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum\\limits_{i = 1}^{n} b_{i1} a_{ni} &amp; \\sum\\limits_{i = 1}^{n} b_{i2} a_{ni} &amp; \\sum\\limits_{i = 1}^{n} b_{i3} a_{ni} &amp; \\dots &amp; \\sum\\limits_{i = 1}^{n} b_{in} a_{ni} \\\\ \\end{bmatrix} \\] \\[ \\sum\\limits_{i = 1}^{n} b_{iy} a_{xi} = \\begin{bmatrix} b_{1y} \\\\ b_{2y} \\\\ b_{3y} \\\\ \\vdots \\\\ b_{ny} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} a_{x1} \\\\ a_{x2} \\\\ a_{x3} \\\\ \\vdots \\\\ a_{xn} \\\\ \\end{bmatrix} = B e_y \\cdot A^T e_x \\] \\[ AB = \\begin{bmatrix} B \\hat{x} \\cdot A^T \\hat{x} &amp; B \\hat{y} \\cdot A^T \\hat{x} &amp; B \\hat{z} \\cdot A^T \\hat{x} &amp; \\dots &amp; B \\hat{\\omega} \\cdot A^T \\hat{x} \\\\ B \\hat{x} \\cdot A^T \\hat{y} &amp; B \\hat{y} \\cdot A^T \\hat{y} &amp; B \\hat{z} \\cdot A^T \\hat{y} &amp; \\dots &amp; B \\hat{\\omega} \\cdot A^T \\hat{y} \\\\ B \\hat{x} \\cdot A^T \\hat{z} &amp; B \\hat{y} \\cdot A^T \\hat{z} &amp; B \\hat{z} \\cdot A^T \\hat{z} &amp; \\dots &amp; B\\hat{\\omega} \\cdot A^T \\hat{z} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ B \\hat{x} \\cdot A^T \\hat{\\omega} &amp; B \\hat{y} \\cdot A^T \\hat{\\omega} &amp; B \\hat{\\omega} \\cdot A^T \\hat{z} &amp; \\dots &amp; B \\hat{\\omega} \\cdot A^T \\hat{\\omega} \\\\ \\end{bmatrix} \\]"},{"location":"brainstorm.html#1-1-and-the-foundations-of-modern-mathematics","title":"\\(1 + 1\\) and the foundations of modern mathematics","text":"<p>Before I start, none of this was scripted.</p> <p>What I want to do here is prove that \\(1 + 1 = 2\\). But most of the time, you find yourself in a loop of defining things. For example: what is \\(1\\)? \\(1\\) is the successor of \\(0\\). What is the successor operation? The successor operation is the function that-</p> <p>\\(1600\\) Lines.</p> <p>\"Stop right there!\" Said person #\\(2\\), \"What is a function?\".</p> <p>\"Okay, fine!\" Said person #\\(1\\), \"I'll instead say that \\(1\\) is the set that contains \\(0\\).\" (written \\(\\{ 0 \\}\\).)</p> <p>\"That's better, but can you express \\(\\{ 0 \\}\\) more formally?\" Said person #\\(2\\).</p> <p>\"Well, what I mean by that is: \\(\u2200x.x \\in \\{ 0 \\} \\iff x = 0\\).\" Said person #\\(1\\). (For any/all \\(x\\), \\(x\\) is within \\(\\{ 0 \\}\\) precisely when \\(x = 0\\).)</p> <p>\"But what is \\(=\\)?\" Said person #\\(2\\).</p> <p>\"Axiom #\\(1\\) of ZFC: the axiom of extensionality\" said person #\\(1\\), \"it states that \\(S = T\\) precisely when for any \\(s \\in S\\), \\(s \\in T\\), and for any \\(t \\in T\\), \\(t \\in S\\)\"</p> <p>\"Okay, so what you're saying is that \\(\u2200x.x \\in 1 \\iff (\u2200y.y \\in x \u2192 y \\in 0) \u2227 (\u2200y.y \\in 0 \u2192 y \\in x)\\).\" Said person #\\(2\\).</p> <p>\"Yes. Is there anything else left undefined?\" Said person #\\(1\\).</p> <p>\"Yes, always!\" Said person #\\(2\\). \"What is \\(0\\)?\"</p> <p>\"Are you really gonna make me answer that?\" Said person #\\(1\\).</p> <p>\"Yes!\" Said person #\\(2\\).</p> <p>\"Okay, fine!\" Person #\\(1\\) said with frustration. \"\\(0\\) is \u00d8\" (the empty set) \"is the set with nothing in it, so \\(\u00ac\u2203x.x \\in \u00d8\\).\"</p> <p>\"So, what you really meant by \\(\u2200x.x \\in 1 \\iff (\u2200y.y \\in x \u2192 y \\in 0) \u2227 (\u2200y.y \\in 0 \u2192 y \\in x)\\) was \\(\u2200x.x \\in 1 \\iff \u00ac\u2203y.y \\in x\\).\" Said person #\\(2\\)</p> <p>\"Yes!\" Said person #\\(1\\).</p> <p>\"So \\(1\\) is the set of all empty sets, of which there are only one\" Said person #\\(2\\) \"am I understanding this correctly?\" Said person #\\(2\\)</p> <p>\"Yes!\" Said person #\\(1\\).</p> <p>I'm tired of this conversation between a mathematician and probably a mathematical snob who only accepts the truest logical statements crafted from pure mathematical set theory.</p> \\[ \u2200x_1.(\u2200x_2.x_2 \\in x_1 \\iff \u00ac\u2203x_3.x_3 \\in x_2) \u2192 x_1 + x_1 = 2 \\] \\[ \\text{\"For any variable (call it } x_1 \\text{), that variable being } 1 \\text{ implies that adding it to itself results in } 2 \\text{, that is, there is not an } x_3 \\in x_2 \\text{\"} \\] \\[ \u2200x_1.x_1 \\in 2 \\iff (\u2200x_2.x_2 \\in x_1 \\iff \u00ac\u2203x_3.x_3 \\in x_2) \\] \\[ \\text{\"For any variable (call it } x_1 \\text{), } x_1 \\in 2 \\text{ implies that } x_1 = 1 \\text{, that is, for any variable (call it } x_2 \\text{), } x_2 \\in 1 \\text{ precisely when } x_2 = 0 \\text{, that is, there is not an } x_3 \\in x_2 \\text{\"} \\] \\[ \u2200x_1.\u2200x_2.(\u2200x_3.x_3 \\in x_1 \\iff \u00ac\u2203x_4.x_4 \\in x_3) \u2227 (\u2200x_3.x_3 \\in x_2 \\iff (\u2200x_4.x_4 \\in x_3 \\iff \u00ac\u2203x_5.x_5 \\in x_4)) \u2192 \u2200x_3.x_3 \\in x_1 + x_1 \\iff x_3 \\in x_2 \\] \\[ A(m, n) = m + n \\] \\[ A(n, 0) = n \\] \\[ S(n) = n + 1 \\] \\[ A(m, S(n)) = S(A(m, n)) \\] \\[ \u2200m.\u2200n.n = S(m) \\iff (\u2200x.x \\in n \\iff x = m) \\] \\[ \u2200m.\u2200n.n = S(m) \\iff (\u2200x_1.x_1 \\in n \\iff (\u2200x_2.x_2 \\in x_1 \\iff x_2 \\in m)) \\] <p>I feel like doing something else, how about Russel's paradox? It states that there is no set that conain only sets that don't contain themselves.</p> \\[ \u00ac\u2203x_1.\u2200x_2.x_2 \\in x_1 \\iff \u00acx_2 \\in x_2 \\] <p>Next: the first axiom of set theory (The Axiom of Extensionality). It states that two sets are equal if they have the same elements, but I think it actually means that if two sets are equal (i.e. they have the same elements), a set cannot contain just one of them, it has to contain either both or neither of the sets.</p> \\[ \u2200x_1.\u2200x_2.x_1 = x_2 \u2192 \u2200x_3.x_1 \\in x_3 \\iff x_2 \\in x_3 \\] \\[ \u2200x_1.\u2200x_2.(\u2200x_3.x_3 \\in x_1 \\iff x_3 \\in x_2) \u2192 \u2200x_3.x_1 \\in x_3 \\iff x_2 \\in x_3 \\] <p>Next: the second axiom of set theory (The Axiom of Foundation). It states that every set must have an element disjoint from itself (i.e. an element where the union of that element and the original set is empty (i.e. they don't have any common elements)).</p> \\[ \u2200x_1.x_1 \\ne \u00d8 \u2192 \u2203x_2.x_2 \\in x_1 \u2227 x_2 \u2229 x_1 = \u00d8 \\] \\[ \u2200x_1.\u00acx_1 = \u00d8 \u2192 \u2203x_2.x_2 \\in x_1 \u2227 \u00ac\u2203x_3.x_3 \\in x_1 \u2227 x_3 \\in x_2 \\] \\[ \u2200x_1.x_1 = \u00d8 \u2228 \u2203x_2.x_2 \\in x_1 \u2227 \u00ac\u2203x_3.x_3 \\in x_1 \u2227 x_3 \\in x_2 \\] \\[ \u2200x_1.\u00ac(\u2203x_2.x_2 \\in x_1) \u2228 \u2203x_2.x_2 \\in x_1 \u2227 \u00ac\u2203x_3.x_3 \\in x_1 \u2227 x_3 \\in x_2 \\] <p>Next: the third axiom of set theory (The Axiom of Pairing). Actually, I'm not going to use the axiom of pairing, I'm going to use the closely related singleton axiom, It states that if you have a set then there exists the set containing that set, as opposed to the axiom of pairing which says that if you have two sets then there is a set containing both of them. These two statements are equivalent, but I prefer the first one.</p> <p>Also I realized that this axiom makes the axiom a regularity redundant. Let's say that \\(S = \\{ S \\}\\). then you would say that \\(S\\) is a set because it is equal to the set containing \\(S\\). So we would also need to assume that \\(S\\) is a set for that to work, so that would mean that we need to prove that \\(S\\) is a set, so that would mean that we need to prove that \\(S\\) is a set, you just never get to the bottom of it and you can never declare that \\(S\\) is a set.</p> <p>Also this is a weird kind of axiom because it doesn't always make the set containing a set into a set, you still have to prove it with the other rules. So we just knocked out two axioms of set theory with one stone.</p>"},{"location":"brainstorm.html#textsymm_4","title":"\\(\\text{Symm}_4\\)","text":"<p>I'm going to use a notation and this is how it works: I will notate \\((1, 2, 3)(4, 5)\\) as the function on a string that brings the first term to the second place in the string, the second term to the third place in the string, and the third thing back to the first term in the string. And then also swapping the fourth and fifth terms. And also I'm going to do composition from left to right instead of from right to left, so the composition of \\(f\\) and \\(g\\) does \\(f\\) first then \\(g\\).</p> <p>Here's a chart of each permutation multiplied by each other permutation of four elements:</p> \\(\\times\\) () (1, 2) (2, 3) (1, 2, 3) (1, 3, 2) (1, 3) (3, 4) (1, 2)(3, 4) (2, 3, 4) (1, 2, 3, 4) (1, 3, 4, 2) (1, 3, 4) (2, 4, 3) (1, 2, 4, 3) (2, 4) (1, 2, 4) (1, 3)(2, 4) (1, 3, 2, 4) (1, 4, 3, 2) (1, 4, 3) (1, 4, 2) (1, 4) (1, 4, 2, 3) (1, 4)(2, 3) () () (1, 2) (2, 3) (1, 2, 3) (1, 3, 2) (1, 3) (3, 4) (1, 2)(3, 4) (2, 3, 4) (1, 2, 3, 4) (1, 3, 4, 2) (1, 3, 4) (2, 4, 3) (1, 2, 4, 3) (2, 4) (1, 2, 4) (1, 3)(2, 4) (1, 3, 2, 4) (1, 4, 3, 2) (1, 4, 3) (1, 4, 2) (1, 4) (1, 4, 2, 3) (1, 4)(2, 3) (1, 2) (1, 2) (2, 3) (2, 3) (1, 2, 3) (1, 2, 3) (1, 3, 2) (1, 3, 2) (1, 3) (1, 3) (3, 4) (3, 4) (1, 2)(3, 4) (1, 2)(3, 4) (2, 3, 4) (2, 3, 4) (1, 2, 3, 4) (1, 2, 3, 4) (1, 3, 4, 2) (1, 3, 4, 2) (1, 3, 4) (1, 3, 4) (2, 4, 3) (2, 4, 3) (1, 2, 4, 3) (1, 2, 4, 3) (2, 4) (2, 4) (1, 2, 4) (1, 2, 4) (1, 3)(2, 4) (1, 3)(2, 4) (1, 3, 2, 4) (1, 3, 2, 4) (1, 4, 3, 2) (1, 4, 3, 2) (1, 4, 3) (1, 4, 3) (1, 4, 2) (1, 4, 2) (1, 4) (1, 4) (1, 4, 2, 3) (1, 4, 2, 3) (1, 4)(2, 3) (1, 4)(2, 3) <p>\\(1717\\) Lines.</p> <p>I don't feel like filling out the rest of the \\(529\\) entries of this table. Instead I'll ask my dad to write some code to do it for me, and then use that information to create a new times table.</p>"},{"location":"brainstorm.html#magic-squares","title":"magic squares","text":"<p>If you don't know, a magic square is a square of numbers, typically \\(3x3\\), where the sum, for the sum of each row, each column, and the two diagonals are all equal. Particularly, what I would like to do here is find how many numbers it takes to define an affine magic square. That is, one without the diagonal requirement, because then if you rotate it, or move the first column to the end, or the first row to the end, or swap to rows or two columns, then it will still work.</p> \\[ \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\\\ \\end{bmatrix} \\] \\[ a + b + c = d + e + f = g + h + i = a + d + g = b + e + h = c + f + i = m \\] \\[ a + b + c = m \\] \\[ a = m - b - c \\] \\[ a + b + c = a + d + g \\] \\[ b + c = d + g \\] \\[ d + e + f = g + h + i = b + e + h = c + f + i = m \\] \\[ b + c = d + g \\]"},{"location":"brainstorm.html#a-puzzle","title":"a puzzle","text":"<p>Okay, here's the puzzle: you are greeted by \\(10\\) boxes that contain a random number from \\(0\\) to \\(1\\) and you want to get the biggest one. So what you can do is you open the first box and you can either choose to take it or leave it, and if you leave it, then you can never come back to it. What is your strategy for getting the highest number? And how does that strategy scale for more or fewer boxes?</p> <p>According to a veritasium video about the number \\(37\\), the answer is to check the first \\(37\\)%, rejects all of those, and then after that you choose one that's bigger than the biggest one in those first \\(37\\)%, choose that one, and if those first \\(37\\)% contained the biggest one (which happens 37% of the time), then tough luck!</p> <p>I thought: what if you didn't choose some boxes to reject always and then choose the best one after that? Then I was thinking: what is the perfect strategy?</p> <p>Well, for one box it's really obvious: just choose that box. For two boxes, it also seems kind of obvious: if the first one is more than one half, then do it, and if it's less than one half, pass it.</p> <p>What about three boxes? Well, I know that if I somehow get down to two, then I know a strategy. So at what threshold \\(t\\) should I keep it? Well, what's the expected value for two of them? Half of the time it's less than \\(\\frac{1}{2}\\) and you have to skip it, with an average value for the other one being \\(\\frac{1}{2}\\), but half of the time it's more than \\(\\frac{1}{2}\\) and the average is \\(\\frac{3}{4}\\). So if you compute \\(\\frac{1}{2} \\frac{1}{2} + \\frac{1}{2} \\frac{3}{4} = \\frac{1}{4} + \\frac{3}{8} = \\frac{2}{8} + \\frac{3}{8} = \\frac{5}{8}\\). So if the value in the first box is more than \\(\\frac{5}{8}\\), then keep it, and if it's less, skip it.</p> <p>But what about a more general case? If I have \\(n\\) boxes then what is the threshold (call it \\(f(n)\\)) where if it's more than that, I should keep it, and if it's less than that, I should leave it, and if it's exactly that, then either one.</p> <p>Well, I know that it's equal to the expected score for \\(n - 1\\) boxes (denoted \\(\\text{ES} (n - 1)\\)), and \\(\\text{ES} (n)\\) must be equal to the probability that a random number from \\(0\\) to \\(1\\) is less than \\(f(n)\\) (that is, \\(f(n)\\) itself), multiplied by \\(\\text{ES} (n - 1)\\), plus the probability that it is more than \\(f(n)\\) (that is, \\(1 - f(n)\\)), and in that case, the expected value is \\(\\frac{1 + f(n)}{2} = \\frac{1}{2} + \\frac{f(n)}{2}\\). So the expected score (and hence \\(f(n + 1)\\)) is given by:</p> \\[ f(n + 1) = f(n) \\text{ES} (n - 1) + (1 - f(n)) (\\frac{1}{2} + \\frac{f(n)}{2}) = f(n) f(n) + 1 \\frac{1}{2} + 1 \\frac{f(n)}{2} - f(n) \\frac{1}{2} - f(n) \\frac{f(n)}{2} = f(n)^2 + \\frac{1}{2} + \\frac{f(n)}{2} - \\frac{f(n)}{2} - \\frac{f(n)^2}{2} \\] \\[ f(n + 1) = \\frac{f(n)^2}{2} + \\frac{1}{2} \\] <p>And coupled with the results that there is no threshold for one box (i.e. the threshold is \\(0\\)), we now have an inductive formula for the perfect strategy.</p> <p>So, the answer to the original question for \\(n = 10\\) is: If the first one is more than about \\(0.849\\), then keep it, if not, then pass it. And if the second one is more than about \\(0.836\\), then keep it, if not, then pass it. And if the third one is more than about \\(0.82\\), then keep it, if not, then pass it. And if the fourth one is more than about \\(0.8\\), then keep it, if not, then pass it. And if the fifth one is more than about \\(0.775\\), then keep it, if not, then pass it. And if the sixth one is more than about \\(0.741\\), then keep it, if not, then pass it. And if the seventh one is more than about \\(0.695\\), then keep it, if not, then pass it. And if the eighth one is more than \\(0.625\\), then keep it, if not, then pass it. And if the nineth one is more than \\(\\frac{1}{2}\\), then keep it, if not, then pass it.</p>"},{"location":"brainstorm.html#textsymm_3","title":"\\(\\text{Symm}_3\\)","text":"<p>Last time I tried this it was too much data and my dad still hasn't finished that code. So I'm going to make a multiplication table of a more manageable size, then find all of its symmetric beauty.</p> \\(\\times\\) () (1, 2) (2, 3) (1, 2, 3) (1, 3, 2) (1, 3) () () (1, 2) (2, 3) (1, 2, 3) (1, 3, 2) (1, 3) (1, 2) (1, 2) () (1, 3, 2) (1, 3) (2, 3) (1, 2, 3) (2, 3) (2, 3) (1, 2, 3) () (1, 2) (1, 3) (1, 3, 2) (1, 2, 3) (1, 2, 3) (2, 3) (1, 3) (1, 3, 2) () (1, 2) (1, 3, 2) (1, 3, 2) (1, 3) (1, 2) () (1, 2, 3) (2, 3) (1, 3) (1, 3) (1, 3, 2) (1, 2, 3) (2, 3) (1, 2) () <p>\\(1776 + 4\\) Lines.</p> <p>Process: swap around pieces of paper labeled \\(1\\), \\(2\\), and \\(3\\), doing another swap, and looking at the table of what the answer should be based on the pieces of paper (see table below), and then repeating that \\(25\\) times.</p> \\(123\\) () \\(213\\) (1, 2) \\(132\\) (2, 3) \\(312\\) (1, 2, 3) \\(231\\) (1, 3, 2) \\(321\\) (1, 3) <p>So now I'm going to swap out the swaps with letters of the alphabet, and remove the trivial first row and first column.</p> \\(\\times\\) \\(a\\) \\(b\\) \\(c\\) \\(d\\) \\(e\\) \\(a\\) \\(id\\) \\(d\\) \\(e\\) \\(b\\) \\(c\\) \\(b\\) \\(c\\) \\(id\\) \\(a\\) \\(e\\) \\(d\\) \\(c\\) \\(b\\) \\(e\\) \\(d\\) \\(id\\) \\(a\\) \\(d\\) \\(e\\) \\(a\\) \\(id\\) \\(c\\) \\(b\\) \\(e\\) \\(d\\) \\(c\\) \\(b\\) \\(a\\) \\(id\\) <p>\\(1802\\) Lines.</p> <p>If you were wondering, I'm not using \\(i\\) for the identity, because in group theory, you use an \\(e\\). But \\(e\\) was already taken, so instead of using \\(i\\), I used \\(id\\), which is the standard category theory notation.</p> <p>Notice any patterns? Well, the first thing that jumped out to me a few months ago was the string of \\(id\\)s across the diagonal, interrupted by \\(c\\) and \\(d\\).</p> <p>The reason why was because my first encounter with this group was an equivalent group \\(D_3\\) (the dihedral group of order \\(3\\) or the group of all rotations and reflections of a triangle that leave the corners looking the same), as opposed to \\(\\text{Symm}_3\\), which is the group of all ways to arrange three objects. What I noticed was that \\(a\\), \\(b\\), and \\(e\\) were reflections, so of course doing them twice would result in the same thing.</p>"},{"location":"brainstorm.html#sir-model","title":"SIR model","text":"\\[ \\text{S} (0) = 1 \\] \\[ \\frac{d \\text{S}}{dt} = -ln(R_0) \\text{I} (t) \\] \\[ \\text{I} (0) = dt \\] \\[ \\frac{d \\text{I}}{dt} = (ln(R_0) - c) \\text{I} (t) \\] \\[ \\text{R} (0) = 0 \\] \\[ \\frac{d \\text{R}}{dt} = c \\text{I} (t) \\]"},{"location":"brainstorm.html#extreme-sir-model","title":"extreme SIR model","text":"\\[ f_1 (0) = 1 \\] \\[ f_n (0) = 0 \\] \\[ \\frac{df_1}{dt} = -x_1 f_1 (t) \\] \\[ \\frac{df_n}{dt} = x_{n - 1} f_{n - 1} (t) - x_n f_n (t) \\] \\[ f_1 (t) = C e^{-x_1 t} \\] \\[ f_1 (0) = 1 = C e^{-x_1 0} = C e^0 = 1C = C \\] \\[ C = 1 \\] \\[ f_1 (t) = e^{-x_1 t} \\] \\[ f_2 (0) = 0 \\] \\[ \\frac{df_2}{dt} = x_1 f_{1} (t) - x_2 f_2 (t) \\] \\[ \\frac{df_2}{dt} = x_1 e^{-x_1 t} - x_2 f_2 (t) \\] \\[ \\text{And after plugging this into a differential equation solver (shout out to Symbolab), I have a solution.} \\] \\[ f_2 (t) = \\frac{x_1}{(x_2 - x_1) e^{x_1 t}} + \\frac{C}{e^{x_2 t}} \\] \\[ f_2 (0) = 0 = \\frac{x_1}{(x_2 - x_1) e^{x_1 0}} + \\frac{C}{e^{x_2 0}} = \\frac{x_1}{(x_2 - x_1) e^0} + \\frac{C}{e^0} = \\frac{x_1}{1 (x_2 - x_1)} + \\frac{C}{1} = \\frac{x_1}{x_2 - x_1} + C \\] \\[ \\frac{x_1}{x_2 - x_1} + C = 0 \\] \\[ C = -\\frac{x_1}{x_2 - x_1} \\] \\[ f_2 (t) = \\frac{x_1}{(x_2 - x_1) e^{x_1 t}} - \\frac{\\frac{x_1}{x_2 - x_1}}{e^{x_2 t}} = \\frac{x_1}{(x_2 - x_1) e^{x_1 t}} - \\frac{x_1}{(x_2 - x_1) e^{x_2 t}} = \\frac{x_1}{(x_2 - x_1)} e^{-x_1 t} - \\frac{x_1}{(x_2 - x_1)} e^{-x_2 t} \\] \\[ f_2 (t) = \\frac{x_1}{(x_2 - x_1)} (e^{-x_1 t} - e^{-x_2 t}) \\]"},{"location":"brainstorm.html#textsymm_4-attempt-2","title":"\\(\\text{Symm}_4\\) attempt \\(2\\)","text":"<p>Status update: my dad still hasn't finished the code so I wrote my own code. The original output is:</p> [0 1 2 3] [1 0 2 3] [0 2 1 3] [2 0 1 3] [1 2 0 3] [2 1 0 3] [0 1 3 2] [1 0 3 2] [0 3 1 2] [3 0 1 2] [1 3 0 2] [3 1 0 2] [0 2 3 1] [2 0 3 1] [0 3 2 1] [3 0 2 1] [2 3 0 1] [3 2 0 1] [1 2 3 0] [2 1 3 0] [1 3 2 0] [3 1 2 0] [2 3 1 0] [3 2 1 0] [0 1 2 3] [0 1 2 3] [1 0 2 3] [0 2 1 3] [2 0 1 3] [1 2 0 3] [2 1 0 3] [0 1 3 2] [1 0 3 2] [0 3 1 2] [3 0 1 2] [1 3 0 2] [3 1 0 2] [0 2 3 1] [2 0 3 1] [0 3 2 1] [3 0 2 1] [2 3 0 1] [3 2 0 1] [1 2 3 0] [2 1 3 0] [1 3 2 0] [3 1 2 0] [2 3 1 0] [3 2 1 0] [1 0 2 3] [1 0 2 3] [0 1 2 3] [2 0 1 3] [0 2 1 3] [2 1 0 3] [1 2 0 3] [1 0 3 2] [0 1 3 2] [3 0 1 2] [0 3 1 2] [3 1 0 2] [1 3 0 2] [2 0 3 1] [0 2 3 1] [3 0 2 1] [0 3 2 1] [3 2 0 1] [2 3 0 1] [2 1 3 0] [1 2 3 0] [3 1 2 0] [1 3 2 0] [3 2 1 0] [2 3 1 0] [0 2 1 3] [0 2 1 3] [1 2 0 3] [0 1 2 3] [2 1 0 3] [1 0 2 3] [2 0 1 3] [0 3 1 2] [1 3 0 2] [0 1 3 2] [3 1 0 2] [1 0 3 2] [3 0 1 2] [0 3 2 1] [2 3 0 1] [0 2 3 1] [3 2 0 1] [2 0 3 1] [3 0 2 1] [1 3 2 0] [2 3 1 0] [1 2 3 0] [3 2 1 0] [2 1 3 0] [3 1 2 0] [2 0 1 3] [2 0 1 3] [2 1 0 3] [1 0 2 3] [1 2 0 3] [0 1 2 3] [0 2 1 3] [3 0 1 2] [3 1 0 2] [1 0 3 2] [1 3 0 2] [0 1 3 2] [0 3 1 2] [3 0 2 1] [3 2 0 1] [2 0 3 1] [2 3 0 1] [0 2 3 1] [0 3 2 1] [3 1 2 0] [3 2 1 0] [2 1 3 0] [2 3 1 0] [1 2 3 0] [1 3 2 0] [1 2 0 3] [1 2 0 3] [0 2 1 3] [2 1 0 3] [0 1 2 3] [2 0 1 3] [1 0 2 3] [1 3 0 2] [0 3 1 2] [3 1 0 2] [0 1 3 2] [3 0 1 2] [1 0 3 2] [2 3 0 1] [0 3 2 1] [3 2 0 1] [0 2 3 1] [3 0 2 1] [2 0 3 1] [2 3 1 0] [1 3 2 0] [3 2 1 0] [1 2 3 0] [3 1 2 0] [2 1 3 0] [2 1 0 3] [2 1 0 3] [2 0 1 3] [1 2 0 3] [1 0 2 3] [0 2 1 3] [0 1 2 3] [3 1 0 2] [3 0 1 2] [1 3 0 2] [1 0 3 2] [0 3 1 2] [0 1 3 2] [3 2 0 1] [3 0 2 1] [2 3 0 1] [2 0 3 1] [0 3 2 1] [0 2 3 1] [3 2 1 0] [3 1 2 0] [2 3 1 0] [2 1 3 0] [1 3 2 0] [1 2 3 0] [0 1 3 2] [0 1 3 2] [1 0 3 2] [0 2 3 1] [2 0 3 1] [1 2 3 0] [2 1 3 0] [0 1 2 3] [1 0 2 3] [0 3 2 1] [3 0 2 1] [1 3 2 0] [3 1 2 0] [0 2 1 3] [2 0 1 3] [0 3 1 2] [3 0 1 2] [2 3 1 0] [3 2 1 0] [1 2 0 3] [2 1 0 3] [1 3 0 2] [3 1 0 2] [2 3 0 1] [3 2 0 1] [1 0 3 2] [1 0 3 2] [0 1 3 2] [2 0 3 1] [0 2 3 1] [2 1 3 0] [1 2 3 0] [1 0 2 3] [0 1 2 3] [3 0 2 1] [0 3 2 1] [3 1 2 0] [1 3 2 0] [2 0 1 3] [0 2 1 3] [3 0 1 2] [0 3 1 2] [3 2 1 0] [2 3 1 0] [2 1 0 3] [1 2 0 3] [3 1 0 2] [1 3 0 2] [3 2 0 1] [2 3 0 1] [0 3 1 2] [0 3 1 2] [1 3 0 2] [0 3 2 1] [2 3 0 1] [1 3 2 0] [2 3 1 0] [0 2 1 3] [1 2 0 3] [0 2 3 1] [3 2 0 1] [1 2 3 0] [3 2 1 0] [0 1 2 3] [2 1 0 3] [0 1 3 2] [3 1 0 2] [2 1 3 0] [3 1 2 0] [1 0 2 3] [2 0 1 3] [1 0 3 2] [3 0 1 2] [2 0 3 1] [3 0 2 1] [3 0 1 2] [3 0 1 2] [3 1 0 2] [3 0 2 1] [3 2 0 1] [3 1 2 0] [3 2 1 0] [2 0 1 3] [2 1 0 3] [2 0 3 1] [2 3 0 1] [2 1 3 0] [2 3 1 0] [1 0 2 3] [1 2 0 3] [1 0 3 2] [1 3 0 2] [1 2 3 0] [1 3 2 0] [0 1 2 3] [0 2 1 3] [0 1 3 2] [0 3 1 2] [0 2 3 1] [0 3 2 1] [1 3 0 2] [1 3 0 2] [0 3 1 2] [2 3 0 1] [0 3 2 1] [2 3 1 0] [1 3 2 0] [1 2 0 3] [0 2 1 3] [3 2 0 1] [0 2 3 1] [3 2 1 0] [1 2 3 0] [2 1 0 3] [0 1 2 3] [3 1 0 2] [0 1 3 2] [3 1 2 0] [2 1 3 0] [2 0 1 3] [1 0 2 3] [3 0 1 2] [1 0 3 2] [3 0 2 1] [2 0 3 1] [3 1 0 2] [3 1 0 2] [3 0 1 2] [3 2 0 1] [3 0 2 1] [3 2 1 0] [3 1 2 0] [2 1 0 3] [2 0 1 3] [2 3 0 1] [2 0 3 1] [2 3 1 0] [2 1 3 0] [1 2 0 3] [1 0 2 3] [1 3 0 2] [1 0 3 2] [1 3 2 0] [1 2 3 0] [0 2 1 3] [0 1 2 3] [0 3 1 2] [0 1 3 2] [0 3 2 1] [0 2 3 1] [0 2 3 1] [0 2 3 1] [1 2 3 0] [0 1 3 2] [2 1 3 0] [1 0 3 2] [2 0 3 1] [0 3 2 1] [1 3 2 0] [0 1 2 3] [3 1 2 0] [1 0 2 3] [3 0 2 1] [0 3 1 2] [2 3 1 0] [0 2 1 3] [3 2 1 0] [2 0 1 3] [3 0 1 2] [1 3 0 2] [2 3 0 1] [1 2 0 3] [3 2 0 1] [2 1 0 3] [3 1 0 2] [2 0 3 1] [2 0 3 1] [2 1 3 0] [1 0 3 2] [1 2 3 0] [0 1 3 2] [0 2 3 1] [3 0 2 1] [3 1 2 0] [1 0 2 3] [1 3 2 0] [0 1 2 3] [0 3 2 1] [3 0 1 2] [3 2 1 0] [2 0 1 3] [2 3 1 0] [0 2 1 3] [0 3 1 2] [3 1 0 2] [3 2 0 1] [2 1 0 3] [2 3 0 1] [1 2 0 3] [1 3 0 2] [0 3 2 1] [0 3 2 1] [1 3 2 0] [0 3 1 2] [2 3 1 0] [1 3 0 2] [2 3 0 1] [0 2 3 1] [1 2 3 0] [0 2 1 3] [3 2 1 0] [1 2 0 3] [3 2 0 1] [0 1 3 2] [2 1 3 0] [0 1 2 3] [3 1 2 0] [2 1 0 3] [3 1 0 2] [1 0 3 2] [2 0 3 1] [1 0 2 3] [3 0 2 1] [2 0 1 3] [3 0 1 2] [3 0 2 1] [3 0 2 1] [3 1 2 0] [3 0 1 2] [3 2 1 0] [3 1 0 2] [3 2 0 1] [2 0 3 1] [2 1 3 0] [2 0 1 3] [2 3 1 0] [2 1 0 3] [2 3 0 1] [1 0 3 2] [1 2 3 0] [1 0 2 3] [1 3 2 0] [1 2 0 3] [1 3 0 2] [0 1 3 2] [0 2 3 1] [0 1 2 3] [0 3 2 1] [0 2 1 3] [0 3 1 2] [2 3 0 1] [2 3 0 1] [2 3 1 0] [1 3 0 2] [1 3 2 0] [0 3 1 2] [0 3 2 1] [3 2 0 1] [3 2 1 0] [1 2 0 3] [1 2 3 0] [0 2 1 3] [0 2 3 1] [3 1 0 2] [3 1 2 0] [2 1 0 3] [2 1 3 0] [0 1 2 3] [0 1 3 2] [3 0 1 2] [3 0 2 1] [2 0 1 3] [2 0 3 1] [1 0 2 3] [1 0 3 2] [3 2 0 1] [3 2 0 1] [3 2 1 0] [3 1 0 2] [3 1 2 0] [3 0 1 2] [3 0 2 1] [2 3 0 1] [2 3 1 0] [2 1 0 3] [2 1 3 0] [2 0 1 3] [2 0 3 1] [1 3 0 2] [1 3 2 0] [1 2 0 3] [1 2 3 0] [1 0 2 3] [1 0 3 2] [0 3 1 2] [0 3 2 1] [0 2 1 3] [0 2 3 1] [0 1 2 3] [0 1 3 2] [1 2 3 0] [1 2 3 0] [0 2 3 1] [2 1 3 0] [0 1 3 2] [2 0 3 1] [1 0 3 2] [1 3 2 0] [0 3 2 1] [3 1 2 0] [0 1 2 3] [3 0 2 1] [1 0 2 3] [2 3 1 0] [0 3 1 2] [3 2 1 0] [0 2 1 3] [3 0 1 2] [2 0 1 3] [2 3 0 1] [1 3 0 2] [3 2 0 1] [1 2 0 3] [3 1 0 2] [2 1 0 3] [2 1 3 0] [2 1 3 0] [2 0 3 1] [1 2 3 0] [1 0 3 2] [0 2 3 1] [0 1 3 2] [3 1 2 0] [3 0 2 1] [1 3 2 0] [1 0 2 3] [0 3 2 1] [0 1 2 3] [3 2 1 0] [3 0 1 2] [2 3 1 0] [2 0 1 3] [0 3 1 2] [0 2 1 3] [3 2 0 1] [3 1 0 2] [2 3 0 1] [2 1 0 3] [1 3 0 2] [1 2 0 3] [1 3 2 0] [1 3 2 0] [0 3 2 1] [2 3 1 0] [0 3 1 2] [2 3 0 1] [1 3 0 2] [1 2 3 0] [0 2 3 1] [3 2 1 0] [0 2 1 3] [3 2 0 1] [1 2 0 3] [2 1 3 0] [0 1 3 2] [3 1 2 0] [0 1 2 3] [3 1 0 2] [2 1 0 3] [2 0 3 1] [1 0 3 2] [3 0 2 1] [1 0 2 3] [3 0 1 2] [2 0 1 3] [3 1 2 0] [3 1 2 0] [3 0 2 1] [3 2 1 0] [3 0 1 2] [3 2 0 1] [3 1 0 2] [2 1 3 0] [2 0 3 1] [2 3 1 0] [2 0 1 3] [2 3 0 1] [2 1 0 3] [1 2 3 0] [1 0 3 2] [1 3 2 0] [1 0 2 3] [1 3 0 2] [1 2 0 3] [0 2 3 1] [0 1 3 2] [0 3 2 1] [0 1 2 3] [0 3 1 2] [0 2 1 3] [2 3 1 0] [2 3 1 0] [2 3 0 1] [1 3 2 0] [1 3 0 2] [0 3 2 1] [0 3 1 2] [3 2 1 0] [3 2 0 1] [1 2 3 0] [1 2 0 3] [0 2 3 1] [0 2 1 3] [3 1 2 0] [3 1 0 2] [2 1 3 0] [2 1 0 3] [0 1 3 2] [0 1 2 3] [3 0 2 1] [3 0 1 2] [2 0 3 1] [2 0 1 3] [1 0 3 2] [1 0 2 3] [3 2 1 0] [3 2 1 0] [3 2 0 1] [3 1 2 0] [3 1 0 2] [3 0 2 1] [3 0 1 2] [2 3 1 0] [2 3 0 1] [2 1 3 0] [2 1 0 3] [2 0 3 1] [2 0 1 3] [1 3 2 0] [1 3 0 2] [1 2 3 0] [1 2 0 3] [1 0 3 2] [1 0 2 3] [0 3 2 1] [0 3 1 2] [0 2 3 1] [0 2 1 3] [0 1 3 2] [0 1 2 3] <p>And then I'll use <code>ctrl + f</code> \\(24\\) times to swap out the lists with letters.</p> \\(id\\) \\(a\\) \\(b\\) \\(c\\) \\(d\\) \\(e\\) \\(f\\) \\(g\\) \\(h\\) \\(i\\) \\(j\\) \\(k\\) \\(l\\) \\(m\\) \\(n\\) \\(o\\) \\(p\\) \\(q\\) \\(r\\) \\(s\\) \\(t\\) \\(u\\) \\(v\\) \\(w\\) \\(id\\) \\(id\\) \\(a\\) \\(b\\) \\(c\\) \\(d\\) \\(e\\) \\(f\\) \\(g\\) \\(h\\) \\(i\\) \\(j\\) \\(k\\) \\(l\\) \\(m\\) \\(n\\) \\(o\\) \\(p\\) \\(q\\) \\(r\\) \\(s\\) \\(t\\) \\(u\\) \\(v\\) \\(w\\) \\(a\\) \\(a\\) \\(id\\) \\(c\\) \\(b\\) \\(e\\) \\(d\\) \\(g\\) \\(f\\) \\(i\\) \\(h\\) \\(k\\) \\(j\\) \\(m\\) \\(l\\) \\(o\\) \\(n\\) \\(q\\) \\(p\\) \\(s\\) \\(r\\) \\(u\\) \\(t\\) \\(w\\) \\(v\\) \\(b\\) \\(b\\) \\(d\\) \\(id\\) \\(e\\) \\(a\\) \\(c\\) \\(h\\) \\(j\\) \\(f\\) \\(k\\) \\(g\\) \\(i\\) \\(n\\) \\(p\\) \\(l\\) \\(q\\) \\(m\\) \\(o\\) \\(t\\) \\(v\\) \\(r\\) \\(w\\) \\(s\\) \\(u\\) \\(c\\) \\(c\\) \\(e\\) \\(a\\) \\(d\\) \\(id\\) \\(b\\) \\(i\\) \\(k\\) \\(g\\) \\(j\\) \\(f\\) \\(h\\) \\(o\\) \\(q\\) \\(m\\) \\(p\\) \\(l\\) \\(n\\) \\(u\\) \\(w\\) \\(s\\) \\(v\\) \\(r\\) \\(t\\) \\(d\\) \\(d\\) \\(b\\) \\(e\\) \\(id\\) \\(c\\) \\(a\\) \\(j\\) \\(h\\) \\(k\\) \\(f\\) \\(i\\) \\(g\\) \\(p\\) \\(n\\) \\(q\\) \\(l\\) \\(o\\) \\(m\\) \\(v\\) \\(t\\) \\(w\\) \\(r\\) \\(u\\) \\(s\\) \\(e\\) \\(e\\) \\(c\\) \\(d\\) \\(a\\) \\(b\\) \\(id\\) \\(k\\) \\(i\\) \\(j\\) \\(g\\) \\(h\\) \\(f\\) \\(q\\) \\(o\\) \\(p\\) \\(m\\) \\(n\\) \\(l\\) \\(w\\) \\(u\\) \\(v\\) \\(s\\) \\(t\\) \\(r\\) \\(f\\) \\(f\\) \\(g\\) \\(l\\) \\(m\\) \\(r\\) \\(s\\) \\(id\\) \\(a\\) \\(n\\) \\(o\\) \\(t\\) \\(u\\) \\(b\\) \\(c\\) \\(h\\) \\(i\\) \\(v\\) \\(w\\) \\(d\\) \\(e\\) \\(j\\) \\(k\\) \\(p\\) \\(q\\) \\(g\\) \\(g\\) \\(f\\) \\(m\\) \\(l\\) \\(s\\) \\(r\\) \\(a\\) \\(id\\) \\(o\\) \\(n\\) \\(u\\) \\(t\\) \\(c\\) \\(b\\) \\(i\\) \\(h\\) \\(w\\) \\(v\\) \\(e\\) \\(d\\) \\(k\\) \\(j\\) \\(q\\) \\(p\\) \\(h\\) \\(h\\) \\(j\\) \\(n\\) \\(p\\) \\(t\\) \\(v\\) \\(b\\) \\(d\\) \\(l\\) \\(q\\) \\(r\\) \\(w\\) \\(id\\) \\(e\\) \\(f\\) \\(k\\) \\(s\\) \\(u\\) \\(a\\) \\(c\\) \\(g\\) \\(i\\) \\(m\\) \\(o\\) \\(i\\) \\(i\\) \\(k\\) \\(o\\) \\(q\\) \\(u\\) \\(w\\) \\(c\\) \\(e\\) \\(m\\) \\(p\\) \\(s\\) \\(v\\) \\(a\\) \\(d\\) \\(g\\) \\(j\\) \\(r\\) \\(t\\) \\(id\\) \\(b\\) \\(f\\) \\(h\\) \\(l\\) \\(n\\) \\(j\\) \\(j\\) \\(h\\) \\(p\\) \\(n\\) \\(v\\) \\(t\\) \\(d\\) \\(b\\) \\(q\\) \\(l\\) \\(w\\) \\(r\\) \\(e\\) \\(id\\) \\(k\\) \\(f\\) \\(u\\) \\(s\\) \\(c\\) \\(a\\) \\(i\\) \\(g\\) \\(o\\) \\(m\\) \\(k\\) \\(k\\) \\(i\\) \\(q\\) \\(o\\) \\(w\\) \\(u\\) \\(e\\) \\(c\\) \\(p\\) \\(m\\) \\(v\\) \\(s\\) \\(d\\) \\(a\\) \\(j\\) \\(g\\) \\(t\\) \\(r\\) \\(b\\) \\(id\\) \\(h\\) \\(f\\) \\(n\\) \\(l\\) \\(l\\) \\(l\\) \\(r\\) \\(f\\) \\(s\\) \\(g\\) \\(m\\) \\(n\\) \\(t\\) \\(id\\) \\(u\\) \\(a\\) \\(o\\) \\(h\\) \\(v\\) \\(b\\) \\(w\\) \\(c\\) \\(i\\) \\(j\\) \\(p\\) \\(d\\) \\(q\\) \\(e\\) \\(k\\) \\(m\\) \\(m\\) \\(s\\) \\(g\\) \\(r\\) \\(f\\) \\(l\\) \\(o\\) \\(u\\) \\(a\\) \\(t\\) \\(id\\) \\(n\\) \\(i\\) \\(w\\) \\(c\\) \\(v\\) \\(b\\) \\(h\\) \\(k\\) \\(q\\) \\(e\\) \\(p\\) \\(d\\) \\(j\\) \\(n\\) \\(n\\) \\(t\\) \\(h\\) \\(v\\) \\(j\\) \\(p\\) \\(l\\) \\(r\\) \\(b\\) \\(w\\) \\(d\\) \\(q\\) \\(f\\) \\(s\\) \\(id\\) \\(u\\) \\(e\\) \\(k\\) \\(g\\) \\(m\\) \\(a\\) \\(o\\) \\(c\\) \\(i\\) \\(o\\) \\(o\\) \\(u\\) \\(i\\) \\(w\\) \\(k\\) \\(q\\) \\(m\\) \\(s\\) \\(c\\) \\(v\\) \\(e\\) \\(p\\) \\(g\\) \\(r\\) \\(a\\) \\(t\\) \\(d\\) \\(j\\) \\(f\\) \\(l\\) \\(id\\) \\(n\\) \\(b\\) \\(h\\) \\(p\\) \\(p\\) \\(v\\) \\(j\\) \\(t\\) \\(h\\) \\(n\\) \\(q\\) \\(w\\) \\(d\\) \\(r\\) \\(b\\) \\(l\\) \\(k\\) \\(u\\) \\(e\\) \\(s\\) \\(id\\) \\(f\\) \\(i\\) \\(o\\) \\(c\\) \\(m\\) \\(a\\) \\(g\\) \\(q\\) \\(q\\) \\(w\\) \\(k\\) \\(u\\) \\(i\\) \\(o\\) \\(p\\) \\(v\\) \\(e\\) \\(s\\) \\(c\\) \\(m\\) \\(j\\) \\(t\\) \\(d\\) \\(r\\) \\(a\\) \\(g\\) \\(h\\) \\(n\\) \\(b\\) \\(l\\) \\(id\\) \\(f\\) \\(r\\) \\(r\\) \\(l\\) \\(s\\) \\(f\\) \\(m\\) \\(g\\) \\(t\\) \\(n\\) \\(u\\) \\(id\\) \\(o\\) \\(a\\) \\(v\\) \\(h\\) \\(w\\) \\(b\\) \\(i\\) \\(c\\) \\(p\\) \\(j\\) \\(q\\) \\(d\\) \\(k\\) \\(e\\) \\(s\\) \\(s\\) \\(m\\) \\(r\\) \\(g\\) \\(l\\) \\(f\\) \\(u\\) \\(o\\) \\(t\\) \\(a\\) \\(n\\) \\(id\\) \\(w\\) \\(i\\) \\(v\\) \\(c\\) \\(h\\) \\(b\\) \\(q\\) \\(k\\) \\(p\\) \\(e\\) \\(j\\) \\(d\\) \\(t\\) \\(t\\) \\(n\\) \\(v\\) \\(h\\) \\(p\\) \\(j\\) \\(r\\) \\(l\\) \\(w\\) \\(b\\) \\(q\\) \\(d\\) \\(s\\) \\(f\\) \\(u\\) \\(id\\) \\(k\\) \\(e\\) \\(m\\) \\(g\\) \\(o\\) \\(a\\) \\(i\\) \\(c\\) \\(u\\) \\(u\\) \\(o\\) \\(w\\) \\(i\\) \\(q\\) \\(k\\) \\(s\\) \\(m\\) \\(v\\) \\(c\\) \\(p\\) \\(e\\) \\(r\\) \\(g\\) \\(t\\) \\(a\\) \\(j\\) \\(d\\) \\(l\\) \\(f\\) \\(n\\) \\(id\\) \\(h\\) \\(b\\) \\(v\\) \\(v\\) \\(p\\) \\(t\\) \\(j\\) \\(n\\) \\(h\\) \\(w\\) \\(q\\) \\(r\\) \\(d\\) \\(l\\) \\(b\\) \\(u\\) \\(k\\) \\(s\\) \\(e\\) \\(f\\) \\(id\\) \\(o\\) \\(i\\) \\(m\\) \\(c\\) \\(g\\) \\(a\\) \\(w\\) \\(w\\) \\(q\\) \\(u\\) \\(k\\) \\(o\\) \\(i\\) \\(v\\) \\(p\\) \\(s\\) \\(e\\) \\(m\\) \\(c\\) \\(t\\) \\(j\\) \\(r\\) \\(d\\) \\(g\\) \\(a\\) \\(n\\) \\(h\\) \\(l\\) \\(b\\) \\(f\\) \\(id\\) <p>\\(1922\\) Lines.</p> <p>Notice any patterns? Me neither.</p>"},{"location":"brainstorm.html#group-theory","title":"group theory?","text":"<p>I'm making this page to celebrate the up upcoming \\(2000\\)th line.</p> <p>Here's some great videos about the subject that should get at least some credit: Group theory, abstraction, and the 196,883-dimensional monster, What is Group Theory? \u2014 Group Theory Ep. 1 (Yes, I know, there's a part two. But I decided not to watch it because I was worried that if I watched it then I would just copy some of it), This playlist by Another Roof (I've only seen the third part).</p> <p>I'll start off with the textbook definition of a group:</p> <p>PS I don't have a group theory textbook.</p> <p>It is a set or collection of things together with a binary operation (e.g. addition or multiplication because they input two things and output one thing) (this binary operation is usually denoted with a composition circle (this thing: \u2218) so that is the notation that I will use) such that...</p> <p>\\(1\\). Closure: (this one is sometimes a given) If you have \\(a\\) and \\(b\\) in the group, then \\(a \u2218 b\\) is also within the group.</p> <p>\\(2\\). Associativity: If you have \\(a\\), \\(b\\) and \\(c\\) in the group, then \\((a \u2218 b) \u2218 c\\) is equal to \\(a \u2218 (b \u2218 c)\\). For this reason, I will be denoting both as \\(a \u2218 b \u2218 c\\)</p> <p>\\(3\\). Identity (or neutral depending on where you're from): There must always be a term in the group (call it \\(e\\)) where if you have \\(a\\) in the group, then \\(a \u2218 e\\) is equal to \\(e \u2218 a\\) is equal to \\(a\\).</p> <p>\\(4\\). Inverses: If you have \\(a\\) in the group, then there is also \\(a^{-1}\\) in the group where \\(a \u2218 a^{-1}\\) is equal to \\(e\\).</p> <p>Notice there is no point where I say that the operation is commutative (i.e. \\(a \u2218 b = b \u2218 a\\)). If it is commutative, it is also known as an Abelian group.</p> <p>Also by the way, it is common to notate \\(a \u2218 a \u2218 a \u2218 ... \u2218 a\\) \\(n\\) times as \\(a^n\\)</p> <p>A good way to think about what groups actually are is as symmetries. This is because these four rules are exactly what you would expect rotations and reflections to do with the operation of doing one after the other.</p> <p>For example, now the inverses rule makes sense because if you rotate clockwise then of course you should also be able to rotate counterclockwise.</p> <p>An example of a group is the integers with the operation of addition, but not the integers with an operation of multiplication because only \\(1\\) and \\(-1\\) have an inverse. The rational numbers under multiplication (excluding \\(0\\)) also form a group and so can the rationals under addition. Also of course, the real numbers with addition and multiplication (excluding \\(0\\)).</p>"},{"location":"brainstorm.html#proofs","title":"proofs","text":"<p>Proof number one: there's only one identity element.</p> <p>This proof uses a proof by contradiction strategy. Let's say that there are more than one identity element. So I'm going to choose the first two being \\(e_1\\) and \\(e_2\\).</p> <p>Now, I ask of you, what is \\(e_1 \u2218 e_2\\)? Because on the one hand, it should equal \\(e_2\\) because \\(e_1\\) times anything is that thing. But on the other hand, it should equal \\(e_1\\) because anything times \\(e_2\\) is that thing.</p> <p>Thus, because they are both equal to \\(e_1 \u2218 e_2\\), they must themselves be equal. Thus there is only one identity element.</p> <p>And you can keep going with this logic, doing the same thing with the next one, and the next one, until there is only one left.</p> <p>QED!</p> <p>Proof number two: The inverse of the inverse is the original.</p> <p>Every element has an inverse. So, by definition, the inverse has an inverse.</p> <p>Let's operate all of these together and see what happens.</p> \\[ a \u2218 a^{-1} \u2218 (a^{-1})^{-1} \\] <p>This should of course equal \\(a\\) because \\(a^{-1}\\) times its inverse should cancel out. But also this should equal \\((a^{-1})^{-1}\\) because \\(a\\) and its inverse should cancel out. Thus, because they are both equal to the same thing, they themselves must be equal.</p> <p>QED!</p> <p>Proof number three: The inverse can cancel out from either side.</p> <p>The term \\(a^{-1} \u2218 a\\) can also be simplified. Because \\(a\\) is equal to \\((a^{-1})^{-1}\\), I can cancel \\(a^{-1}\\) with its inverse, resulting in the identity.</p> \\[ a^{-1} \u2218 a = a^{-1} \u2218 (a^{-1})^{-1} = e \\] <p>QED!</p> <p>Proof number four: There's only one inverse for a given term.</p> <p>This one uses the same general strategy as proof number one. Let's assume that there were multiple inverses, denoted \\(a^{-1}_1\\) and \\(a^{-1}_2\\). Then of course, \\(a \u2218 a^{-1}_1 = e\\).</p> <p>Let's see what happens when you multiply both sides on the left by \\(a^{-1}_2\\).</p> \\[ a^{-1}_2 \u2218 a \u2218 a^{-1}_1 = a^{-1}_2 \u2218 e \\] <p>Then \\(a^{-1}_2\\) and \\(a\\) would cancel out resulting in \\(a^{-1}_1\\) on the left. But on the other side, the identity element cancels out resulting in \\(a^{-1}_2\\). Thus, because they are both equal to the same thing, they themselves must be equal.</p> <p>\\(2000\\) Lines, wow.</p> <p>QED!</p> <p>Proof number five: \\((a^2)^{-1} = (a^{-1})^2\\) and they can both be denoted as \\(a^{-2}\\).</p> \\[ a^2 \u2218 (a^2)^{-1} = e \\] \\[ a^2 \u2218 (a^{-1})^2 = a \u2218 a \u2218 a^{-1} \u2218 a^{-1} = a \u2218 a^{-1} = e \\] <p>Because these are both the inverse of \\(a^2\\) and because of proof number four, they must both be the same.</p> <p>\\(2013\\) Lines (I was born in \\(2013\\)).</p> <p>QED!</p>"},{"location":"brainstorm.html#newtonian-physics","title":"Newtonian physics","text":"<p>I tried to just write this down on paper, but I thought it would be much more convenient to write it down over here.</p> <p>I'm going to try to use Newtonian physics to describe collision of spheres. Each sphere has an index \\(n\\) with a center \\(c_n\\), velocity \\(v_n\\) acceleration \\(a_n\\), and so on. If there's a property that doesn't have a subscript, then it works with any number.</p> \\[ \\text{V} = \\frac{4}{3} \\pi r^3 \\] <p>\\(2025\\) Lines.</p> \\[ \\text{V} = \\frac{4 \\pi}{3} r^3 \\] \\[ \\text{V} = \\frac{2 \\tau}{3} r^3 \\] \\[ \\text{V} = \\frac{2}{3} \\tau r^3 \\] \\[ \\text{D} = \\frac{\\text{M}}{\\text{V}} \\] \\[ \\text{M} = \\text{V} \\text{D} \\] \\[ \\text{V} = \\frac{\\text{M}}{\\text{D}} \\]"},{"location":"brainstorm.html#the-fibonacci-part-of-the-eigen-page-but-better","title":"the Fibonacci part of the eigen page, but better","text":"<p>Here's the obvious definition of the Fibonacci sequence:</p> \\[ F_0 = 0 \\] \\[ F_1 = 1 \\] \\[ F_n = F_{n - 1} + F_{n - 2} \\] <p>Yes, I know, it normally starts at \\(1\\) and \\(2\\), but there's a reason why I'm starting it at \\(0\\) and \\(1\\). But we still have the following:</p> \\[ F_{n + 2} = F_{n + 1} + F_n \\] \\[ F_{n + 1} = F_{n + 1} \\] \\[ \\begin{bmatrix} F_{n + 2} \\\\ F_{n + 1} \\\\ \\end{bmatrix} = \\begin{bmatrix} F_{n + 1} + F_n \\\\ F_{n + 1} \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 F_{n + 1} + 1 F_n \\\\ 1 F_{n + 1} + 0 F_n \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} F_{n + 2} \\\\ F_{n + 1} \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} F_{n + 1} \\\\ F_n \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} F_2 \\\\ F_1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} F_1 \\\\ F_0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} F_3 \\\\ F_2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} F_2 \\\\ F_1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^2 \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} F_4 \\\\ F_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} F_3 \\\\ F_2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^2 \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^3 \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} F_5 \\\\ F_4 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} F_4 \\\\ F_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^3 \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^4 \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\] <p>\\(2102\\) Lines.</p> \\[ \\vdots \\] \\[ \\begin{bmatrix} F_{n + 1} \\\\ F_n \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^n \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\] <p>I know you're probably thinking: Yeah, that's a pretty nice formula. But still, how do I raise this matrix to the power of \\(n\\)? Well, to answer this question, we need a little bit of eigenvectors and eigenvalues.</p> <p>That's enough work for one day.</p> <p>The eigenvectors of a matrix \\(A\\) are vectors where if you apply the matrix's operation, that just ends up scaling the vector by some constant \\(\\lambda\\) (a.k.a. the eigenvalue). In written form, it looks like this:</p> \\[ A \\vec{v} = \\lambda \\vec{v} \\] <p>Also, an \\(n \\times n\\) matrix usually has \\(n\\) eigenvectors*.</p> <p>Side note!</p> \\[ A (c \\vec{v}) = \\lambda (c \\vec{v}) \\] <p>*What this means is that the scaled copies of a given eigenvector are also eigenvectors and have the same eigenvalue. For this reason, scaled copies of an eigenvector are usually considered as just one.</p> <p>But, assuming that there are two eigenvectors, I can do something special: I can change the basis.</p> \\[ A(a \\vec{u} + b \\vec{v}) = A(a \\vec{u}) + A(b \\vec{v}) = a(A \\vec{u}) + b(A \\vec{v}) \\] \\[ A(a \\vec{u} + b \\vec{v}) = a \\lambda_1 \\vec{u} + b \\lambda_2 \\vec{v} \\] \\[ A(A(a \\vec{u} + b \\vec{v})) = A^2 (a \\vec{u} + b \\vec{v}) = A(a \\lambda_1 \\vec{u} + b \\lambda_2 \\vec{v}) = a \\lambda_1^2 \\vec{u} + b \\lambda_2^2 \\vec{v} \\] \\[ A^2 (a \\vec{u} + b \\vec{v}) = a \\lambda_1^2 \\vec{u} + b \\lambda_2^2 \\vec{v} \\] \\[ A(A^2 (a \\vec{u} + b \\vec{v})) = A^3 (a \\vec{u} + b \\vec{v}) = A(a \\lambda_1^2 \\vec{u} + b \\lambda_2^2 \\vec{v}) = a \\lambda_1^3 \\vec{u} + b \\lambda_2^3 \\vec{v} \\] \\[ A^3 (a \\vec{u} + b \\vec{v}) = a \\lambda_1^3 \\vec{u} + b \\lambda_2^3 \\vec{v} \\] \\[ \\vdots \\] \\[ A^n (a \\vec{u} + b \\vec{v}) = a \\lambda_1^n \\vec{u} + b \\lambda_2^n \\vec{v} \\] <p>As you can see, this can be very useful for finding \\(\\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^n\\). We just need to figure out the eigenvectors, eigenvalues, and how to combine them.</p> <p>That's enough work for now.</p> <p>It's been \\(3\\) hours.</p> \\[ \\varphi^2 = \\varphi + 1 \\] \\[ \\psi^2 = \\psi + 1 \\] \\[ \\varphi = \\frac{1 + \\sqrt{5}}{2} = 1.618... \\] \\[ \\psi = \\frac{1 - \\sqrt{5}}{2} = -0.618... = -\\frac{1}{\\varphi} = 1 - \\varphi \\] \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot \\varphi + 1 \\cdot 1 \\\\ 1 \\cdot \\varphi + 0 \\cdot 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} \\varphi + 1 \\\\ \\varphi \\\\ \\end{bmatrix} = \\begin{bmatrix} \\varphi^2 \\\\ \\varphi \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix} = \\varphi \\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot \\psi + 1 \\cdot 1 \\\\ 1 \\cdot \\psi + 0 \\cdot 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} \\psi + 1 \\\\ \\psi \\\\ \\end{bmatrix} = \\begin{bmatrix} \\psi^2 \\\\ \\psi \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix} = \\psi \\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}^n (a \\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix} + b \\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix}) = a \\varphi^n \\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix} + b \\psi^n \\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix} \\] <p>But, now the question becomes: how do I write \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix}\\) in terms of \\(\\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix}\\) and \\(\\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix}\\)?</p> \\[ \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} = a \\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix} + b \\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix} \\] <p>\\(2201\\) Lines.</p> \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} (x \\hat{x} + y \\hat{y}) = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} (x \\hat{x}) + \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} (y \\hat{y}) = x \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\hat{x} + y \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\hat{y} \\] \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} = x \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + y \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} \\varphi &amp; \\psi \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ \\end{bmatrix} = a \\begin{bmatrix} \\varphi \\\\ 1 \\\\ \\end{bmatrix} + b \\begin{bmatrix} \\psi \\\\ 1 \\\\ \\end{bmatrix} \\] <p>\\(2222 + 2\\) Lines.</p> \\[ \\begin{bmatrix} \\varphi &amp; \\psi \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} \\varphi &amp; \\psi \\\\ 1 &amp; 1 \\\\ \\end{bmatrix}^{-1} \\begin{bmatrix} \\varphi &amp; \\psi \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ \\end{bmatrix} = \\begin{bmatrix} \\varphi &amp; \\psi \\\\ 1 &amp; 1 \\\\ \\end{bmatrix}^{-1} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} a \\\\ b \\\\ \\end{bmatrix} = \\begin{bmatrix} \\varphi &amp; \\psi \\\\ 1 &amp; 1 \\\\ \\end{bmatrix}^{-1} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} \\]"},{"location":"brainstorm.html#an-inductive-set-theoretic-proof-unfinished","title":"an inductive set theoretic proof (unfinished)","text":"<p>This proof partially comes from Another Roof.</p> <p>Also, before I start with how addition is set theoretically defined, I have the notation \\(\\text{S} (n)\\) pronounced \"the successor of \\(n\\)\" which means \\(n + 1\\), which can easily be defined within set theory.</p> \\[ n + 0 : = n \\] \\[ a + \\text{S} (b) : = \\text{S} (a + b) \\] <p>Here's an example:</p> \\[ n + 3 = n + \\text{S} (2) : = \\text{S} (n + 2) = \\text{S} (n + \\text{S} (1)) : = \\text{S} (\\text{S} (n + 1)) = \\text{S} (\\text{S} (n + \\text{S} (0))) : = \\text{S} (\\text{S} (\\text{S} (n + 0))) \\] \\[ n + 3 = \\text{S} (\\text{S} (\\text{S} (n))) \\] <p>Now, how do I inductively prove that the following is true?</p> \\[ a + b = b + a \\] <p>Obviously I use induction on \\(b\\). First, base case, set \\(b\\) to zero.</p> \\[ a + 0 = 0 + a \\] \\[ a + 0 : = a \\] \\[ 0 + a = a \\] <p>So, of course, to figure this out, just use induction on \\(a\\)!</p> <p>Base case:</p> \\[ 0 + 0 = 0 \\] <p>This statement is true by definition.</p> <p>Inductive reasoning:</p> \\[ 0 + a = a \\] \\[ 0 + \\text{S} (a) : = \\text{S} (0 + a) = \\text{S} (a) = \\text{S} (a) \\]"},{"location":"brainstorm.html#set-theory-3","title":"set theory 3?","text":""},{"location":"brainstorm.html#what-is-a-set","title":"what is a set?","text":"<p>A set is a collection of things...</p> <p>... Okay, fine I'll be more specific. A set can contain a certain amount of objects, so for example, I could have the set containing this mouse that I'm holding and this chair that I'm sitting on.</p> <p>So a set kind of means a collection or a group, but group theory is a whole other page.</p> <p>Note: the elements (that is, the things part of the set, a.k.a. the members of it) are not ordered within a set, and a set having the same element twice doesn't change the set. When you ask the question \"is this thing part of your set?\" I can only answer with a yes or a no.</p> <p>In other words, sets are only defined by their elements.</p> <p>\\(2300\\) Lines.</p> <p>As for some notation, the set of all even numbers would be written as \\(\\{ 2, 4, 6, 8 ... \\}\\), \\(\\{\\) numbers \\(n: n\\) is even \\(\\}\\), or \\(\\{ x: x\\) is a number and is even \\(\\}\\). (Pronounced \"the set containing \\(2\\), \\(4\\), \\(6\\), \\(8\\) and so on\", \"the set of all numbers \\(n\\) where \\(n\\) is even\", and \"the set of all \\(x\\) where \\(x\\) is a number, and furthermore, \\(x\\) is even\" respectively.)</p> <p>Also, the symbol for \"\\(x\\) is a member of \\(S\\)\" where \\(S\\) is a set is \\(x \\in S\\).</p> <p>But sets can contain more than just things, they can contain other sets.</p> <p>But this leads to a paradox: would \\(\\{S:\\) not \\(S \\in S \\}\\) (the set containing only the sets that don't contain themselves) contain itself? (P.S. there's a great video on this.)</p> <p>Clearly there's a problem with this. Let's take a step back.</p>"},{"location":"brainstorm.html#what-is-a-set-really","title":"what is a set, really?","text":""},{"location":"brainstorm.html#combinatorial-games-and-surreal-numbers","title":"combinatorial games and surreal numbers?","text":"<p>Based off of a video (HACKENBUSH: a window to a new world of math by Owen Maitzen) based off of a book (Winning ways for your mathematical plays by John Conway, Elwin Berlekamp, and Richard guy (sorry, couldn't find the book)).</p> <p>Now that I'm done with the opening credits, you're probably asking the following question:</p>"},{"location":"brainstorm.html#what-is-a-combinatorial-game","title":"what is a combinatorial game?","text":"<p>A combinatorial game is a two-player turn based game where there is no randomness or secret information. So like chess, checkers, tic-tac-toe, connect \\(4\\), games like that. From now on, I will call the players blue and red.</p> <p>But when I'm talking about things that apply to most combinatorial games, I'll actually use a game that you've probably never heard about: hackenbush.</p>"},{"location":"brainstorm.html#what-is-hackenbush","title":"what is hackenbush?","text":"<p>Great question, I'm glad you asked. Hackenbush is a simple game played on a \"bush\" made of red and blue branches. Players take turns chopping a branch of their respective color, and all the branches above fall down. You win by chopping the last branch* and lose by having no valid moves.</p> <p>*You also win if all of your opponent's branches crash down to the ground, and hence leaving them with no valid moves on their turn.</p> <p>Idk. I'd rather talk about...</p>"},{"location":"brainstorm.html#ordinals","title":"ordinals","text":"\\[ 0 = \u00d8 \\] \\[ 1 = \\{ 0 \\} \\] \\[ 2 = \\{ 1 \\} \\] \\[ 3 = \\{ 2 \\} \\] \\[ \\omega = \\{ 0, 1, 2, 3 ... \\} \\] \\[ \\omega + 1 = \\{ \\omega \\} \\] \\[ \\omega + 2 = \\{ \\omega + 1 \\} \\] \\[ \\omega + 3 = \\{ \\omega + 2 \\} \\] \\[ \\omega + \\omega = 2 \\omega = \\{ \\omega, \\omega + 1, \\omega + 2, \\omega + 3 ... \\} \\] \\[ 2 \\omega + 1 = \\{ 2 \\omega \\} \\] \\[ 2 \\omega + 2 = \\{ 2 \\omega + 1 \\} \\] \\[ 2 \\omega + 3 = \\{ 2 \\omega + 2 \\} \\] \\[ 2 \\omega + \\omega = 3 \\omega = \\{ 2 \\omega, 2 \\omega + 1, 2 \\omega + 2, 2 \\omega + 3 ... \\} \\] \\[ \\omega \\omega = \\omega^2 = \\{ \\omega, 2 \\omega, 3 \\omega ... \\} \\] \\[ \\omega^2 + 1 = \\{ \\omega^2 \\} \\] \\[ \\omega^2 + 2 = \\{ \\omega^2 + 1 \\} \\] \\[ \\omega^2 + 3 = \\{ \\omega^2 + 2 \\} \\] \\[ \\omega^2 + \\omega = \\{ \\omega^2, \\omega^2 + 1, \\omega^2 + 2, \\omega^2 + 3 ... \\} \\] \\[ \\omega^2 + \\omega + 1 = \\{ \\omega^2 + \\omega \\} \\] \\[ \\omega^2 + \\omega + 2 = \\{ \\omega^2 + \\omega + 1 \\} \\] \\[ \\omega^2 + \\omega + 3 = \\{ \\omega^2 + \\omega + 2 \\} \\] \\[ \\omega^2 + \\omega + \\omega = \\omega^2 + 2 \\omega = \\{ \\omega^2 + \\omega, \\omega^2 + \\omega + 1, \\omega^2 + \\omega + 2, \\omega^2 + \\omega + 3 ... \\} \\] \\[ \\omega^2 + \\omega \\omega = \\omega^2 + \\omega^2 = 2 \\omega^2 = \\{ \\omega^2, \\omega^2 + \\omega, \\omega^2 + 2 \\omega ... \\} \\] \\[ 2 \\omega^2 + 1 = \\{ 2 \\omega^2 \\} \\] \\[ 2 \\omega^2 + 2 = \\{ 2 \\omega^2 + 1 \\} \\] \\[ 2 \\omega^2 + 3 = \\{ 2 \\omega^2 + 2 \\} \\] \\[ 2 \\omega^2 + \\omega = \\{ 2 \\omega^2, 2 \\omega^2 + 1, 2 \\omega^2 + 2, 2 \\omega^2 + 3 ... \\} \\] \\[ 2 \\omega^2 + \\omega + 1 = \\{ 2 \\omega^2 + \\omega \\} \\] \\[ 2 \\omega^2 + \\omega + 2 = \\{ 2 \\omega^2 + \\omega + 1 \\} \\] \\[ 2 \\omega^2 + \\omega + 3 = \\{ 2 \\omega^2 + \\omega + 2 \\} \\] \\[ 2 \\omega^2 + \\omega + \\omega = 2 \\omega^2 + 2 \\omega = \\{ 2 \\omega^2 + \\omega, 2 \\omega^2 + \\omega + 1, 2 \\omega^2 + \\omega + 2, 2 \\omega^2 + \\omega + 3 ... \\} \\] <p>\\(2400\\) Lines.</p> \\[ 2 \\omega^2 + \\omega \\omega = 2 \\omega^2 + \\omega^2 = 3 \\omega^2 = \\{ 2 \\omega^2, 2 \\omega^2 + \\omega, 2 \\omega^2 + 2 \\omega ... \\} \\] \\[ \\omega \\omega^2 = \\omega^3 = \\{ \\omega^2, 2 \\omega^2, 3 \\omega^2 ... \\} \\] \\[ \\omega^3 + \\omega \\omega^2 = \\omega^3 + \\omega^3 = 2 \\omega^3 = \\{ \\omega^3, \\omega^3 + \\omega^2, \\omega^3 + 2 \\omega^2 ... \\} \\] \\[ 2 \\omega^3 + \\omega \\omega^2 = 2 \\omega^3 + \\omega^3 = 3 \\omega^3 = \\{ 2 \\omega^3, 2 \\omega^3 + \\omega^2, 2 \\omega^3 + 2 \\omega^2 ... \\} \\] \\[ \\omega \\omega^3 = \\omega^4 = \\{ \\omega^3, 2 \\omega^3, 3 \\omega^3 ... \\} \\] \\[ \\omega^{\\omega} = \\{ \\omega, \\omega^2, \\omega^3, \\omega^4 ... \\} \\] \\[ \\omega \\omega^{\\omega} = \\omega^{\\omega + 1} = \\{ \\omega^{\\omega}, 2 \\omega^{\\omega}, 3 \\omega^{\\omega} ... \\} \\] \\[ \\omega \\omega^{\\omega + 1} = \\omega^{\\omega + 2} = \\{ \\omega^{\\omega + 1}, 2 \\omega^{\\omega + 1}, 3 \\omega^{\\omega + 1} ... \\} \\] <p>Wait a minute! I can just do what I've already done, but in the exponents.</p> \\[ \\omega^{\\omega^{\\omega}} = \\{ \\omega^{\\omega}, \\omega^{\\omega^2}, \\omega^{\\omega^3} ... \\} \\] \\[ \\omega^{\\omega^{\\omega^{\\omega}}} = \\{ \\omega^{\\omega^{\\omega}}, \\omega^{\\omega^{\\omega^2}}, \\omega^{\\omega^{\\omega^3}} ... \\} \\] \\[ \\epsilon_0 = \\{ \\omega, \\omega^{\\omega}, \\omega^{\\omega^{\\omega}}, \\omega^{\\omega^{\\omega^{\\omega}}} ... \\} \\]"},{"location":"brainstorm.html#ring-theory","title":"ring theory?","text":"<p>Links: Why Negative Times Negative is Positive - Definition of Ring | Ring Theory E1, Zero Product Property is False - Divisibility, Units, Zero Divisors | Ring Theory E2, and Number Systems Invented to Solve the Hardest Problem - History of Rings | Ring Theory E0 all by EpsilonDelta, and Every Hypercomplex Number Explained #SoME4 by Johttacus J. J. Begallo</p> <p>It's another day.</p>"},{"location":"brainstorm.html#what-is-a-ring","title":"what is a ring?","text":"<p>A ring is a number system where you can add numbers, multiply numbers, and subtract numbers. More formally, a ring is a set that has operations of addition and multiplication that have the following requirements:</p> <p>\\(1\\): Closure (\\(+\\)): if \\(a\\) is in the ring and \\(b\\) is in the ring, then \\(a + b\\) is also in the ring.</p> <p>\\(2\\): Closure (\\(\\cdot\\)): if \\(a\\) is in the ring and \\(b\\) is in the ring, then \\(a b\\) is also in the ring.</p> <p>\\(3\\): Associativity (\\(+\\)): if \\(a\\), \\(b\\), and \\(c\\) are all in the ring, then \\((a + b) + c = a + (b + c) = a + b + c\\).</p> <p>\\(4\\): Associativity (\\(\\cdot\\)): if \\(a\\), \\(b\\), and \\(c\\) are all in the ring, then \\((a b) c = a (b c) = a b c\\).</p> <p>\\(5\\): Commutativity (\\(+\\)): if \\(a\\) is in the ring and \\(b\\) is in the ring, then \\(a + b = b + a\\).</p> <p>\\(6\\): Identity (\\(+\\)): there must always be a \\(0\\) where, if you pick any \\(a\\) in the ring, then \\(a + 0 = a\\).</p> <p>\\(7\\): Identity (\\(\\cdot\\)): there must always be a \\(1\\) where, if you pick any \\(a\\) in the ring, then \\(1 a = a 1 = a\\).</p> <p>\\(8\\): Inverses (\\(+\\)): if \\(a\\) is in the ring, then there must always be a \\(-a\\) in the ring where \\(a + -a = 0\\).</p> <p>\\(9\\): Distributivity (\\(+\\) &amp; \\(\\cdot\\)): if \\(a\\), \\(b\\), and \\(c\\) are all in the ring, then \\(a (b + c) = (b + c) a = a b + a c\\).</p> <p>So, if you know the definition of a group, then ignoring distributivity gives you the following: if you remove multiplication, you're left with an abelian group, and if you remove addition, you're left with something that's like a group, but it has no inverse requirement.</p> <p>It's another day.</p> <p>Notice: I said addition, multiplication, and subtraction at the start. This is to emphasize that you can take the negatives of things, but not necessarily the multiplicative inverses.</p> <p>After re-watching the first half of Why Negative Times Negative is Positive - Definition of Ring | Ring Theory E1 yesterday, or two days ago, or three days ago, or however long ago I last worked on this, I have forgotten what's inside of the video, and I'll just do something else for the next part.</p>"},{"location":"brainstorm.html#why-is-a-ring","title":"why is a ring?","text":"<p>This isn't a chapter as much as it is the reason why these rules are in place, especially distributivity. It turns out, some of these rules are designed so that the first definition of multiplication you learned actually works.</p> \\[ R \\text{ is a ring} \\] \\[ a \\text{ is in } R \\] \\[ 3 a = (1 + 1 + 1) a = 1 a + 1 a + 1 a = a + a + a \\] <p>I came up with that in the middle of the night.</p> \\[ \\frac{1}{3} \\text{ is in } R \\] \\[ \\frac{1}{3} a = x \\] \\[ 3 (\\frac{1}{3} a) = 3 x \\] \\[ \\frac{1}{3} a + \\frac{1}{3} a + \\frac{1}{3} a = (\\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3}) a = x + x + x \\] \\[ \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} : = 1 \\] \\[ (\\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3}) a = 1 a = a \\] \\[ x + x + x = a \\] <p>I came up with that after seeing a scene in the movie Arrival where someone said something like \\(\\frac{1}{6}\\) means \\(1\\) of \\(6\\) parts.</p> \\[ b \\text{ is in } R \\] \\[ a b = a (b + 0) = a b + a 0 \\] \\[ a b = a b + a 0 \\] \\[ a b + -(a b) = a b + a 0 + -(a b) \\] \\[ a 0 = 0 \\] <p>\\(2500\\) Lines.</p> <p>I came up with that after trying to prove that it's true.</p> \\[ (-1) a = x \\] \\[ a 0 = 0 \\] \\[ a 0 = a (1 + (-1)) = a 1 + a (-1) = a + a (-1) \\] \\[ a + a (-1) = 0 \\] \\[ a + a (-1) + (-a) = 0 + (-a) \\] \\[ a (-1) = -a \\] <p>I came up with this one while writing it, and I think it was also in Why Negative Times Negative is Positive - Definition of Ring | Ring Theory E1.</p> <p>There's probably more to put in this chapter, but you can just watch Why Negative Times Negative is Positive - Definition of Ring | Ring Theory E1</p>"},{"location":"brainstorm.html#basic-ring-information","title":"basic ring information","text":"<p>You might have noticed that it's been a while (actually, I think it's been a month), and that's because I always decided to work on the problem set I had instead of working on this page, but now problem set \\(3\\) has been turned in, I can finally work on this page!</p> <p>But anyways, I thought it would be helpful if I showed examples of what's a ring and what isn't a ring. The definition of a ring is very general, so most things you can think of as number systems count as rings. (And even some things you wouldn't ever really think of as number systems are rings!)</p> <p>The most famous rings I can think of are, well, there's the integers, the rational numbers, the real numbers, matrices, and complex numbers.</p> <p>Something strange just happened: when I pressed the edit button, for the first time in this website's history, there wasn't an empty line.</p> <p>Anyways, it's Wednesday. Wednesday is a week after midterms, which is a week and a day after problem set five, and it said earlier that it was problem set three, so it's been a month.</p> <p>What about the different types of rings? Well, a commutative ring is one where the multiplication operation is commutative.  (Remember how I only wrote down commutativity for addition?) A division ring is one where every number has a multiplicative inverse. If something is both a commutative and a division ring, it's called a field, the highest honor a ring can have.</p> <p>But the requirements for a division ring still seem too strict (and don't allow for primes, because any \\(p\\) is equal to \\(x (x^{-1} p)\\), for \\(x \\ne 0\\)). If only there was another requirement that all division rings have where it would be nice if you had it for your ring. Well, it turns out there is! They're called cancelable rings, and they're called that because in a cancelable ring, the following holds:</p> \\[ a b = a c, a \\ne 0 \u2192 b = c \\] <p>It turns out that this is the same as the property that if \\(ab = 0\\), then \\(a = 0\\) or \\(b = 0\\). If this is not the case, then \\(a\\) is called a left zero divisor and \\(b\\) is called a right zero divisor. Here's a proof that cancelable ring and no zero divisors are the same statement:</p> <p>Proof of backward case:</p> \\[ a b = a c \\] \\[ a \\ne 0 \\] \\[ a b - a c = 0 \\] \\[ a (b - c) = 0 \\] \\[ \\text{Because } a \\ne 0 \\text{, } b - c = 0 \\] \\[ b = c \\] <p>\\(2555\\) Lines.</p> <p>Proof of forward case:</p> \\[ a b = 0 \\] \\[ a (b + 0) = 0 \\] \\[ a b + a 0 = 0 \\] <p>This is obviously true if \\(a = 0\\), but if \\(a \\ne 0\\) then we can use our assumption that the ring is cancelable. This would mean that \\(b = 0\\) which, together with \\(a = 0\\) being a solution, means that means that if \\(ab = 0\\) then \\(a = 0\\) or \\(b = 0\\).</p> <p>Now it's time to finish categorizing rings. As you know, if a group has no zero divisors, then it's cancelable. But if a ring is cancelable and is also commutative, then it's called an integral ring (because it behaves like the integers). Also, if something is a division ring, then it should be obvious that there are no zero divisors, because if \\(ab = 0\\) and \\(a\\) and \\(b\\) are both nonzero, then you can just divide by \\(b\\) and then get that \\(a = 0\\).</p>"},{"location":"calculus_2.html","title":"Calculus part 2","text":""},{"location":"calculus_2.html#more-about-e","title":"More about \\(e\\)","text":"\\[ e = (1 + dx)^{\\frac{1}{dx}} = \\lim_{\\Delta x \\to 0} (1 + \\Delta x)^{\\frac{1}{\\Delta x}} = \\lim_{N \\to \\infty} (1 + \\frac{1}{N})^N \\] \\[ e^x = \\lim_{N \\to \\infty} (1 + \\frac{1}{N})^{Nx} = \\lim_{N \\to \\infty} (1 + \\frac{x}{Nx})^{Nx} \\] \\[ Nx \\to \\infty \\] \\[ N \\to \\infty \\] \\[ e^x = \\lim_{N \\to \\infty} (1 + \\frac{x}{N})^{N} \\] \\[ e^x = (1 + x dx)^{\\frac{1}{dx}} \\] <p>you will see why this is useful here</p> \\[ \\sum\\limits_{n=0}^{\\infty}C_n x^n = e^x \\] \\[ \\sum\\limits_{n=0}^{\\infty}C_n n x^{n - 1} = e^x = \\sum\\limits_{n=0}^{\\infty}C_n x^n \\] \\[ \\sum\\limits_{n=0}^{\\infty}C_n n x^{n - 1} = \\sum\\limits_{n=-1}^{\\infty}C_{n + 1} (n + 1) x^n = c_0 0 x^{-1} + \\sum\\limits_{n=0}^{\\infty}C_{n + 1} (n + 1) x^n = \\sum\\limits_{n=0}^{\\infty}C_n x^n \\]"},{"location":"calculus_2.html#more-about-e-to-the-x","title":"More about \\(e\\) to the \\(x\\)","text":"\\[ C_{n + 1} (n + 1) = C_n \\] \\[ C_{n + 1} = \\frac{C_n}{n + 1} \\] \\[ C_n = \\frac{C_{n - 1}}{n} \\] \\[ e^0 = 1 \\] \\[ C_0 = 1 \\] \\[ C_n = \\frac{1}{n!} \\] \\[ e^x = \\sum\\limits_{n=0}^{\\infty}\\frac{x^n}{n!} \\] \\[ e = \\sum\\limits_{n=0}^{\\infty}\\frac{1}{n!} \\] \\[ \\sum\\limits_{n=0}^{\\infty}C_n x^n = e^x \\] \\[ \\sum\\limits_{n=0}^{\\infty}C_n n x^{n - 1} = e^x = \\sum\\limits_{n=0}^{\\infty}C_n x^n \\] \\[ \\sum\\limits_{n=0}^{\\infty}C_n n x^{n - 1} = \\sum\\limits_{n=-1}^{\\infty}C_{n + 1} (n + 1) x^n = c_0 0 x^{-1} + \\sum\\limits_{n=0}^{\\infty}C_{n + 1} (n + 1) x^n = \\sum\\limits_{n=0}^{\\infty}C_n x^n \\] \\[ C_{n + 1} (n + 1) = C_n \\] \\[ C_{n + 1} = \\frac{C_n}{n + 1} \\] \\[ C_n = \\frac{C_{n - 1}}{n} \\] \\[ e^0 = 1 \\] \\[ C_0 = 1 \\] \\[ C_n = \\frac{1}{n!} \\] \\[ e^x = \\sum\\limits_{n=0}^{\\infty}\\frac{x^n}{n!} \\]"},{"location":"calculus_2.html#quotient-rule","title":"quotient rule","text":"\\[ (\\frac{f}{g})\\prime = ? \\] \\[ \\frac{f(x)}{g(x)} = h(x) \\] \\[ g(x) h(x) = f(x) \\] \\[ (g(x) h(x))\\prime = f\\prime (x) \\] \\[ g\\prime (x) h(x) + g(x) h\\prime (x) = f\\prime (x) \\] \\[ g(x) h\\prime (x) = f\\prime (x) - g\\prime (x) h(x) \\] \\[ g(x) h\\prime (x) = f\\prime (x) - g\\prime (x) \\frac{f(x)}{g(x)} \\] \\[ g(x) h\\prime (x) = f\\prime (x) - \\frac{f(x) g\\prime(x)}{g(x)} \\] \\[ g(x) h\\prime (x) = \\frac{f\\prime (x) g(x)}{g(x)} - \\frac{f(x) g\\prime (x)}{g(x)} \\] \\[ g(x) h\\prime (x) = \\frac{f\\prime (x) g(x) - f(x) g\\prime (x)}{g(x)} \\] \\[ h\\prime (x) = \\frac{f\\prime (x) g(x) - f(x) g\\prime (x)}{g^2(x)} \\] \\[ (\\frac{f}{g})\\prime = \\frac{f\\prime g - f g\\prime}{g^2} \\]"},{"location":"calculus_2.html#lhopitals-rule","title":"L'Hopital's rule","text":"\\[ f(c) = 0 \\] \\[ g(c) = 0 \\] \\[ ? = \\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\frac{f(c + dx)}{g(c + dx)} = \\frac{f(c) + df_c}{g(c) + dg_c} = \\frac{df_c}{dg_c} = \\frac{\\frac{df_c}{dc}}{\\frac{dg_c}{dc}} \\] \\[ \\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\lim_{x \\to c} \\frac{\\frac{d}{dx} f(x)}{\\frac{d}{dx} g(x)} \\]"},{"location":"complex.html","title":"Complex","text":""},{"location":"complex.html#complex-numbers","title":"Complex numbers","text":"\\[ i = \\sqrt{-1} \\] \\[ a + bi = c + di \\iff c = a, d = b \\] \\[ (a + bi)(c + di) = ac + adi + bci + bdii = ac + adi + bci - bd = (ac - bd) + (ad + bc)i \\] \\[ (a + bi)^2 = (aa - bb) + (ab + ab)i = (a^2 - b^2) + 2abi \\] \\[ (a + bi)r = ar + bri \\] \\[ \\text{ccong} (a + bi) = a - bi \\] <p>\\(f\\) is the definition of \\(sin(\\theta)\\)</p> <p>\\(g\\) is the definition of \\(cos(\\theta)\\)</p> <p>\\(\\theta\\) is the angle from the right of the \\(x\\) axis to the line \\(h\\)</p> <p>Now, I will define \\(f(x)\\) as a function that takes in a real number and outputs a complex number that is \\(x\\) radians around the unit circle, so like the geogebra file above, but in the complex plane. using the file, we can say that \\(f(x) = cos(x) + isin(x)\\), and any complex number \\(z\\) has an angle \\(\\theta\\) measured in radians and a distance \\(r\\) from the origin, so \\(z\\) equals \\(rf(\\theta)\\), except people don't use \\(f(\\theta)\\) because there are other complex functions \\(f(z)\\), and \\(rcos(x) + irsin(x)\\) doesn't reach the high bar for perfection set by mathematicians </p> <p>so now, the question is: solve for \\(f(x)\\)</p> <p>i'll start by multiplying \\(f(x) \\cdot f(y)\\)</p> <p>and using angle addition derived in the trigonometry section...</p> \\[ f(x)f(y) = (cos(x) + isin(x))(cos(y) + isin(y)) = (cos(x)cos(y) - sin(x)sin(y)) + (cos(x)sin(y) + sin(x)cos(y))i = cos(x + y) + isin(x + y) = f(x + y) \\] <p>hmmm \\(f(x)f(y) = f(x + y)\\) sounds familiar... Oh right! This is an exponential, but what's the base?</p> <p>while, a common proof that I once used that if the derivative of \\(g(x)\\) is \\(g(x)\\), then \\(g(x) = ce^x\\), it has a method of:</p> <p>\\(\\frac{g(x + dx) - g(x)}{dx} = g(x)\\), \\(g(x + dx) = g(x)(1 + dx)\\), \\(g(x + 2dx) = g(x)(1 + dx)^2\\), \\(g(x + ndx) = g(x)(1 + dx)^n\\), \\(g(0 + \\frac{x}{dx} dx) = g(x) = g(0)(1 + dx)^{\\frac{x}{dx}}\\)</p> <p>and then, using facts from Calculus part 2, \\(g(x) = g(0)e^x = ce^x\\).</p> <p>So I will use a proof like that, the proof will go like this:</p>"},{"location":"complex.html#proof","title":"proof","text":"\\[ f(x + dx) = f(x)f(dx) \\] \\[ f(dx) = cos(dx) + isin(dx) = cos(0 + dx) + isin(0 + dx) = (cos(0) + dx \\text{ } cos'(0)) + (sin(0) + dx \\text{ } sin'(0))i \\] <p>\\(cos(x)\\) reaches a peak at \\(0\\), so \\(cos'(0) = 0\\)</p> <p>while as you zoom into the right of the unit circle, the height is the distance traveled and the sine of a small angle is that angle, so \\(sin'(0) = 1\\)</p> \\[ f(dx) = (1 + 0dx) + (0 + 1dx)i = 1 + dxi \\] \\[ f(x + dx) = f(x)(1 + dxi)\\] \\[ f(x + 2dx) = f(x)(1 + dxi)^2 \\] \\[ f(x + ndx) = f(x)(1 + dxi)^n \\] <p>and if you saw Calculus part 2, you know that...</p> \\[ f(0 + \\frac{x}{dx}dx) = f(x) = f(0)((1 + dxi)^{\\frac{1}{dx}})^x = f(0)(e^i)^x \\] <p>so surprisingly, the base of the exponential is \\(e^i\\), except many of you would've guessed that because of eueler's identity, which I was trying to derive, anyways there is one more step (more like three but you get the point)</p> \\[ f(0) = 1 \\] \\[ f(x) = e^{ix} \\] <p>thus:</p> \\[ cos(x) + isin(x) = e^{ix} \\] <p>proof complete!</p>"},{"location":"complex.html#now-what","title":"now what?","text":"<p>well everything! Now complex multiplication is much easier, for example \\(z \\cdot w = r e^{i \\theta} \\rho e^{i \\psi} = r \\rho e^{\\theta + \\psi}\\)</p> <p>and if multiplying by a real number is scaling the distance from the origin, then complex multiplication has the much more intuitive interpretation that most people teach: multiplying the magnitudes and adding the angles.</p> <p>another interpretation of complex multiplication is that if \\(w(z) = wz\\) then \\(w(1) = w\\) and scaling the input scales the output and then rotating the input rotates the output, in other words complex multiplication looks like scaling and rotating</p> <p>Except I think that just saying things like \"multiplying by a number on the unit circle rotates a point counterclockwise\" which by the way is true, needs rigorous proof. And I wanted to add more cases, but this page only needs one more fact before saying \"now everything\" so that the right of the page makes sense, anyways the fact is that the only reason why mathematicians use \\(e^{i \\theta}\\) is either to show that \\(e^{i \\theta} e^{i \\psi} = e^{i(\\theta + \\psi)}\\), to show that \\(\\frac{d}{dx} e^{ix} = ie^{ix}\\) which makes sense if you think about it, or to confuse complex number beginners possibly making them not want to learn about complex numbers in the first place</p>"},{"location":"complex.html#now-everything","title":"now everything","text":"<p>quaternions!</p> <p>jk</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>ok, if you want quaternions, then...</p> <p>you have to find it!</p> <p>..........</p> <p>...........</p> <p>..........</p> <p>...........</p> <p>...........</p>"},{"location":"complex.html#the-holy-grail-of-complex-numbers-i-forgot-to-do-this-3-months-ago","title":"the Holy Grail of complex numbers, I forgot to do this \\(3\\) months ago","text":"\\[ cos(\\pi) + isin(\\pi) = e^{i \\pi} \\] \\[ e^{i \\pi} = -1 + i0 = -1 \\] \\[ e^{i \\pi} + 1 = 0 \\]"},{"location":"complex_2.html","title":"Complex II","text":""},{"location":"complex_2.html#the-roots-of-unity","title":"the roots of unity","text":"\\[ z = r e^{i \\theta} \\] \\[ \\tau = : 2 \\pi \\] \\[ e^{i \\tau} = 1 \\] \\[ e^{k i \\tau} = 1 \\] \\[ z = r e^{i \\theta + k i \\tau} \\] \\[ z^n = 1 \\] \\[ (r e^{i \\theta} e^{k i \\tau})^n = 1 \\] \\[ r^n {e^{i \\theta}}^n {e^{k i \\tau}}^n = 1 \\] \\[ r^n e^{i \\theta n} e^{k i \\tau n} = 1 \\] \\[ r^n = 1 \\] \\[ r = 1 \\] \\[ e^{i \\theta n} e^{k i \\tau n} = 1 \\] \\[ e^{i \\theta n} = e^{m i \\tau} \\] \\[ (z^n)^m = 1^m = 1 \\] \\[ i \\theta n = m i \\tau \\] \\[ \\theta n = \\tau m \\] \\[ \\theta = \\tau \\frac{m}{n} \\] \\[ rou_{m, n} = e^{i \\tau \\frac{m}{n}} \\] <p>The \"rou\" means root of unity, because unity means \\(1\\), and root because \\(z\\) is the \\(n\\)'th root of \\(1\\). but all of the roots of unity can be constructed with what is called the principal value \\(rou_{1, n}\\) (actually no, the principal value is \\(rou_{0, n}\\), so just \\(1\\)), but that is:</p> \\[ e^{i \\frac{\\tau}{n}} \\] <p>This is because the more common \\(e^{i \\tau \\frac{m}{n}}\\) is just \\((e^{i \\frac{\\tau}{n}})^m\\), so the following statement is:</p> \\[ rou_{m, n} = (rou_{1, n})^m \\] \\[ rou_n = : rou_{1, n} \\] \\[ rou_{m, n} = rou_n^m \\] <p>This also looks kind of like notation, but it is just a power.</p>"},{"location":"derivatives.html","title":"Calculus","text":""},{"location":"derivatives.html#derivatives","title":"Derivatives","text":"\\[ \\frac{f(x + dx) - f(x)}{dx} = \\frac{df}{dx} = f\\prime (x) \\]"},{"location":"derivatives.html#sum-rule","title":"sum rule","text":"\\[ \\frac{d(f(x) + g(x))}{dx} = \\frac{f(x + dx) + g(x + dx) - f(x) -g(x)}{dx} = \\frac{f(x + dx) - f(x)}{dx} + \\frac{g(x + dx) - g(x)}{dx} = \\frac{df}{dx} + \\frac{dg}{dx} \\] \\[ \\frac{d(f(x) + g(x))}{dx} = \\frac{df}{dx} + \\frac{dg}{dx} \\] \\[ (f + g)\\prime = f\\prime + g\\prime \\]"},{"location":"derivatives.html#product-rule","title":"product rule","text":"\\[ f(x + dx) = f(x) + df \\] \\[ \\frac{d(f(x)g(x))}{dx} = \\frac{(f(x) + df)(g(x) + dg) - f(x)g(x)}{dx} = \\frac{f(x)g(x) + f(x)dg + dfg(x) + dfdg - f(x)g(x)}{dx} = f(x)\\frac{dg}{dx} + \\frac{df}{dx}g(x) + \\frac{dfdg}{dx} \\] \\[ \\frac{dfdg}{dx} \\to 0 \\] \\[ (fg)\\prime = fg\\prime + f\\prime g \\]"},{"location":"derivatives.html#chain-rule","title":"chain rule","text":"\\[ \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} = \\frac{df}{dx} (x) \\] \\[ \\Delta x \\to 0 \\] \\[ \\frac{d(f(g(x)))}{dx} = \\frac{f(g(x + dx)) - f(g(x))}{dx} = \\frac{f(g(x) + dg) - f(g(x))}{dx} \\] \\[ dg \\to 0 \\] \\[ \\frac{f(g(x) + dg) - f(g(x))}{dg} = \\frac{df}{dx}(g(x)) \\] \\[ \\frac{f(g(x) + dg) - f(g(x))}{dg} \\frac{dg}{dx} = \\frac{df}{dx}(g(x)) \\frac{dg}{dx} = \\frac{f(g(x) + dg) - f(g(x))}{dx} = \\frac{d(f(g(x)))}{dx} \\] \\[ \\frac{d(f(g(x)))}{dx} = \\frac{df}{dx}(g(x))\\frac{dg}{dx} \\] \\[ (f(g))\\prime = f\\prime(g) g\\prime \\]"},{"location":"derivatives.html#mbc-rule","title":"mbc rule","text":"\\[ \\frac{dcf(x)}{dx} = c f\\prime(x) \\] \\[ (cf)\\prime = c f\\prime \\]"},{"location":"derivatives.html#exponent-rule","title":"exponent rule","text":"\\[ \\frac{d(a^x)}{dx} = \\frac{a^{x + dx} -a^x}{dx} = \\frac{a^x a^{dx} - a^x}{dx} = a^x \\frac{a^{dx} - 1}{dx} \\] \\[ \\frac{d(a^x)}{dx} = a^x \\frac{a^{dx} - 1}{dx} \\] \\[ \\text{(lets figure this out later!)} \\]"},{"location":"derivatives.html#e","title":"e","text":"\\[ \\frac{de^x}{dx} = e^x = e^x \\frac{e^{dx} - 1}{dx} \\] \\[ \\frac{e^{dx} - 1}{dx} = 1 \\] \\[ e^{dx} -1 = dx \\] \\[ e^{dx} = 1 + dx \\] \\[ e = (1 + dx)^{\\frac{1}{dx}} \\] \\[ log_e (x) = : ln(x) \\]"},{"location":"derivatives.html#logarithmic-derivitave","title":"logarithmic derivitave","text":"\\[ e^{ln(f(x))} = f(x) \\] \\[ \\frac{d(e^{ln(f(x)})}{dx} = (ln(f(x)))\\prime e^{ln(f(x))} = f(x) (ln(f(x)))\\prime = f\\prime(x) \\] \\[ (ln(f))\\prime = \\frac{f\\prime}{f} \\] \\[ f(x) = a^x \\] \\[ ln(f(x)) = ln(a^x) = x ln(a) \\] \\[ \\frac{f\\prime (x)}{f(x)} = ln(a) \\] \\[ (a^x)\\prime = a^x ln(a) \\]"},{"location":"derivatives.html#power-rule","title":"power rule","text":"\\[ f(x) = x^n \\] \\[ ln(f(x)) = ln(x^n) = n ln(x) \\] \\[ ln\\prime(x) = \\frac{1}{x} \\] \\[ \\frac{f\\prime(x)}{f(x)} = \\frac{n}{x} \\] \\[ (x^n)\\prime = x^n \\frac{n}{x} \\] \\[ (x^n)\\prime = n x^{n - 1} \\] \\[ \\text{in exactly } 100 \\text{ lines.} \\]"},{"location":"eigen.html","title":"Eigenstuff","text":"<p>Learn more here, here, here, here, here, here, and here in that order if you stop at 3:54 on the third, 10:44 on the fourth, 16:01 on the fifth, 4:53 on the sixth, and 12:21 on the seventh</p> <p>by the way, any matrix to the power of zero is the identity matrix \\(I\\)</p> <p>and you can add and subtract matrices term by term</p>"},{"location":"eigen.html#i-want-to-take-a-matrix-to-a-large-power-n","title":"I want to take a matrix to a large power \\(n\\)","text":"<p>if the matrix (I'll call it \\(A\\)) is diagonal, in other words if it is of the form below</p> \\[ \\begin{bmatrix} a &amp; 0 &amp; 0 &amp; \\dots \\\\ 0 &amp; b &amp; 0 &amp; \\dots \\\\ 0 &amp; 0 &amp; c &amp; \\dots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\\\ \\end{bmatrix} \\] <p>or because I am assuming \\(A\\) is a \\(2 \\times 2\\) matrix, it's probably more like</p> \\[ \\begin{bmatrix} a &amp; 0 \\\\ 0 &amp; b \\\\ \\end{bmatrix}\\] <p>than \\(A^n\\) is the matrix below:</p> \\[ \\begin{bmatrix} a^n &amp; 0 \\\\ 0 &amp; b^n \\\\ \\end{bmatrix}\\] <p>but if \\(A\\) is not diagonal, than just switch to a basis (in particular, an eigenbasis that I will dedicate the rest of the page or) the eigenvector as it is called, is also just useful in other cases like when you need to find the next fibonacci number, or find the flow in a population of foxes and rabbits (even if you need to see the entire second video) which I will do both of those at the end of the page</p> <p>So the top question is...</p>"},{"location":"eigen.html#what-even-is-an-eigenvector","title":"what even is an eigenvector?","text":"<p>an eigenvector is really a set of vectors (that I will refer to from now on as an eigenset) that are all multiples of each other can be defined as the slope of the line that connects the tips of vectors in said eigenset, and an eigenvector is defined as a vector within said eigenset, unfortunately the slope is not the corresponding eigenvalue, but that this slope can also be \\(\\frac{1}{0}\\) if it is pointing straight up, also on the note of eigensets, there can be two (two slopes, not two vectors) with any given matrix, I'll give you an example</p> <p>but you are probably just waiting for the definition, and here it is:</p> <p>under multiplication by matrix \\(A\\), most vectors are rotated and scaled, but any eigenvector (or vector within the eigenset) is just scaled, and it makes sense that \\(2\\) times the eigenvector isn't scaled either, thus every vector within the eigenset is an eigenvector, and the eigenvalue? that corresponds to how much each vector is scaled, and two different eigensets usually have different eigenvalues.</p> <p>But you are probably just waiting for the example, and here it is:</p> <p>a matrix with a determinant of \\(0\\), and a rank of \\(1\\) has eigensets of the line that it squishes space onto, and the null space of the matrix.</p> <p>Anyways, on to my favorite part! (And reason I made this page)</p>"},{"location":"eigen.html#how-to-find-the-eigenvectors-and-eigenvalues","title":"how to find the eigenvectors and eigenvalues","text":"\\[ \\text{eigenvector} = \\vec{v} = \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix} \\] \\[ \\text{eigenvalue} = \\lambda \\] \\[ \\text{eigenset} = r = \\frac{v_y}{v_x} \\] \\[ \\text{matrix} = A \\] \\[ A \\vec{v} = \\lambda \\vec{v} \\] \\[ I = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\] \\[ I \\vec{v} = \\vec{v} \\] \\[ A \\vec{v} = \\lambda (I \\vec{v}) = (\\lambda I) \\vec{v} \\] \\[ A \\vec{v} - (\\lambda I) \\vec{v} = \\vec{0} \\] \\[ (A - \\lambda I) \\vec{v} = \\vec{0} \\] \\[ \\lambda I = \\begin{bmatrix} \\lambda &amp; 0 \\\\ 0 &amp; \\lambda \\\\ \\end{bmatrix} \\] \\[ A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} a - \\lambda &amp; b \\\\ c &amp; d - \\lambda \\\\ \\end{bmatrix} \\vec{v} = \\vec{0} \\] \\[ \\begin{bmatrix} a - \\lambda &amp; b \\\\ c &amp; d - \\lambda \\\\ \\end{bmatrix} = A^{\\star} \\] \\[ a - \\lambda = a^{\\star} \\] \\[ b = b^{\\star} \\] \\[ c = c^{\\star} \\] \\[ d - \\lambda = d^{\\star} \\] \\[ A^{\\star} = \\begin{bmatrix} a^{\\star} &amp; b^{\\star} \\\\ c^{\\star} &amp; d^{\\star} \\\\ \\end{bmatrix} \\] \\[ A^{\\star} \\vec{v} = \\vec{0} \\] \\[ \\vec{v} = v_x \\hat{i} + v_y \\hat{j} \\neq \\vec{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\end{bmatrix} = 0 \\hat{i} + 0 \\hat{j}  \\] \\[ A^{\\star} \\vec{v} = A^{\\star} (v_x \\hat{i} + v_y \\hat{j}) = v_x (A^{\\star} \\hat{i}) + v_y (A^{\\star} \\hat{j}) = 0 \\] <p>\\(100\\) lines.</p> \\[ v_x (A^{\\star} \\hat{i}) = - v_y (A^{\\star} \\hat{j}) \\] \\[ -\\frac{v_x}{v_y} A^{\\star} \\hat{i} = A^{\\star} \\hat{j} \\] \\[ -\\frac{v_x}{v_y} A^{\\star}_x = A^{\\star}_y \\] \\[ -\\frac{v_x}{v_y} \\begin{bmatrix} a^{\\star} \\\\ c^{\\star} \\\\ \\end{bmatrix} = \\begin{bmatrix} b^{\\star} \\\\ d^{\\star} \\\\ \\end{bmatrix} \\] \\[ -\\frac{1}{r} \\begin{bmatrix} a^{\\star} \\\\ c^{\\star} \\\\ \\end{bmatrix} = \\begin{bmatrix} b^{\\star} \\\\ d^{\\star} \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} a^{\\star} \\\\ c^{\\star} \\\\ \\end{bmatrix} = \\begin{bmatrix} -r b^{\\star} \\\\ -r d^{\\star} \\\\ \\end{bmatrix} \\] \\[ a^{\\star} = -r b^{\\star} \\] \\[ c^{\\star} = -r d^{\\star} \\] \\[ -r d^{\\star} = c^{\\star} \\] \\[ -r a^{\\star} d^{\\star} = -b^{\\star} r c^{\\star} \\] \\[ a^{\\star} d^{\\star} = b^{\\star} c^{\\star} \\] \\[ a^{\\star} d^{\\star} - b^{\\star} c^{\\star} = 0 \\] \\[ (a - \\lambda)(d - \\lambda) - bc = 0 \\] \\[ ad - a \\lambda - d \\lambda + \\lambda^2 - bc = 0 \\] \\[ (1) \\lambda^2 + (-(a + d)) \\lambda + (ad - bc) = 0 \\] \\[ \\lambda = \\frac{a + d}{2} \\pm \\frac{\\sqrt{a^2 + 2ad + d^2 - 4ad+ 4bc}}{2} = \\frac{a + d}{2} \\pm \\frac{\\sqrt{a^2 - 2ad + d^2 + 4bc}}{2} \\] \\[ \\lambda = \\begin{Bmatrix} bd = 0 \\to \\lambda = a, \\lambda = d \\\\ bd \\neq 0 \\to \\lambda = \\frac{a + d + \\sqrt{(a - d)^2 + 4bc}}{2}, \\lambda = \\frac{a + d - \\sqrt{(a - d)^2 + 4bc}}{2} \\\\ \\end{Bmatrix} \\] \\[ -r = \\frac{a^{\\star}}{b^{\\star}} = \\frac{c^{\\star}}{d^{\\star}} \\] \\[ r = -\\frac{a^{\\star}}{b^{\\star}} = -\\frac{c^{\\star}}{d^{\\star}} = -\\frac{a - \\lambda}{b} = -\\frac{c}{d - \\lambda} \\] <p>Another thing that is important is that if an eigenset is stable equilibrium (sorry, I couldn't find a good video), the eigenvector is an equilibrium.</p> <p>So if vectors will slowly drift towards the eigenset as you apply the transformation, then it is convergent, but if vectors will slowly drift away from the eigenset as you apply the transformation, then it is divergent.</p> <p>I wanted to prove how to compute this, but then I had to find out the point to line distance formula, which requires the inverse pythagorean theorem, which requires the pythagorean theorem, and I don't know how to prove that without geometry</p> <p>so I will just tell you if an eigenset is convergent or not in the next part which I said I would do eventually... </p>"},{"location":"eigen.html#fibonacci","title":"fibonacci","text":"\\[ F_n = F_{n - 1} + F_{n - 2} \\] \\[ F_1 = 1 \\] \\[ F_2 = 1 \\] \\[ F_{n + 1} = ?(F_n) \\] \\[ F_n = 1 F_{n - 1} + 1 F_{n - 2} \\] \\[ F_{n - 1} = 1 F_{n - 1} + 0 F_{n - 2} \\] \\[ \\begin{bmatrix} F_n \\\\ F_{n - 1} \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} F_{n - 1} \\\\ F_{n - 2} \\\\ \\end{bmatrix} \\] \\[ \\lambda = \\frac{1 + \\sqrt{5}}{2} = \\phi, \\lambda = \\frac{1 - \\sqrt{5}}{2} = \\psi \\] \\[ \\phi = : 1 + \\frac{1}{\\phi} = \\frac{1 + \\sqrt{5}}{2} \\] \\[ \\psi = : 1 + \\frac{1}{\\psi} = \\frac{1 - \\sqrt{5}}{2} \\] <p>The \\(\\phi\\) one is convergent, the \\(\\psi\\) one is not.</p> \\[ r = \\frac{1}{\\phi}, r =\\frac{1}{\\psi}  \\] <p>Once again, the \\(\\phi\\) one is convergent, the \\(\\psi\\) one is not.</p> <p>but, if it is convergent when making the next fibonacci number, then all possible starting conditions* will eventually approach the \\(\\frac{F_n}{F_{n - 1}} = \\phi\\), but if that is true then...</p> <p>*Even if it is not stable, if the starting conditions lie directly on the alternative line, the ratio of terms doesn't approach, but always is \\(\\psi\\), but that is irrational, so an easy fix is to make both of the initial conditions integers. Anyways...</p> \\[ \\frac{F_{n + 1}}{F_n} \\to \\phi \\] \\[ F_{n + 1} \\approx \\phi F_n \\] <p>okay, I have the next fibonacci number, onto foxes and rabbits</p>"},{"location":"eigen.html#which-one-would-win-in-a-boxing-match-the-fox-or-the-rabbit","title":"which one would win in a boxing match, the fox or the rabbit?","text":"<p>\\(200\\) lines.</p> \\[ \\frac{d}{dt} \\begin{bmatrix} f(t) \\\\ g(t) \\\\ \\end{bmatrix} = \\begin{bmatrix} f\\prime (t) \\\\ g\\prime (t) \\\\ \\end{bmatrix} \\] \\[ r(t) = \\text{ The number of rabbits.} \\] \\[ f(t) = \\text{ The number of foxes.} \\] \\[ r\\prime (t) = \\alpha r(t) - \\beta f(t) \\] \\[ f\\prime (t) = \\gamma f(t) + \\delta r(t) \\] \\[ \\begin{bmatrix} r\\prime (t) \\\\ f\\prime (t) \\\\ \\end{bmatrix} = \\begin{bmatrix} \\alpha &amp; - \\beta \\\\ \\delta &amp; \\gamma \\\\ \\end{bmatrix} \\begin{bmatrix} r(t) \\\\ f(t) \\\\ \\end{bmatrix} \\] \\[ \\lambda = \\begin{Bmatrix} \\beta \\gamma = 0 \\to \\lambda = \\alpha, \\lambda = \\gamma \\\\ \\beta \\gamma \\neq 0 \\to \\lambda = \\frac{\\alpha + \\gamma + \\sqrt{(\\alpha - \\gamma)^2 - 4 \\beta \\delta}}{2}, \\lambda = \\frac{\\alpha + \\gamma - \\sqrt{(\\alpha - \\gamma)^2 - 4 \\beta \\delta}}{2} \\\\ \\end{Bmatrix} \\] \\[ r = \\frac{\\alpha - \\lambda}{\\beta} \\]"},{"location":"fibbonacci.html","title":"Fibbonacci","text":""},{"location":"fibbonacci.html#fibonacci","title":"fibonacci","text":"\\[ F_n = F_{n - 1} + F_{n - 2} \\] \\[ F_1 = 1 \\] \\[ F_2 = 1 \\] \\[ x^2 = x + 1 \\] \\[ x = \\begin{Bmatrix} \\frac{1}{2} + \\frac{\\sqrt{5}}{2} = \\phi \\\\ \\frac{1}{2} - \\frac{\\sqrt{5}}{2} = \\psi \\\\ \\end{Bmatrix} \\] \\[ (-\\frac{1}{x})^2 = \\frac{1}{x^2} \\] \\[ 1 = \\frac{1}{x} + \\frac{1}{x^2} \\] \\[ 1 - \\frac{1}{x} = \\frac{1}{x^2} = (-\\frac{1}{x})^2 \\] \\[ (-\\frac{1}{x})^2 = (-\\frac{1}{x}) + 1 \\] \\[ x = 1 + \\frac{1}{x} \\] \\[ -\\frac{1}{x} ? = ? x = 1 + \\frac{1}{x} \\] \\[ -1 - \\frac{1}{x} = -x ? = ? \\frac{1}{x} \\] \\[ -x^2 ? = ? 1 \\] \\[ x ? = ? i, -i \\] \\[ \\phi \\ne i \\] \\[ \\phi \\ne -i \\] \\[ \\psi \\ne i \\] \\[ \\psi \\ne -i \\] \\[ \\text{Thus...} \\] \\[ \\phi^{-1} = -\\psi \\] \\[ \\psi^{-1} = -\\phi \\] \\[ \\psi^{n - 1} = -\\phi \\psi^n \\] \\[ \\phi^{n - 1} = -\\psi \\phi^n \\] \\[ \\text{And don't forget that!} \\] \\[ \\phi^2 = \\phi + 1 \\] \\[ \\phi^3 = \\phi^2 + \\phi = 2 \\phi + 1 \\] \\[ \\phi^4 = 2 \\phi^2 + \\phi = 3 \\phi + 2 \\] \\[ \\vdots \\] \\[ \\phi^n = c_{n, n} \\phi + c_{n, n - 1} \\] \\[ \\phi^{n + 1} = c_{n + 1, n + 1} \\phi + c_{n + 1, n} = \\phi^n \\phi = c_{n, n} \\phi^2 + c_{n, n - 1} \\phi = c_{n, n} \\phi + c_{n, n} + c_{n, n - 1} \\phi \\] \\[ c_{n + 1, n} = c_{n, n} \\] \\[ c_{n + 2, n} = c_{n + 1, n} = c_{n, n} \\] \\[ \\vdots \\] \\[ c_{n + k, n} = c_{n, n} \\] \\[ C_n = : c_{n, n} \\] \\[ \\phi^n = C_n \\phi + C_{n - 1} \\] \\[ \\phi^{n + 1} = C_{n + 1} \\phi + C_n = \\phi^n \\phi = C_n \\phi^2 + C_{n - 1} \\phi = C_n \\phi + C_n + C_{n - 1} \\phi \\] \\[ C_{n + 1} = C_n + C_{n - 1} \\] \\[ C_n = C_{n - 1} + C_{n - 2} \\] \\[ \\phi^2 = \\phi + 1 = \\phi C_2 + C_1 \\] \\[ C_1 = 1 \\] \\[ C_2 = 1 \\] \\[ C_n = F_n \\] \\[ \\phi^n = F_n \\phi + F_{n - 1} \\] \\[ \\text{Yay, now I can solve for $F_n$ if it weren't for the second fibonacci term, so how can I solve that?} \\] \\[ \\text{Well, this is only because $\\phi^2 = \\phi + 1$, but same goes for $\\psi$, so...} \\] \\[ \\psi^n = F_n \\psi + F_{n - 1} \\] \\[ \\text{And subtracting, we get...} \\] \\[ \\phi^n - \\psi^n = F_n \\phi + F_{n - 1} -  F_n \\psi - F_{n - 1} = (\\phi - \\psi) F_n = (\\frac{1}{2} + \\frac{\\sqrt{5}}{2} - \\frac{1}{2} + \\frac{\\sqrt{5}}{2}) F_n = \\sqrt{5} F_n \\] \\[ F_n = \\frac{\\phi^n - \\psi^n}{\\sqrt{5}} = \\frac{\\phi^n - \\psi^n}{\\phi - \\psi} \\] \\[ F_n \\approx \\frac{\\phi^n}{\\sqrt{5}} \\]"},{"location":"fibbonacci.html#luca-numbers","title":"luca numbers","text":"\\[ L_n = : F_{n + 1} + F_{n - 1} = L_{n - 1} + L_{n - 2} \\] \\[ L_1 = 1 \\] \\[ L_2 = 3 \\] \\[ L_0 = 2 \\text{ And } F_0 = 0 \\text{ btw.} \\] \\[ L_n = \\frac{\\phi^{n + 1} - \\psi^{n + 1}}{\\phi - \\psi} + \\frac{\\phi^{n - 1} - \\psi^{n - 1}}{\\phi - \\psi} = \\frac{\\phi^{n + 1} - \\psi^{n + 1} + \\phi^{n - 1} - \\psi^{n - 1}}{\\phi - \\psi} \\] \\[ \\text{I hope that you remembered!} \\] \\[ L_n = \\frac{\\phi^n \\phi - \\psi^n \\psi - \\phi^n \\psi + \\psi^n \\phi}{\\phi - \\psi} = \\frac{\\phi^n (\\phi - \\psi) + \\psi^n (\\phi - \\psi)}{\\phi - \\psi} \\] \\[ L_n = \\phi^n + \\psi^n \\] \\[ L_n \\approx \\phi^n \\]"},{"location":"fibbonacci.html#fibonacci-numbers","title":"fibonacci numbers?","text":"\\[ L_{n + 1} + L_{n - 1} = \\phi^{n + 1} + \\psi^{n + 1} + \\phi^{n - 1} + \\psi^{n - 1} = \\phi^n \\phi + \\psi^n \\psi - \\phi^n \\psi - \\psi^n \\phi = \\phi^n (\\phi - \\psi) - \\psi^n (\\phi - \\psi) = \\phi^n \\sqrt{5} - \\psi^n \\sqrt{5} = \\frac{\\phi^n - \\psi^n}{\\sqrt{5}} 5 \\] \\[ L_{n + 1} + L_{n - 1} = 5 F_n \\] \\[ F_n = \\frac{L_{n + 1} + L_{n - 1}}{5} \\] \\[ L_n \\phi + L_{n - 1} = (\\phi^n + \\psi^n) \\phi + \\phi^{n - 1} + \\psi^{n - 1} = \\phi^{n + 1} + \\psi^n \\phi + \\phi^{n - 1} - \\psi^n \\phi = \\phi^{n + 1} + \\phi^{n - 1} = \\phi^n (\\phi + \\phi^{-1}) \\] \\[ \\phi^n \\sqrt{5} = L_n \\phi + L_{n - 1} = \\phi^{n + 1} + \\phi^{n - 1} \\] \\[ \\phi^n = \\frac{L_n \\phi + L_{n - 1}}{\\sqrt{5}} = \\frac{\\phi^{n + 1} + \\phi^{n - 1}}{\\sqrt{5}} \\] \\[ \\phi^n = \\phi^{n - 1} + \\phi^{n - 2} \\text{ btw} \\]"},{"location":"fibbonacci.html#everything-up-till-now","title":"everything up 'till now","text":"\\[ F_1 = 1 \\] \\[ F_2 = 1 \\] \\[ F_0 = 0 \\] \\[ \\phi^1 = \\phi \\] \\[ \\phi^2 = \\phi + 1 \\] \\[ \\phi^0 = 1 \\] \\[ L_1 = 1 \\] \\[ L_2 = 3 \\] \\[ L_0 = 2 \\] <p>.</p> \\[ F_n = F_{n - 1} + F_{n - 2} \\] \\[ \\phi^n = \\phi^{n - 1} + \\phi^{n - 2} \\] \\[ L_n = L_{n - 1} + L_{n - 2} \\] <p>.</p> \\[ F_n = \\frac{\\phi^n - \\psi^n}{\\sqrt{5}} \\] \\[ \\phi^n = F_n \\phi + F_{n - 1} = \\frac{L_n \\phi + L_{n - 1}}{\\sqrt{5}} \\] \\[ L_n = \\phi^n + \\psi^n \\] <p>.</p> \\[ F_n = \\frac{L_{n + 1} + L_{n - 1}}{5} \\] \\[ \\phi^n = \\frac{\\phi^{n + 1} + \\phi^{n - 1}}{\\sqrt{5}} \\] \\[ L_n = F_{n + 1} + F_{n - 1} \\] <p>.</p> \\[ F_n \\approx \\frac{\\phi^n}{\\sqrt{5}} \\] \\[ \\phi^n \\approx \\frac{F_{n + 1}}{F_n} \\] \\[ L_n \\approx \\phi^n \\]"},{"location":"fibbonacci.html#one-more-thing","title":"one more thing","text":"\\[ \\text{how would one combine } \\phi^n = F_n \\phi + F_{n - 1} \\text{ and } \\phi^n = \\frac{L_n \\phi + L_{n - 1}}{\\sqrt{5}}? \\] \\[ \\text{for one, rearrange the second one} \\] \\[ \\phi^n = F_n \\phi + F_{n - 1} \\] \\[ \\phi^n \\sqrt{5} = L_n \\phi + L_{n - 1} \\] \\[ F_{n - 1} = F_n - F_{n - 2} \\] \\[ L_{n - 1} = F_n + F_{n - 2} \\] \\[ \\phi^n = F_n \\phi + F_n - F_{n - 2} \\] \\[ \\phi^n \\sqrt{5} = L_n \\phi + F_n + F_{n - 2} \\] \\[ (1 + \\sqrt{5}) \\phi^n = 2 \\phi \\phi^n = (F_n + L_n) \\phi + F_n - F_{n - 2} + F_n + F_{n - 2} = (F_n + L_n) \\phi + 2F_n \\] \\[ \\phi \\phi^n = \\frac{1}{2} (F_n + L_n) \\phi + F_n \\] \\[ \\phi^n = \\frac{1}{2} (F_n + L_n) + F_n \\phi^{-1} = \\frac{1}{2} F_n + \\frac{1}{2} L_n - F_n \\psi = \\frac{1}{2} F_n + \\frac{1}{2} L_n - \\frac{1}{2} F_n + \\frac{\\sqrt{5}}{2} F_n \\] \\[ \\phi^n = \\frac{1}{2} L_n + \\frac{\\sqrt{5}}{2} F_n \\approx \\frac{1}{2} \\phi^n + \\frac{\\sqrt{5}}{2} \\frac{\\phi^n}{\\sqrt{5}} = \\phi^n \\]"},{"location":"fibbonacci.html#fibonacci-2","title":"fibonacci 2","text":"\\[ S_n = ? \\] \\[ S_0 = a \\] \\[ S_1 = b \\] \\[ S_n = S_{n - 1} + S_{n - 2} \\] \\[ F_{-1} = 1 \\text{ Btw.} \\] \\[ S_2 = a + b \\] \\[ S_3 = a + b + b = a + 2b \\] \\[ S_4 = a + 2b + a + b = 2a + 3b \\] \\[ S_5 = 2a + 3b + a + 2b = 3a + 5b \\] <p>Yes, I know that this text is not centered and is in a different font. I'm tired of proving things that the corresponding youtube narrator just says \"I'll leave this as an exercise for the viewer\", for example I used lines \\(58\\) thru \\(88\\) just to prove that the numbers that you were seeing were the fibonacci numbers, I got too lazy to prove this next one, so I hate to say it, but I will leave proving the following statement as an exercise for the viewer:</p> \\[ S_n = F_n b + F_{n - 1} a = \\frac{(\\phi^n - \\psi^n) b + (\\phi^{n - 1} - \\psi^{n - 1}) a}{\\phi - \\psi} = \\frac{\\phi^n b - \\psi^n b - \\psi \\phi^n a + \\phi \\psi^n a}{\\phi - \\psi} \\] \\[ S_n = \\frac{\\phi^n (b - \\psi a) - \\psi^n (b - \\phi a)}{\\phi - \\psi} \\]"},{"location":"fractional_calculas.html","title":"Fractional calculus","text":""},{"location":"fractional_calculas.html#fractextcalculus2","title":"\\(\\frac{\\text{calculus}}{2}\\)","text":"<p>Credit: Morphocular (cool chanel)</p> \\[ \\frac{d^{\\frac{1}{2}}}{dx^{\\frac{1}{2}}} f(x) = ? \\] \\[ f(x) = x^n \\] \\[ \\frac{d}{dx} x^n = n x^{n - 1} \\] \\[ \\frac{d^2}{dx^2} x^n = n (n - 1) x^{n - 2} \\] \\[ \\vdots \\] \\[ \\frac{d^k}{dx^k} x^n = n \\cdot (n - 1) \\cdot (n - 2) \\cdot ... \\cdot (n - k + 1) x^{n - k} = \\frac{n \\cdot (n - 1) \\cdot (n - 2) \\cdot ... \\cdot 1 x^{n - k}}{(n - k) \\cdot (n - k - 1) \\cdot (n - k - 2) \\cdot ... \\cdot 1} \\] \\[ \\frac{d^k}{dx^k} x^n = \\frac{n! x^{n - k}}{(n - k)!} \\] \\[ \\frac{d^p}{dx^p} x^n = \\frac{n! x^{n - p}}{(n - p)!} \\] \\[ \\text{where } (n - p)! = : \\int_{0}^{\\infty} t^{n - p} e^{-t} dt \\]"},{"location":"fractional_calculas.html#text_2-int_0x","title":"\\(\\text{}_2 \\int_{0}^{x}\\)","text":"\\[ \\int f(x) dx = \\int_{0}^{x} f(t) dt \\] \\[ \\int \\int f(x) dx^2 = ? = \\text{}_2 \\int_{0}^{x} f(t) dt^2 \\] \\[ ? = \\int_{0}^{x} \\int_{0}^{y} f(t) dt dy \\] \\[ \\frac{d^2}{dx^2} ? = \\frac{\\partial^2}{\\partial x^2} ? = : f(x) \\] \\[ ! = \\int_{0}^{x} (x - t) f(t) dt = \\int_{0}^{x} g(x, t) dt \\] \\[ g(x, t) = (x - t) f(t) \\] \\[ \\frac{\\partial}{\\partial x} ! = \\frac{\\partial}{\\partial x} \\int_{0}^{x} g(x, t) dt = \\frac{\\int_{0}^{x + dx} g(x + dx, t) dt - \\int_{0}^{x} g(x, t) dt}{dx} = \\frac{\\int_{0}^{x} g(x + dx, t) dt - \\int_{0}^{x} g(x, t) dt + g(x + dx, x + dx) dt}{dx} = \\frac{\\int_{0}^{x} g(x + dx, t) dt - \\int_{0}^{x} g(x, t) dt}{dx} + \\frac{g(x + dx, x + dx) dt}{dx} = \\int_{0}^{x} \\frac{g(x + dx, t) - g(x, t)}{dx} dt + \\frac{(x + dx - x - dx) f(t) dt}{dx} = \\int_{0}^{x} \\frac{\\partial}{\\partial x} g(x, t) dt \\] \\[ \\frac{\\partial}{\\partial x} g(x, t) = \\frac{\\partial}{\\partial x} (x - t) f(t) = f(t) \\] \\[ \\frac{\\partial}{\\partial x} ! = \\int_{0}^{x} f(t) dt \\] \\[ \\frac{\\partial^2}{\\partial x^2} ! = f(x) \\] \\[ ? = \\text{ } !  \\] \\[ \\text{}_2 \\int_{0}^{x} f(t) dt^2 = \\int_{0}^{x} (x - t) f(t) dt \\] \\[ \\text{}_2 \\int_{0}^{x} f(t) dt^2 - \\text{}_2 \\int_{0}^{a} f(t) dt^2 = \\int_{0}^{x} (x - t) f(t) dt - \\int_{0}^{a} (x - t) f(t) dt \\] \\[ \\text{}_2 \\int_{a} f(x) dx^2 = \\int_{a}^{x} (x - t) f(t) dt \\]"},{"location":"fractional_calculas.html#text_3-int_ax","title":"\\(\\text{}_3 \\int_{a}^{x}\\)","text":"\\[ \\text{}_3 \\int_{a}^{x} f(t) dt^3 = \\text{}_2 \\int_{a}^{x} \\int_{a}^{t} f(s) ds dt^2 = \\int_{a}^{x} (x - t) \\int_{a}^{t} f(s) ds dt \\] \\[ (uv) \\prime = u v \\prime + u \\prime v \\] \\[ \\int (uv) \\prime dx = \\int u v \\prime dx + \\int u \\prime v dx \\] \\[ uv = \\int u \\frac{dv}{dx} dx + \\int \\frac{du}{dx} v dx \\] \\[ uv = \\int u dv + \\int du v \\] \\[ \\int u dv = uv - \\int du v \\] \\[ u = \\int_{a}^{t} f(s) ds \\] \\[ dv = (x - t) dt \\] \\[ du = f(t) dt \\] \\[ v = - \\frac{1}{2} (x - t)^2 \\] \\[ \\text{}_3 \\int_{a}^{x} f(t) dt^3 = \\int_{t=a}^{x} u dv = [uv]_ {t=a}^{x} - \\int_{a}^{x} - \\frac{1}{2} (x - t)^2 f(t) dt = [- \\frac{1}{2} (x - t)^2 \\int_{a}^{t} f(s) ds]_ {t=a}^{x} + \\int_{a}^{x} - \\frac{1}{2} (x - t)^2 f(t) dt = (- \\frac{1}{2} (x - a)^2 \\int_{a}^{a} f(s) ds) - (- \\frac{1}{2} (x - x)^2 \\int_{a}^{x} f(s) ds) + \\int_{a}^{x} \\frac{1}{2} (x - t)^2 f(t) dt \\] \\[ \\text{}_3 \\int_{a} f(x) dx^3 = \\frac{1}{2} \\int_{a}^{x} (x - t)^2 f(t) dt \\]"},{"location":"fractional_calculas.html#text_p-int_ax","title":"\\(\\text{}_p \\int_{a}^{x}\\)","text":"\\[ \\vdots \\] \\[  \\text{}_n \\int_{a}^{x} f(t) dt = \\int_{a}^{x} g(x, t, n) f(t) dt \\] \\[  \\text{}_{n + 1} \\int_{a}^{x} f(t) dt^{n + 1} = \\text{}_n \\int_{a}^{x} \\int_{a}^{t} f(s) ds dt^n = \\int_{a}^{x} g(x, t, n) \\int_{a}^{t} f(s) ds dt \\] \\[ u = \\int_{a}^{t} f(s) ds \\] \\[ dv = g(x, t, n) dt \\] \\[ du = f(t) dt \\] \\[ v ? = ? - g(x, t, n + 1) \\] \\[ \\text{}_{n + 1} \\int_{a}^{x} f(t) dt^{n + 1} = \\int_{t=a}^{x} u dv = [uv]_ {t=a}^{x} - \\int_{a}^{x} - g(x, t, n + 1) f(t) dt = [- g(x, t, n + 1) \\int_{a}^{t} f(s) ds]_ {t=a}^{x} + \\int_{a}^{x} g(x, t, n + 1) f(t) dt = (- g(x, a, n + 1) \\int_{a}^{a} f(s) ds) - (- g(x, x, n + 1) \\int_{a}^{x} f(s) ds) + \\int_{a}^{x} g(x, t, n + 1) f(t) dt = \\int_{a}^{x} g(x, t, n + 1) f(t) dt + g(x, x, n + 1) \\int_{a}^{x} f(s) ds = \\int_{a}^{x} g(x, t, n + 1) f(t) dt \\] \\[ g(x, x, n + 1) = 0 \\text{ AND } \\frac{d}{dt} g(x, t, n + 1) = - g(x, t, n) \\Rightarrow v = - g(x, t, n + 1) \\] \\[ \\frac{1}{n!} (x - x)^n = 0 \\] \\[ \\frac{d}{dt} \\frac{1}{n!} (x - t)^n = - \\frac{1}{n} n (x - t)^{n - 1} = - \\frac{1}{(n - 1)!} (x - t)^{n - 1} \\] \\[ g(x, t, 2) = x - t \\] \\[ \\frac{1}{(2 - 1)!} (x - t)^{2 - 1} = x - t \\] \\[ g(x, t, n) = \\frac{1}{(n - 1)!} (x - t)^{n - 1} \\] \\[ \\text{}_n \\int_{a} f(x) dx^n = \\frac{1}{(n - 1)!} \\int_{a}^{x} (x - t)^{n - 1} f(t) dt \\] \\[ \\text{}_n \\int_{a} f(x) dx^n = \\frac{1}{\\Gamma (n)} \\int_{a}^{x} (x - t)^{n - 1} f(t) dt \\] <p>(this is Gamma)</p> \\[ \\text{}_\\frac{1}{2} \\int_{a} f(x) dx^{\\frac{1}{2}} : = \\frac{1}{\\Gamma (\\frac{1}{2})} \\int_{a}^{x} (x - t)^{n - 1} f(t) dt = \\frac{1}{\\sqrt{\\pi}} \\int_{a}^{x} (x - t)^{n - 1} f(t) dt \\]"},{"location":"fractional_calculas.html#differintegral-not-derivigral","title":"differintegral (not derivigral)","text":"\\[ \\frac{d^{\\frac{1}{2}}}{dx^{\\frac{1}{2}}} f(x) = : \\frac{d}{dx} \\frac{1}{\\sqrt{\\pi}} \\int_{a}^{x} (x - t)^{n - 1} f(t) dt \\] \\[ \\frac{d^p}{dx^p} f(x) = : \\begin{Bmatrix} p = 0 : f(x) \\\\ p \\text{ is an integer and greater than } 0 : \\frac{d^p}{dx^p} f(x) \\\\ p \\text{ is an integer and less than } 0 : \\frac{1}{(|p| - 1)!} \\int_{a}^{x} (x - t)^{|p| - 1} f(t) dt \\\\ p &lt; 0 : \\frac{1}{\\Gamma (|p|)} \\int_{a}^{x} (x - t)^{|p| - 1} f(t) dt \\\\ p &gt; 0 : \\frac{d^k}{dx^k} (\\frac{1}{\\Gamma (\\alpha)} \\int_{a}^{x} (x - t)^{\\alpha - 1} f(t) dt)  \\\\ \\text{where } p + \\alpha = \\text{integer } k \\\\ \\end{Bmatrix} \\] <p>and here's a graph!</p> <p>the calculator could not take derivatives of fractional integrals, so I used the limit definition of the derivative and sadly, that is why  you can only go from \\(-1\\) to \\(1\\)</p>"},{"location":"gamma.html","title":"Gamma","text":""},{"location":"gamma.html#n","title":"n!","text":"<p>Credit: BriTheMathGuy</p> \\[ n! = ? \\] \\[ n! = n(n - 1)(n - 2)... 1 \\] \\[ \\frac{d}{dx} x^n = n x^{n - 1} \\] \\[ \\frac{d^n}{dx^n} f(x) = : \\frac{d}{dx} ( \\frac{d^{n - 1}}{dx^{n - 1}} f(x) ) \\] \\[ \\frac{d^1}{dx^1} f(x) = : \\frac{d}{dx} f(x) \\] \\[ \\frac{d^2}{dx^2} x^n = n(n - 1) x^{n - 2} \\] \\[ \\frac{d^a}{dx^a} x^n = \\frac{n! x^{n - a}}{(n - a)!} \\] \\[ \\frac{d^n}{dx^n} x^n = \\frac{n! x^{n - n}}{(n - n)!} = n! \\] \\[ \\frac{d^n}{dx^n} \\frac{1}{x} = \\frac{(-1)^n n!}{x^{n - 1}} \\] \\[ \\frac{d^n}{dx^n} (-\\frac{1}{x}) = \\frac{n!}{(-x)^{n + 1}} \\] \\[ \\text{if} \\frac{n!}{(-x)^{n + 1}} \\text{is a function of x and x can be a constant, then} \\frac{d^n}{dx^n} (-\\frac{1}{x}) \\text{is not. so} \\frac{n!}{(-x)^{n + 1}} \\text{at} x = -1$ \\text{is} n! \\] \\[ \\text{problem solved!... but that is a bit too many derivatives...} \\] \\[ \\text{so an example of something easy to differentiate is } e^x \\] \\[ \\text{this line of text is in memorial of spamming quad quad quad quad quad quad quad quad instead of using the /text feature} \\] \\[ \\frac{d}{dx} e^x = e^x \\] \\[ \\frac{\\partial}{\\partial t} e^{xt} = x e^{xt} \\] \\[ \\frac{d}{dx} (\\int f(x) dx) = : f(x) \\] \\[ \\int_{a}^{b} f(x) dx = : \\int f(b) - \\int f(a) \\] \\[ \\int f(x) = \\int_{a}^{x} f(t) dt \\] \\[ \\int_{0}^{\\infty} f(x) dx = : (\\lim_{x \\to \\infty} f(x)) - f(0) \\] \\[ \\int e^{xt} dt = \\frac{e^{xt}}{x} \\] \\[ \\int_{0}^{\\infty} e^{xt} dt = \\frac{e^{\\infty x}}{x} - e^{0x} = -\\frac{1}{x}, x&lt;0 \\] \\[ \\frac{d^n}{dx^n} \\int_{0}^{\\infty} e^{xt} dt = \\frac{d^n}{dx^n} (-\\frac{1}{x}), x&lt;0 \\] \\[ \\int_{0}^{\\infty} \\frac{\\partial^n}{\\partial x^n} e^{xt} dt = \\frac{n!}{(-x)^{n + 1}}, x&lt;0 \\] \\[ \\int_{0}^{\\infty} t^n e^{xt} dt = \\frac{n!}{(-x)^{n + 1}}, x&lt;0 \\] \\[ \\int_{0}^{\\infty} t^n e^{-t} dt = n! \\] \\[ \\Gamma (x) = : \\int_{0}^{\\infty} t^{x - 1} e^{-t} dt \\] <p>problem solved! but this has no constraint that \\(n\\) is an integer, so I will use \\(x\\) now that it is freed up from the formula, and the factorial has its own definition so the \\(4 \\mu l(a)\\) has been demoted to \\(\\Gamma (x + 1)\\)</p> <p>(four-mu-la)</p>"},{"location":"gamma.html#_1","title":".","text":"<p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>. you found it!</p> <p>quaternions</p>"},{"location":"geometric_algebra.html","title":"Geometric algebra","text":""},{"location":"geometric_algebra.html#why-cant-youyou-cant-multiply-two-vectors","title":"why can't you/you can't multiply two vectors","text":"\\[ \\vec{u} = \\begin{bmatrix} u_x \\\\ u_y \\\\ u_z \\\\ \\end{bmatrix} = u_x \\hat{i} + u_y \\hat{j} + u_z \\hat{k} \\] \\[ \\vec{v} = \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\\\ \\end{bmatrix} = v_x \\hat{i} + v_y \\hat{j} + v_z \\hat{k} \\] \\[ \\vec{u} \\vec{v} = (u_x \\hat{i} + u_y \\hat{j} + u_z \\hat{k}) (v_x \\hat{i} + v_y \\hat{j} + v_z \\hat{k}) = u_x \\hat{i} v_x \\hat{i} + u_x \\hat{i} v_y \\hat{j} + u_x \\hat{i} v_z \\hat{k} + u_y \\hat{j} v_x \\hat{i} + u_y \\hat{j} v_y \\hat{j} + u_y \\hat{j} v_z \\hat{k} + u_z \\hat{k} v_x \\hat{i} + u_z \\hat{k} v_y \\hat{j} + u_z \\hat{k} v_z \\hat{k} \\] \\[ \\text{For lack of a better way to display this, } \\vec{u} \\vec{v} \\text{ equals the sum of the things below:} \\] \\[ (\\hat{i} \\hat{i})(u_x v_x) + (\\hat{i} \\hat{j})(u_x v_y) + (\\hat{i} \\hat{k})(u_x v_z) \\] \\[ (\\hat{j} \\hat{i})(u_y v_x) + (\\hat{j} \\hat{j})(u_y v_y) + (\\hat{j} \\hat{k})(u_y v_z) \\] \\[ (\\hat{k} \\hat{i})(u_z v_x) + (\\hat{k} \\hat{j})(u_z v_y) + (\\hat{k} \\hat{k})(u_z v_z) \\]"},{"location":"geometric_algebra.html#geometric-algebra","title":"geometric algebra","text":"<p>WARNING! This page requires knowing linear algebra, mostly just vectors, and the first part of A Swift Introduction to Geometric Algebra (literally, that was the name) to know what a \\(k\\)-vector and a multivector is (stop at 12:20). (Also, it's where these ideas come from (at least in this chapter and the complex numbers chapter coming soon... And the Maxwell's equation chapter at the end).) Here is the definition for the product of basis vectors: The product of a basis vector \\(e_i\\) and it self is \\(1\\), and the product of two basis vectors \\(e_i\\) and \\(e_j\\) equals \\(-e_j e_i (i \\ne j)\\). This means that you can do this at any point in the product of basis vectors (this should make sense). By the way, \\(U = \\hat{i} \\hat{j} \\hat{k}\\)</p> \\[ \\hat{i} = \\hat{x} \\] \\[ \\hat{j} = \\hat{y} \\] \\[ \\hat{k} = \\hat{z} \\] \\[ \\text{Now is about as good of a time as any to simplify the product.} \\] \\[ \\vec{u} \\vec{v} = \\begin{pmatrix} (\\hat{x} \\hat{x})(u_x v_x) + (\\hat{x} \\hat{y})(u_x v_y) + (\\hat{x} \\hat{z})(u_x v_z) +  \\\\ (\\hat{y} \\hat{x})(u_y v_x) + (\\hat{y} \\hat{y})(u_y v_y) + (\\hat{y} \\hat{z})(u_y v_z) + \\\\ (\\hat{z} \\hat{x})(u_z v_x) + (\\hat{z} \\hat{y})(u_z v_y) + (\\hat{z} \\hat{z})(u_z v_z) \\\\ \\end{pmatrix} = \\begin{pmatrix} (\\hat{x} \\hat{x})(u_x v_x) + (\\hat{x} \\hat{y})(u_x v_y) - (\\hat{z} \\hat{x})(u_x v_z) -  \\\\ (\\hat{x} \\hat{y})(u_y v_x) + (\\hat{y} \\hat{y})(u_y v_y) + (\\hat{y} \\hat{z})(u_y v_z) + \\\\ (\\hat{z} \\hat{x})(u_z v_x) - (\\hat{y} \\hat{z})(u_z v_y) + (\\hat{z} \\hat{z})(u_z v_z) \\\\ \\end{pmatrix} = \\begin{pmatrix} u_x v_x + (\\hat{x} \\hat{y})(u_x v_y) - (\\hat{z} \\hat{x})(u_x v_z) -  \\\\ (\\hat{x} \\hat{y})(u_y v_x) + u_y v_y + (\\hat{y} \\hat{z})(u_y v_z) + \\\\ (\\hat{z} \\hat{x})(u_z v_x) - (\\hat{y} \\hat{z})(u_z v_y) + u_z v_z \\\\ \\end{pmatrix} = u_x v_x +  u_y v_y + u_z v_z + \\begin{pmatrix} (\\hat{x} \\hat{y})(u_x v_y - u_y v_x) +  \\\\ (\\hat{y} \\hat{z})(u_y v_z - u_z v_y) + \\\\ (\\hat{z} \\hat{x})(u_z v_x - u_x v_z) \\\\ \\end{pmatrix} = \\vec{u} \\cdot \\vec{v} + \\begin{pmatrix} (\\hat{x} \\hat{y})(u_x v_y - u_y v_x) +  \\\\ (\\hat{y} \\hat{z})(u_y v_z - u_z v_y) + \\\\ (\\hat{z} \\hat{x})(u_z v_x - u_x v_z) \\\\ \\end{pmatrix} \\] \\[ \\text{But now, I need to turn a bivector into a vector. What about that } U \\text{ thing?} \\] <p>Puzzle time! Prove that \\(U \\vec{v} = \\vec{v} U\\).</p> \\[ U^2 = \\hat{x} \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\hat{z} = -\\hat{x} \\hat{y} \\hat{z} \\hat{x} \\hat{z} \\hat{y} = \\hat{x} \\hat{y} \\hat{z} \\hat{z} \\hat{x} \\hat{y} = \\hat{x} \\hat{y} \\hat{x} \\hat{y} = -\\hat{x} \\hat{x} \\hat{y} \\hat{y} \\] \\[ U^2 = -1 \\] <p>\\(U\\) is usually called \\(i\\) for this reason.</p> \\[ U^3 = \\hat{x} \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\hat{z} = -\\hat{x} \\hat{y} \\hat{x} \\hat{z} \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\hat{z} = \\hat{x} \\hat{x} \\hat{y} \\hat{z} \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\hat{z} = \\hat{y} \\hat{z} \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\hat{z} = -\\hat{y} \\hat{y} \\hat{z} \\hat{z} \\hat{x} \\hat{y} \\hat{z} = -\\hat{x} \\hat{y} \\hat{z} = -U = \\hat{x} \\hat{z} \\hat{y} = -\\hat{z} \\hat{x} \\hat{y} = \\hat{z} \\hat{y} \\hat{x} \\] \\[ U^4 = U^2 U^2 = (-1) (-1) = 1 \\] \\[ U^4 = U^3 U = (-U) U = -U^2 = -(-1) = 1 \\] <p>.</p> \\[ U^1 = U = \\hat{x} \\hat{y} \\hat{z} \\] \\[ U^2 = 1 \\] \\[ U^3 = -U = \\hat{z} \\hat{y} \\hat{x} = -\\hat{x} \\hat{y} \\hat{z} \\] \\[ U^4 = (-U) U  = 1 \\] <p>.</p> \\[ \\hat{x} \\hat{y} = \\hat{x} \\hat{y} \\hat{z} \\hat{z} = U \\hat{z} \\] \\[ \\hat{y} \\hat{z} = \\hat{y} \\hat{z} \\hat{x} \\hat{x} = -\\hat{y} \\hat{x} \\hat{z} \\hat{x} = \\hat{x} \\hat{y} \\hat{z} \\hat{x} = U \\hat{x} \\] \\[ \\hat{z} \\hat{x} = \\hat{z} \\hat{x} \\hat{y} \\hat{y} = -\\hat{z} \\hat{y} \\hat{x} \\hat{y} = -(-U) \\hat{y} = U \\hat{y} \\] \\[ \\text{Yes! Now I can finally solve the puzzle.} \\] \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\begin{pmatrix} (U \\hat{z})(u_x v_y - u_y v_x) +  \\\\ (U \\hat{x})(u_y v_z - u_z v_y) + \\\\ (U \\hat{y})(u_z v_x - u_x v_z) \\\\ \\end{pmatrix} = \\vec{u} \\cdot \\vec{v} + U \\begin{pmatrix} \\hat{x} (u_y v_z - u_z v_y) +  \\\\ \\hat{y} (u_z v_x - u_x v_z) + \\\\ \\hat{z} (u_x v_y - u_y v_x) \\\\ \\end{pmatrix} = \\vec{u} \\cdot \\vec{v} + U \\begin{bmatrix} u_y v_z - u_z v_y \\\\ u_z v_x - u_x v_z \\\\ u_x v_y - u_y v_x \\\\ \\end{bmatrix} \\] <p>.</p> \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + U \\text{ } \\vec{u} \\times \\vec{v} \\] \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\hat{x} \\hat{y} \\hat{z} \\text{ } \\vec{u} \\times \\vec{v} \\] \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\vec{u} \\times \\vec{v} \\text{ } i \\] <p>Also, the cross product only works in \\(3d\\) while this \\(U \\text{ } \\vec{u} \\times \\vec{v}\\) thing works in any dimension. This operator actually has a name (well, two names), the outer product (as opposed to the dot product sometimes referred to as the inner product) or wedge product for its appearance as a wedge unicode character. This more general cross product is written \\(\\vec{u} \u2227 \\vec{v}\\). (While we're on the topic of products, \\(\\vec{u} \\vec{v}\\) is known as the geometric product of \\(\\vec{u}\\) and \\(\\vec{v}\\).) Also, I have to interrupt this for...</p> <p>\\(100\\) lines! But</p> \\[ \\vec{u} \\vec{v} = \\vec{u} \\cdot \\vec{v} + \\vec{u} \u2227 \\vec{v}. \\]"},{"location":"geometric_algebra.html#frac1vecv-and-vecv2","title":"\\(\\frac{1}{\\vec{v}}\\) (and \\(\\vec{v}^2\\))","text":"\\[ \\text{Let's say that we are in dimension } d \\text{. First, basis vectors} \\] \\[ e_1 = \\hat{i} = \\hat{x} \\] \\[ e_2 = \\hat{j} = \\hat{y} \\] \\[ e_3 = \\hat{k} = \\hat{z} \\] \\[ e_4 = \\hat{l} = \\hat{w} \\] \\[ \\vdots \\] \\[ \\vec{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_d\\\\ \\end{bmatrix} = \\sum\\limits_{n = 1}^{d} u_n e_n \\] \\[ \\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_d \\\\ \\end{bmatrix} = \\sum\\limits_{n = 1}^{d} v_n e_n \\] \\[ \\vec{u} \\vec{v} = \\begin{bmatrix} (\\hat{x} \\hat{x}) u_1 v_1 &amp; (\\hat{x} \\hat{y}) u_1 v_2 &amp; (\\hat{x} \\hat{z}) u_1 v_3 &amp; (\\hat{x} \\hat{w}) u_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) u_1 v_d  \\\\ (\\hat{y} \\hat{x}) u_2 v_1 &amp; (\\hat{y} \\hat{y}) u_2 v_2 &amp; (\\hat{y} \\hat{z}) u_2 v_3 &amp; (\\hat{y} \\hat{w}) u_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) u_2 v_d \\\\ (\\hat{z} \\hat{x}) u_3 v_1 &amp; (\\hat{z} \\hat{y}) u_3 v_2 &amp; (\\hat{z} \\hat{z}) u_3 v_3 &amp; (\\hat{z} \\hat{w}) u_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) u_3 v_d \\\\ (\\hat{w} \\hat{x}) u_4 v_1 &amp; (\\hat{w} \\hat{y}) u_4 v_2 &amp; (\\hat{w} \\hat{z}) u_4 v_3 &amp; (\\hat{w} \\hat{w}) u_4 v_4 &amp; \\dots &amp; (\\hat{w} e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (e_d \\hat{x}) u_d v_1 &amp; (e_d \\hat{y}) u_d v_2 &amp; (e_d \\hat{z}) u_d v_3 &amp; (e_d \\hat{w}) u_d v_4 &amp; \\dots &amp; (e_d e_d) u_d v_d \\\\ \\end{bmatrix} = \\begin{bmatrix} u_1 v_1 &amp; (\\hat{x} \\hat{y}) u_1 v_2 &amp; (\\hat{x} \\hat{z}) u_1 v_3 &amp; (\\hat{x} \\hat{w}) u_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) u_1 v_d  \\\\ (\\hat{y} \\hat{x}) u_2 v_1 &amp; u_2 v_2 &amp; (\\hat{y} \\hat{z}) u_2 v_3 &amp; (\\hat{y} \\hat{w}) u_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) u_2 v_d \\\\ (\\hat{z} \\hat{x}) u_3 v_1 &amp; (\\hat{z} \\hat{y}) u_3 v_2 &amp; u_3 v_3 &amp; (\\hat{z} \\hat{w}) u_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) u_3 v_d \\\\ (\\hat{w} \\hat{x}) u_4 v_1 &amp; (\\hat{w} \\hat{y}) u_4 v_2 &amp; (\\hat{w} \\hat{z}) u_4 v_3 &amp; u_4 v_4 &amp; \\dots &amp; (\\hat{w} e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (e_d \\hat{x}) u_d v_1 &amp; (e_d \\hat{y}) u_d v_2 &amp; (e_d \\hat{z}) u_d v_3 &amp; (e_d \\hat{w}) u_d v_4 &amp; \\dots &amp; u_d v_d \\\\ \\end{bmatrix} \\] \\[ \\vec{u} \\vec{v} = (u_1 v_1 + u_2 v_2 + u_3 v_3 + u_4 v_4 + \\dots + u_d v_d) + \\begin{bmatrix}  &amp; (\\hat{x} \\hat{y}) u_1 v_2 &amp; (\\hat{x} \\hat{z}) u_1 v_3 &amp; (\\hat{x} \\hat{w}) u_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) u_1 v_d  \\\\ -(\\hat{x} \\hat{y}) u_2 v_1 &amp;  &amp; (\\hat{y} \\hat{z}) u_2 v_3 &amp; (\\hat{y} \\hat{w}) u_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) u_2 v_d \\\\ -(\\hat{x} \\hat{z}) u_3 v_1 &amp; -(\\hat{y} \\hat{z}) u_3 v_2 &amp;  &amp; (\\hat{z} \\hat{w}) u_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) u_3 v_d \\\\ -(\\hat{x} \\hat{w}) u_4 v_1 &amp; -(\\hat{y} \\hat{w}) u_4 v_2 &amp; -(\\hat{z} \\hat{w}) u_4 v_3 &amp;  &amp; \\dots &amp; (\\hat{w} e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ -(\\hat{x} e_d) u_d v_1 &amp; -(\\hat{y} e_d) u_d v_2 &amp; -(\\hat{z} e_d) u_d v_3 &amp; -(\\hat{w} e_d) u_d v_4 &amp; \\dots &amp; \\\\ \\end{bmatrix} \\] \\[ \\text{It was really frustrating to write a plus in between, so I just gave up. Just assume that the results above and below are summed together (I kept swiching between \"Just assume that the result above is summed together\" and \"Just assume that the results above and below are summed together\") (you can assume that every matrix on this page is a sum). Here's an idea! Square the vector.} \\] \\[ \\vec{v} \\vec{v} = (v_1 v_1 + v_2 v_2 + v_3 v_3 + v_4 v_4 + \\dots + v_d v_d) + \\begin{bmatrix}  &amp; (\\hat{x} \\hat{y}) v_1 v_2 &amp; (\\hat{x} \\hat{z}) v_1 v_3 &amp; (\\hat{x} \\hat{w}) v_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) v_1 v_d  \\\\ -(\\hat{x} \\hat{y}) v_2 v_1 &amp;  &amp; (\\hat{y} \\hat{z}) v_2 v_3 &amp; (\\hat{y} \\hat{w}) v_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) v_2 v_d \\\\ -(\\hat{x} \\hat{z}) v_3 v_1 &amp; -(\\hat{y} \\hat{z}) v_3 v_2 &amp;  &amp; (\\hat{z} \\hat{w}) v_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) v_3 v_d \\\\ -(\\hat{x} \\hat{w}) v_4 v_1 &amp; -(\\hat{y} \\hat{w}) v_4 v_2 &amp; -(\\hat{z} \\hat{w}) v_4 v_3 &amp;  &amp; \\dots &amp; (\\hat{w} e_d) v_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ -(\\hat{x} e_d) v_d v_1 &amp; -(\\hat{y} e_d) v_d v_2 &amp; -(\\hat{z} e_d) v_d v_3 &amp; -(\\hat{w} e_d) v_d v_4 &amp; \\dots &amp;  \\\\ \\end{bmatrix} \\] <p>(insert filler text here)</p> \\[ \\text{By the way, } \\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{w}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}. \\] \\[ \\text{Also by the way, here's the formulas for the dot product and absolute value of two and one vector respectively.} \\] \\[ \\vec{u} \\cdot \\vec{v} = u_1 v_1 + u_2 v_2 + u_3 v_3 + u_4 v_4 + \\dots + u_d v_d = \\sum\\limits_{n = 1}^{d} u_n v_n \\] \\[ || \\vec{v} || = \\sqrt{v_1^2 + v_2^2 + v_3^2 + v_4^2 + \\dots + v_d^2} \\] \\[ || \\vec{v} ||^2 = || v_1^2 + v_2^2 + v_3^2 + v_4^2 + \\dots + v_d^2 || \\] \\[ \\text{But the square of a number is always positive (this is VGA (vanilla geometric algebra), not CGA ( complex geometric algebra (the term CGA actually means conformal geometric algebra)) also, the square of the square root of itself, and } i^2 \\text{ is still } -1 \\text{, I think that I was confusing the square of the square root with the square root of the square), and the sum of positive numbers is positive, and the absolute value of a positive number is positive, so...} \\] \\[ || \\vec{v} ||^2 = v_1^2 + v_2^2 + v_3^2 + v_4^2 + \\dots + v_d^2 \\] \\[ \\vec{v} \\cdot \\vec{v} = v_1^2 + v_2^2 + v_3^2 + v_4^2 + \\dots + v_d^2 \\] \\[ || \\vec{v} ||^2 = \\vec{v} \\cdot \\vec{v} \\] \\[ \\text{But then I realized, this is halfway to the square of a vector.} \\] \\[ \\vec{v}^2 = \\vec{v} \\cdot \\vec{v} + \\vec{v} \u2227 \\vec{v} = || \\vec{v} ||^2 + \\vec{v} \u2227 \\vec{v} \\] \\[ \\text{If the cross product of a vector with itself is the zero vector, and the } 3d \\text{ wedge product is proportional to the cross product, than it would make sense that the wedge product of a vector with itself is zero as well (this argument works in } 3d \\text{ by the way). You might have noticed earlier that we almost proved this.} \\] \\[ \\vec{v}^2 = || \\vec{v} ||^2 + \\begin{bmatrix}  &amp; (\\hat{x} \\hat{y}) v_1 v_2 &amp; (\\hat{x} \\hat{z}) v_1 v_3 &amp; (\\hat{x} \\hat{w}) v_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) v_1 v_d  \\\\ -(\\hat{x} \\hat{y}) v_2 v_1 &amp;  &amp; (\\hat{y} \\hat{z}) v_2 v_3 &amp; (\\hat{y} \\hat{w}) v_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) v_2 v_d \\\\ -(\\hat{x} \\hat{z}) v_3 v_1 &amp; -(\\hat{y} \\hat{z}) v_3 v_2 &amp;  &amp; (\\hat{z} \\hat{w}) v_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) v_3 v_d \\\\ -(\\hat{x} \\hat{w}) v_4 v_1 &amp; -(\\hat{y} \\hat{w}) v_4 v_2 &amp; -(\\hat{z} \\hat{w}) v_4 v_3 &amp;  &amp; \\dots &amp; (\\hat{w} e_d) v_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ -(\\hat{x} e_d) v_d v_1 &amp; -(\\hat{y} e_d) v_d v_2 &amp; -(\\hat{z} e_d) v_d v_3 &amp; -(\\hat{w} e_d) v_d v_4 &amp; \\dots &amp;  \\\\ \\end{bmatrix} = || \\vec{v} ||^2 + \\begin{bmatrix}  &amp; (\\hat{x} \\hat{y}) v_1 v_2 &amp; (\\hat{x} \\hat{z}) v_1 v_3 &amp; (\\hat{x} \\hat{w}) v_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) v_1 v_d  \\\\ -(\\hat{x} \\hat{y}) v_1 v_2 &amp;  &amp; (\\hat{y} \\hat{z}) v_2 v_3 &amp; (\\hat{y} \\hat{w}) v_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) v_2 v_d \\\\ -(\\hat{x} \\hat{z}) v_1 v_3 &amp; -(\\hat{y} \\hat{z}) v_2 v_3 &amp;  &amp; (\\hat{z} \\hat{w}) v_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) v_3 v_d \\\\ -(\\hat{x} \\hat{w}) v_1 v_4 &amp; -(\\hat{y} \\hat{w}) v_2 v_4 &amp; -(\\hat{z} \\hat{w}) v_3 v_4 &amp;  &amp; \\dots &amp; (\\hat{w} e_d) v_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ -(\\hat{x} e_d) v_1 v_d &amp; -(\\hat{y} e_d) v_2 v_d &amp; -(\\hat{z} e_d) v_3 v_d &amp; -(\\hat{w} e_d) v_4 v_d &amp; \\dots &amp;  \\\\ \\end{bmatrix} \\] \\[ \\text{As you can see, each term is perfectly canceled out by the term across the diagonal. And perfect timing, it will be on } 200 \\text{ lines.} \\] \\[ \\vec{v}^2 = || \\vec{v} ||^2 \\] \\[ \\text{Also, now I can solve for } \\frac{1}{\\vec{v}} \\] \\[ \\vec{v} \\vec{v} = || \\vec{v} ||^2 \\] \\[ \\vec{v} = \\frac{|| \\vec{v} ||^2}{\\vec{v}} \\] \\[ \\frac{1}{\\vec{v}} = \\frac{\\vec{v}}{|| \\vec{v} ||^2} \\] \\[ \\text{I had this proof that } \\frac{1}{\\frac{1}{\\vec{v}}} = \\vec{v} \\text{ that I would put right here, but I accidentally deleted that one (and it is now an exercise for the viewer). In conclusion:} \\] \\[ \\vec{v}^2 = || \\vec{v} ||^2 \\] \\[ \\text{And} \\] \\[ \\frac{1}{\\vec{v}} = \\frac{\\vec{v}}{|| \\vec{v} ||^2}. \\] \\[ \\text{By the way, this means that the inverse of a vector is a vector, } \\frac{\\vec{u}}{\\vec{v}} = \\frac{\\vec{u} \\vec{v}}{|| \\vec{v} ||^2} \\text{. The inverse of a } 2d \\text{ is it's circle inverse, the inverse of a } 3d \\text{ vector is it's sphere inverse, the inverse of a } 4d \\text{ vector is it's hypersphere inverse, and so on.} \\]"},{"location":"geometric_algebra.html#fun-fact","title":"fun fact!","text":"<p>As you know, \\(\\vec{u} \\cdot \\vec{v} = u_1 v_1 + u_2 v_2 + u_3 v_3 + u_4 v_4 + \\dots + u_d v_d\\). But what about \\(\\vec{v} \\cdot \\vec{u}\\)? And what about the wedge product? (By the way, this is about the vector product or sometimes called the geometric product)</p> \\[ \\vec{v} \\cdot \\vec{u} = v_1 u_1 + v_2 u_2 + v_3 u_3 + v_4 u_4 + \\dots + v_d u_d = u_1 v_1 + u_2 v_2 + u_3 v_3 + u_4 v_4 + \\dots + u_d v_d \\] \\[ \\vec{v} \\cdot \\vec{u} = \\vec{u} \\cdot \\vec{v} \\] \\[ \\vec{v} \\vec{u} = \\vec{u} \\cdot \\vec{v} + \\vec{v} \u2227 \\vec{u} \\] \\[ \\vec{u} \u2227 \\vec{v} = \\begin{bmatrix}  &amp; (\\hat{x} \\hat{y}) u_1 v_2 &amp; (\\hat{x} \\hat{z}) u_1 v_3 &amp; (\\hat{x} \\hat{w}) u_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) u_1 v_d  \\\\ -(\\hat{x} \\hat{y}) u_2 v_1 &amp;  &amp; (\\hat{y} \\hat{z}) u_2 v_3 &amp; (\\hat{y} \\hat{w}) u_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) u_2 v_d \\\\ -(\\hat{x} \\hat{z}) u_3 v_1 &amp; -(\\hat{y} \\hat{z}) u_3 v_2 &amp;  &amp; (\\hat{z} \\hat{w}) u_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) u_3 v_d \\\\ -(\\hat{x} \\hat{w}) u_4 v_1 &amp; -(\\hat{y} \\hat{w}) u_4 v_2 &amp; -(\\hat{z} \\hat{w}) u_4 v_3 &amp;  &amp; \\dots &amp; (\\hat{w} e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ -(\\hat{x} e_d) u_d v_1 &amp; -(\\hat{y} e_d) u_d v_2 &amp; -(\\hat{z} e_d) u_d v_3 &amp; -(\\hat{w} e_d) u_d v_4 &amp; \\dots &amp;  \\\\ \\end{bmatrix} \\] \\[ \\vec{v} \u2227 \\vec{u} = \\begin{bmatrix}  &amp; (\\hat{x} \\hat{y}) v_1 u_2 &amp; (\\hat{x} \\hat{z}) v_1 u_3 &amp; (\\hat{x} \\hat{w}) v_1 u_4 &amp; \\dots &amp; (\\hat{x} e_d) v_1 u_d  \\\\ -(\\hat{x} \\hat{y}) v_2 u_1 &amp;  &amp; (\\hat{y} \\hat{z}) v_2 u_3 &amp; (\\hat{y} \\hat{w}) v_2 u_4 &amp; \\dots &amp; (\\hat{y} e_d) v_2 u_d \\\\ -(\\hat{x} \\hat{z}) v_3 u_1 &amp; -(\\hat{y} \\hat{z}) v_3 u_2 &amp;  &amp; (\\hat{z} \\hat{w}) v_3 u_4 &amp; \\dots &amp; (\\hat{z} e_d) v_3 u_d \\\\ -(\\hat{x} \\hat{w}) v_4 u_1 &amp; -(\\hat{y} \\hat{w}) v_4 u_2 &amp; -(\\hat{z} \\hat{w}) v_4 u_3 &amp;  &amp; \\dots &amp; (\\hat{w} e_d) v_4 u_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ -(\\hat{x} e_d) v_d u_1 &amp; -(\\hat{y} e_d) v_d u_2 &amp; -(\\hat{z} e_d) v_d u_3 &amp; -(\\hat{w} e_d) v_d u_4 &amp; \\dots &amp;  \\\\ \\end{bmatrix} = -\\begin{bmatrix}  &amp; -(\\hat{x} \\hat{y}) v_1 u_2 &amp; -(\\hat{x} \\hat{z}) v_1 u_3 &amp; -(\\hat{x} \\hat{w}) v_1 u_4 &amp; \\dots &amp; -(\\hat{x} e_d) v_1 u_d  \\\\ (\\hat{x} \\hat{y}) v_2 u_1 &amp;  &amp; -(\\hat{y} \\hat{z}) v_2 u_3 &amp; -(\\hat{y} \\hat{w}) v_2 u_4 &amp; \\dots &amp; -(\\hat{y} e_d) v_2 u_d \\\\ (\\hat{x} \\hat{z}) v_3 u_1 &amp; (\\hat{y} \\hat{z}) v_3 u_2 &amp;  &amp; -(\\hat{z} \\hat{w}) v_3 u_4 &amp; \\dots &amp; -(\\hat{z} e_d) v_3 u_d \\\\ (\\hat{x} \\hat{w}) v_4 u_1 &amp; (\\hat{y} \\hat{w}) v_4 u_2 &amp; (\\hat{z} \\hat{w}) v_4 u_3 &amp;  &amp; \\dots &amp; -(\\hat{w} e_d) v_4 u_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ (\\hat{x} e_d) v_d u_1 &amp; (\\hat{y} e_d) v_d u_2 &amp; (\\hat{z} e_d) v_d u_3 &amp; (\\hat{w} e_d) v_d u_4 &amp; \\dots &amp;  \\\\ \\end{bmatrix} = -\\begin{bmatrix}  &amp; (\\hat{x} \\hat{y}) u_1 v_2 &amp; (\\hat{x} \\hat{z}) u_1 v_3 &amp; (\\hat{x} \\hat{w}) u_1 v_4 &amp; \\dots &amp; (\\hat{x} e_d) u_1 v_d  \\\\ -(\\hat{x} \\hat{y}) u_2 v_1 &amp;  &amp; (\\hat{y} \\hat{z}) u_2 v_3 &amp; (\\hat{y} \\hat{w}) u_2 v_4 &amp; \\dots &amp; (\\hat{y} e_d) u_2 v_d \\\\ -(\\hat{x} \\hat{z}) u_3 v_1 &amp; -(\\hat{y} \\hat{z}) u_3 v_2 &amp;  &amp; (\\hat{z} \\hat{z}) u_3 v_4 &amp; \\dots &amp; (\\hat{z} e_d) u_3 v_d \\\\ -(\\hat{x} w) u_4 v_1 &amp; -(\\hat{y} w) u_4 v_2 &amp; -(\\hat{z} w) u_4 v_3 &amp;  &amp; \\dots &amp; (w e_d) u_4 v_d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;  &amp; \\vdots \\\\ -(\\hat{x} e_d) u_d v_1 &amp; -(\\hat{y} e_d) u_d v_2 &amp; -(\\hat{z} e_d) u_d v_3 &amp; -(w e_d) u_d v_4 &amp; \\dots &amp;  \\\\ \\end{bmatrix} \\] \\[ \\vec{v} \u2227 \\vec{u} = - \\vec{u} \u2227 \\vec{v} \\] <p>Also, halfway through (well, more like mostly through), if I added one more \\(\\hat{w}\\), it would be \"unable to render expression\". By the way, \\(2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2\\) lines.</p> \\[ \\vec{v} \\vec{u} = \\vec{u} \\cdot \\vec{v} - \\vec{u} \u2227 \\vec{v} \\]"},{"location":"geometric_algebra.html#complex-numbers","title":"complex numbers","text":"\\[ \\text{complex numbers (surprisingly in geometric algebra) come from } 2d \\text{ geometric algebra. So time to re-derive that! (And use } i \\text{ this time instead of } U \\text{.) (I thought that } i \\text{ should have a hat, but that wouldn't work.) Here's the definitions and link to the complex numbers page:} \\] <p>.</p> \\[ \\hat{i} = \\hat{x} \\text{ (This is so that } \\hat{i} \\text{ isn't confused with } i \\text{).} \\] \\[ \\hat{j} = \\hat{y} \\] \\[ \\hat{x} \\hat{y} = i \\] \\[ \\hat{x}^2 = 1 \\] \\[ \\hat{y}^2 = 1 \\] \\[ \\hat{x} \\hat{y} = -\\hat{y} \\hat{x} \\text{(} = i \\text{)} \\] \\[ \\text{And with those, you can derive} \\] \\[ i^2 = -1 \\] \\[ \\text{this is why it was called } i. \\] <p>.</p> \\[ \\text{Also, a bivector in } 2 \\text{ dimensions has one degree of freedom (just like how a vector behaves in } 1d \\text{) (by the way, a } k \\text{-vector in } n \\text{ dimensions has } \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} \\text{ DoF's (degrees of freedom)), so I'll call it, I dunno, a pseudoscalar. (} i \\text{ Is always the unit pseudoscalar no matter the dimension.)} \\] \\[ \\vec{u} = \\begin{bmatrix} u_x \\\\ u_y \\\\ \\end{bmatrix} = u_x \\hat{x} + u_y \\hat{y} \\] \\[ \\vec{v} = \\begin{bmatrix} v_x \\\\ v_y \\\\ \\end{bmatrix} = v_x \\hat{x} + v_y \\hat{y} \\] \\[ \\vec{u} \\vec{v} = \\begin{pmatrix} (\\hat{x} \\hat{x}) u_x v_x &amp; (\\hat{x} \\hat{y}) u_x v_y \\\\ (\\hat{y} \\hat{x}) u_y v_x &amp; (\\hat{y} \\hat{y}) u_y v_y \\\\ \\end{pmatrix} = (u_x v_x + u_y v_y) + (u_x v_y - u_y v_x)i \\] <p>\\(300\\) Lines!</p> \\[ \\vec{u} \u2227 \\vec{v} = i \\cdot Det \\begin{bmatrix} u_x &amp; u_y \\\\ v_x &amp; v_y \\\\ \\end{bmatrix} \\] <p>Here's a determinant video and linear algebra playlist for context (make sure to skip the cross product video) (also, that determinant is probably the only matrix on this page. I'll come back here when I finish the page and put the answer here: ). ( \\(\\text{ }\\) : ) \\(\\text{ }\\) Btw.)</p> \\[  \\text{Text over time! (As opposed to voice over, it's a font switch.) What's something that viewers of this website would want from geometric algebra? I think I have an idea: rotate a vector by the angle between two other vectors. I'll do it in } 20 \\text{ minutes.} \\] \\[ \\text{It's the next day.} \\] \\[ \\vec{v} i = (v_x \\hat{x} + v_y \\hat{y})i = v_x \\hat{x} i + v_y \\hat{y} i = v_x \\hat{x} \\hat{x} \\hat{y} + v_y \\hat{y} \\hat{x} \\hat{y} = v_x \\hat{y} - v_y \\hat{x} \\hat{y} \\hat{y} = v_x \\hat{y} - v_y \\hat{x} \\] <pre><code>def complexify(v_x, v_y):\n  return v_x + v_y * j\n</code></pre> \\[ \\text{I just realized that this isn't a code repo(sitory). It was my instinct to write it in code form.} \\] \\[ \\text{complexify} (v_x \\hat{x} + v_y \\hat{y}) = v_x + v_y i \\] <p>(insert filler text here)</p> \\[ \\text{complexify} (\\vec{v} i) = i \\text{ complexify} (\\vec{v}) \\] \\[ i \\vec{v} = i(v_x \\hat{x} + v_y \\hat{y}) = i v_x x + i v_y \\hat{y} \\] \\[ \\text{But } v_x \\text{ and } v_y \\text{ are scalars, so} \\] \\[ i \\vec{v} = v_x i \\hat{x} + v_y i \\hat{y} = v_x \\hat{x} \\hat{y} \\hat{x} + v_y \\hat{x} \\hat{y} \\hat{y} = -v_x \\hat{y} \\hat{x} \\hat{x} + v_y \\hat{x} = -v_x \\hat{y} + v_y \\hat{x} \\] \\[ \\text{complexify} (i \\vec{v}) = -i \\text{ complexify} (\\vec{v}). \\] \\[ \\text{Next: vector times a complex number.} \\] \\[ \\text{vectorize} (a + bi) = a \\hat{x} + b \\hat{y} \\] \\[ \\text{complexify} (\\text{vectorize} (z)) = z \\] \\[ \\text{vectorize} (\\text{complexify} (\\vec{v})) = \\vec{v} \\] \\[ (a \\hat{x} + b \\hat{y})(c + di) = a \\hat{x} c + a \\hat{x} di + b \\hat{y} c + b \\hat{y} di = \\begin{pmatrix} ac (\\hat{x}) &amp; ad (\\hat{x} i) \\\\ bc (\\hat{y}) &amp; bd (\\hat{y} i) \\\\ \\end{pmatrix} = \\begin{pmatrix} ac (\\hat{x}) &amp; ad (\\hat{x} \\hat{x} \\hat{y}) \\\\ bc (\\hat{y}) &amp; bd (\\hat{y} \\hat{x} \\hat{y}) \\\\ \\end{pmatrix} = \\begin{pmatrix} ac (\\hat{x}) &amp; ad (\\hat{y}) \\\\ bc (\\hat{y}) &amp; -bd (\\hat{x} \\hat{y} \\hat{y}) \\\\ \\end{pmatrix} = \\begin{pmatrix} ac (\\hat{x}) &amp; ad (\\hat{y}) \\\\ bc (\\hat{y}) &amp; -bd (\\hat{x}) \\\\ \\end{pmatrix} = (ac - bd) \\hat{x} + (ad + bc) \\hat{y} \\] \\[ z = a + bi \\] \\[ w = c + di \\] \\[ \\text{vectorize} (z) w = \\text{vectorize} (zw) \\] \\[ \\vec{u} \\text{ complexify} (\\vec{v}) = \\text{vectorize} (\\text{complexify} (\\vec{u}) \\text{ complexify} (v)) \\] \\[ \\vec{v} z = \\text{vectorize} (\\text{complexify} (\\vec{v}) z) \\] <p>Wait a minute! (Which was the catchphrase of sudgy (pronounced soo-gey) in the quick introduction that I mentioned earlier.) If multiplying a vector by a complex number acts like complex number multiplication (rotating and scaling) (exempt it returns a vector, not a complex number), and a vector times a vector is a complex number, then what rotation is that? (Well, the rotation and scaling of a complex number scales by the magnitude and rotates by the angle counterclockwise from the positive \\(x\\) axis.)</p> \\[ \\text{By the way, } \\vec{v} \\vec{u} = \\text{ccong} (\\vec{u} \\vec{v}) \\] \\[ |a + bi| = \\sqrt{a^2 + b^2} \\] \\[ a + bi = \\vec{u} \\vec{v} \\] \\[ a = \\vec{u} \\cdot \\vec{v} = u_x v_x + u_y v_y \\] \\[ bi = \\vec{u} \u2227 \\vec{v} = (u_x v_y - u_y v_x)i \\text{ (This also means that } b = u_x v_y - u_y v_x = \\vec{u} \\times \\vec{v} \\text{).} \\] \\[ || \\vec{u} \\vec{v} || = \\sqrt{(\\vec{u} \\cdot \\vec{v})^2 + (\\vec{u} \\times \\vec{v})^2} = \\sqrt{(u_x v_x + u_y v_y)^2 + (u_x v_y - u_y v_x)^2} = \\sqrt{u_x v_x u_x v_x + u_x v_x u_y v_y + u_y v_y u_x v_x + u_y v_y u_y v_y + u_x v_y u_x v_y - u_x v_y u_y v_x - u_y v_x u_x v_y + u_y v_x u_y v_x} = \\sqrt{u_x^2 v_x^2 + u_x^2 v_y^2 + u_y^2 v_x^2 + u_y^2 v_y^2} = \\sqrt{(u_x^2 + u_y^2)(v_x^2 + v_y^2)} = \\sqrt{u_x^2 + u_y^2} \\sqrt{v_x^2 + v_y^2} \\] \\[ || \\vec{u} || = \\sqrt{u_x^2 + u_y^2} \\] \\[ || \\vec{v} || = \\sqrt{v_x^2 + v_y^2} \\] \\[ || \\vec{u} \\vec{v} || = || \\vec{u} || \\text{ } || \\vec{v} || \\] \\[ \\text{Next: the part that I have no idea how to prove!} \\] \\[ \\vec{u} \\vec{v} = || \\vec{u} || \\text{ } || \\vec{v} || e^{i \\theta} \\] \\[ \\theta = \\text{ The angle counterclockwise from } \\vec{u} \\text{ towards } \\vec{v}. \\] \\[ \\text{By the way, with this we can derive two new equations! (But you probably know the first.)} \\] \\[ \\vec{u} \\cdot \\vec{v} = || \\vec{u} || \\text{ } || \\vec{v} ||  cos(\\theta) \\] \\[ \\vec{u} \u2227 \\vec{v} = || \\vec{u} || \\text{ } || \\vec{v} ||sin(\\theta) i \\] <p>But what I wanted was that \\(\\theta!\\) But I know that it is (almost) impossible to find \\(\\theta\\) when all you know is \\(e^{i \\theta}\\). Wait a minute! If a complex number of the form \\(e^{i \\theta}\\) is just a rotation, and if multiplying a vector by a complex number acts like they were both complex numbers (exempt it returns a vector), than you can rotate a vector \\(\\vec{w}\\) by the angle between two unit vectors (or just ones with inverse magnitude, but normalizing them is easier) without having to even know about complex numbers!</p> \\[ \\hat{u} = \\frac{\\vec{u}}{|| \\vec{u} ||} \\] \\[ \\hat{v} = \\frac{\\vec{v}}{|| \\vec{v} ||} \\] \\[ \\hat{u} \\hat{v} = e^{i \\theta} \\] \\[ \\text{So, the rotation is} \\] \\[ \\vec{w} \\hat{u} \\hat{v}. \\] <p>(Mic drop.) Also, that was on \\(400\\) lines exactly.</p> \\[ \\text{One more thing before maxwell's equation and rotors.} \\] \\[ (c + di)(a \\hat{x} + b \\hat{y}) = ca \\hat{x} + cb \\hat{y} + dia \\hat{x} + dib \\hat{y} = \\begin{pmatrix} ca (\\hat{x}) &amp; cb (\\hat{y}) \\\\ da (i \\hat{x}) &amp; db (i \\hat{y}) \\\\ \\end{pmatrix} = \\begin{pmatrix} ca (\\hat{x}) &amp; cb (\\hat{y}) \\\\ da (\\hat{x} \\hat{y} \\hat{x}) &amp; db (\\hat{x} \\hat{y} \\hat{y}) \\\\ \\end{pmatrix} = \\begin{pmatrix} ca (\\hat{x}) &amp; cb (\\hat{y}) \\\\ -da (\\hat{y} \\hat{x} \\hat{x}) &amp; db (\\hat{x}) \\\\ \\end{pmatrix} = \\begin{pmatrix} ca (\\hat{x}) &amp; cb (\\hat{y}) \\\\ -da (\\hat{y}) &amp; db (\\hat{x}) \\\\ \\end{pmatrix} = (ca + db) \\hat{x} + (cb - da) \\hat{y} = (a \\hat{x} + b \\hat{y})(c - di) \\] \\[ z \\vec{v} = \\vec{v} \\text{ ccong} (z) \\] \\[ \\text{But if you remember, } \\vec{v} \\vec{u} = \\text{ccong} (\\vec{u} \\vec{v}) \\text{ and } \\vec{u} \\vec{v} \\text{ is a complex number, so...} \\] \\[ \\vec{u} \\vec{v} \\vec{w} = \\vec{w} \\text{ ccong} (\\vec{u} \\vec{v}) = \\vec{w} \\vec{v} \\vec{u} \\] \\[ \\text{One more thing.} \\] \\[ \\theta (\\vec{u}, \\vec{v}) = \\text{ The angle counterclockwise from } \\vec{u} \\text{ towards } \\vec{v}. \\] \\[ \\vec{w} \\vec{u} \\vec{v} = \\vec{w} \\text{ Scaled by the length of } \\vec{u} \\text{ and } \\vec{v} \\text{, and rotated by } \\theta (\\vec{u}, \\vec{v}).  \\]"},{"location":"geometric_algebra.html#rotors","title":"Rotors","text":"<p>A rotor is a way to rotate a vector by any angle \\(\\theta\\) in any plane (Yes, plane. I think it makes the same if not more sense to rotate in a plane. I also think that this is similar to the \u2227 v.s. \\(\\times\\) Product.) in any dimension. Let's start with the simplest possible rotor I can think of, in \\(3d\\), \\(90\\)\u00b0, \\(\\hat{x}\\) - \\(\\hat{y}\\) plane. (I'll call \\(\\vec{v}\\) rotated by the name of \\(\\vec{v} \\prime\\))</p> \\[ \\vec{v} = v_x \\hat{x} + v_y \\hat{y} + v_z \\hat{z} \\] \\[ \\vec{v} \\prime = v_x \\hat{y} - v_y \\hat{x} + v_z \\hat{z} \\] \\[ \\vec{v} \\prime = \\vec{v} \\hat{x} \\hat{y} ? \\] \\[ \\vec{v} \\hat{x} \\hat{y} = (v_x \\hat{x} + v_y \\hat{y} + v_z \\hat{z}) \\hat{x} \\hat{y} = v_x \\hat{x} \\hat{x} \\hat{y} + v_y \\hat{y} \\hat{x} \\hat{y} + v_z \\hat{z} \\hat{x} \\hat{y} = v_x \\hat{y} - v_y \\hat{x} \\hat{y} \\hat{y} - v_z \\hat{x} \\hat{z} \\hat{y} = (v_x) \\hat{y} - (v_y) \\hat{x} + (v_z) \\hat{x} \\hat{y} \\hat{z} \\] \\[ \\text{What was that thing he said at this point? Oh right, it was: Well, it almost worked, the } x \\text{ and the } y \\text{ coordinates got rotated correctly, but the } z \\text{ coordinate got turned into this trivector, let's try something else.} \\] \\[ \\text{Do you remember that time when I proved that } z \\vec{v} = \\vec{v} \\text{ ccong} (z) \\text{, that means that I can now prove this: } \\vec{v} z = \\text{ccong} (z) \\vec{v} \\text{. I'll assume that that means } \\hat{y} \\hat{x} \\text{, I'll try that!} \\] \\[ \\hat{y} \\hat{x} \\vec{v} = \\hat{y} \\hat{x} (v_x \\hat{x} + v_y \\hat{y} + v_z \\hat{z}) = \\hat{y} \\hat{x} v_x \\hat{x} + \\hat{y} \\hat{x} v_y \\hat{y} + \\hat{y} \\hat{x} v_z \\hat{z} = v_x \\hat{y} \\hat{x} \\hat{x} + v_y \\hat{y} \\hat{x} \\hat{y} + v_z \\hat{y} \\hat{x} \\hat{z} = v_x \\hat{y} - v_y \\hat{x} \\hat{y} \\hat{y} - v_z xyz = (v_x) y - (v_y) x - (v_z) \\hat{x} \\hat{y} \\hat{z} \\] \\[ \\text{Once again, the } x \\text{ and the } y \\text{ coordinates got rotated correctly, but the } z \\text{ coordinate got turned into the inverse trivector this time, maybe doing both at once would cancel out?} \\] \\[ \\hat{y} \\hat{x} \\vec{v} \\hat{x} \\hat{y} = \\hat{y} \\hat{x} (v_x \\hat{x} + v_y \\hat{y} + v_z \\hat{z}) \\hat{x} \\hat{y} = \\hat{y} \\hat{x} v_x \\hat{x} \\hat{x}  \\hat{y} + \\hat{y} \\hat{x} v_y \\hat{y} \\hat{x} \\hat{y} + \\hat{y} \\hat{x} v_z \\hat{z} \\hat{x} \\hat{y} = v_x \\hat{y} \\hat{x} \\hat{y} + v_y \\hat{y} \\hat{x} \\hat{y} \\hat{x} \\hat{y} + \\hat{y} \\hat{x} v_z \\hat{z} \\hat{x} \\hat{y} = - v_x \\hat{x} \\hat{y} \\hat{y} - v_y \\hat{y} \\hat{y} \\hat{x} \\hat{x} \\hat{y} + \\hat{y} \\hat{x} v_z \\hat{z} \\hat{x} \\hat{y} = - v_x \\hat{x} - v_y \\hat{y} + \\hat{y} \\hat{x} v_z \\hat{z} \\hat{x} \\hat{y} \\] \\[ \\hat{y} \\hat{x} v_z \\hat{z} \\hat{x} \\hat{y} = v_z \\hat{y} \\hat{x} \\hat{z} \\hat{x} \\hat{y} = -v_z \\hat{y} \\hat{x} \\hat{x} \\hat{z} \\hat{y} = -v_z \\hat{y} \\hat{z} \\hat{y} = v_z \\hat{y} \\hat{y} \\hat{z} \\] \\[ \\hat{y} \\hat{x} v_z \\hat{z} \\hat{x} \\hat{y} = v_z \\hat{z} \\] \\[ \\hat{y} \\hat{x} \\vec{v} \\hat{x} \\hat{y} = - v_x \\hat{x} - v_y \\hat{y} + v_z \\hat{z} \\] \\[ \\text{ The } z \\text{ coordinate is correct! Success! But the } x \\text{ and } y \\text{ got messed up, of course they did, we rotated twice. (Once for } \\hat{y} \\hat{x} \\text{ on the left, and once for } \\hat{x} \\hat{y} \\text{ on the right.) Let's add back in that angle } \\theta \\text{ to prove that we did rotate twice.} \\] <p>Do you remember euler's identity, \\(e^{i \\theta} = cos(\\theta) + isin(\\theta)\\) right? Well, this actually for any \\(i\\) whose square is \\(-1\\), the thing is that \\((\\hat{x} \\hat{y})^2 = \\hat{x} \\hat{y} \\hat{x} \\hat{y} = -\\hat{x} \\hat{x} \\hat{y} \\hat{y} = -1\\), so</p> \\[ e^{\\hat{x} \\hat{y} \\theta} = cos(\\theta) + \\hat{x} \\hat{y} \\text{ } sin(\\theta). \\] \\[ \\text{And just to make sure, let's test for } \\theta = 90\u00b0 \\] \\[ cos(90\u00b0) + \\hat{x} \\hat{y} \\text{ } sin(90\u00b0) = 0 + \\hat{x} \\hat{y} \\text{ } 1 = \\hat{x} \\hat{y}. \\] \\[ \\text{Also, } cos(\\theta) = c \\text{ and } sin(\\theta) = s \\] \\[ \\text{So, replace } \\hat{x} \\hat{y} \\text{ and } \\hat{y} \\hat{x} \\text{ with } c + \\hat{x} \\hat{y} \\text{ } s \\text{ and } cos(-\\theta) + \\hat{x} \\hat{y} \\text{ } sin(-\\theta) \\text{ respectively.} \\] \\[ \\text{(The second one can simplify to } c - \\hat{x} \\hat{y} \\text{ } s \\text{).} \\] \\[ \\vec{v} \\prime = (v_x c - v_y s) \\hat{x} + (v_y c + v_x s) \\hat{y} + v_z \\hat{z} \\] \\[ (c - \\hat{x} \\hat{y} \\text{ } s) \\text{ } \\vec{v} \\text{ } (c + \\hat{x} \\hat{y} \\text{ } s) = (c - \\hat{x} \\hat{y} \\text{ } s) (v_x \\hat{x} + v_y \\hat{y} + v_z \\hat{z}) (c + \\hat{x} \\hat{y} \\text{ } s) \\] \\[ (c - \\hat{x} \\hat{y} \\text{ } s) \\text{ } \\vec{v} \\text{ } (c + \\hat{x} \\hat{y} \\text{ } s) = c v_x \\hat{x} \\text{ } c + c v_x \\hat{x} \\hat{x} \\hat{y} \\text{ } s - \\hat{x} \\hat{y} \\text{ } s v_x \\hat{x} \\text{ } c - \\hat{x} \\hat{y} \\text{ } s v_x \\hat{x} \\hat{x} \\hat{y} \\text{ } s + c v_y \\hat{y} \\text{ } c + c v_y \\hat{y} \\hat{x} \\hat{y} \\text{ } s - \\hat{x} \\hat{y} \\text{ } s v_y \\hat{y} \\text{ } c - \\hat{x} \\hat{y} \\text{ } s v_y \\hat{y} \\hat{x} \\hat{y} \\text{ } s + c v_z \\hat{z} \\text{ } c + c v_z \\hat{z} \\hat{x} \\hat{y} \\text{ } s - \\hat{x} \\hat{y} \\text{ } s v_z \\hat{z} \\text{ } c - \\hat{x} \\hat{y} \\text{ } s v_z \\hat{z} \\hat{x} \\hat{y} \\text{ } s = (v_x cc) \\hat{x} + (v_x cs) \\hat{x} \\hat{x} \\hat{y} - (v_x sc) \\hat{x} \\hat{y} \\hat{x} - (v_x ss) \\hat{x} \\hat{y} \\hat{x} \\hat{x} \\hat{y} + (v_y cc) \\hat{y} + (v_y cs) \\hat{y} \\hat{x} \\hat{y} - (v_y sc) \\hat{x} \\hat{y} \\hat{y} - (v_y ss) \\hat{x} \\hat{y} \\hat{y} \\hat{x} \\hat{y} + (v_z cc) \\hat{z} + (v_z cs) \\hat{z} \\hat{x} \\hat{y} - (v_z sc) \\hat{x} \\hat{y} \\hat{z} - (v_z sin(\\theta) sin(\\theta)) \\hat{x} \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\] \\[ \\text{I had to split it in } 2 \\text{, it reached the limit of LaTeX symbols per line.} \\] \\[ \\hat{x} \\hat{x} \\hat{y} = \\hat{y} \\] \\[ \\hat{x} \\hat{y} \\hat{x} = -\\hat{y} \\hat{x} \\hat{x} = -\\hat{y} \\] \\[ \\hat{x} \\hat{y} \\hat{x} \\hat{x} \\hat{y} = \\hat{x} \\hat{y} \\hat{y} = \\hat{x} \\] \\[ \\hat{y} \\hat{x} \\hat{y} = -\\hat{y} \\hat{y} \\hat{x} = -\\hat{x} \\] \\[ \\hat{x} \\hat{y} \\hat{y} = \\hat{x} \\] \\[ \\hat{x} \\hat{y} \\hat{y} \\hat{x} \\hat{y} = \\hat{x} \\hat{x} \\hat{y} = \\hat{y} \\] \\[ \\hat{z} \\hat{x} \\hat{y} = -\\hat{x} \\hat{z} \\hat{y} = \\hat{x} \\hat{y} \\hat{z} \\] \\[ \\hat{x} \\hat{y} \\hat{z} \\hat{x} \\hat{y} = -\\hat{x} \\hat{y} \\hat{x} \\hat{z} \\hat{y} = \\hat{x} \\hat{x} \\hat{y} \\hat{z} \\hat{y} = \\hat{y} \\hat{z} \\hat{y} = -\\hat{z} \\hat{y} \\hat{y} = -\\hat{z} \\] \\[ (c - \\hat{x} \\hat{y} \\text{ } s) \\text{ } \\vec{v} \\text{ } (c + \\hat{x} \\hat{y} \\text{ } s) = (v_x cc) \\hat{x} + (v_x cs) \\hat{y} + (v_x sc) \\hat{y} - (v_x ss) \\hat{x} + (v_y cc) \\hat{y} - (v_y cs) \\hat{x} - (v_y sc) \\hat{x} - (v_y ss) \\hat{y} + (v_z cc) \\hat{z} + (v_z cs) \\hat{x} \\hat{y} \\hat{z} - (v_z sc) \\hat{x} \\hat{y} \\hat{z} + (v_z ss) \\hat{z} \\] \\[ (v_x cc) \\hat{x} + (v_x cs) \\hat{y} + (v_x sc) \\hat{y} - (v_x ss) \\hat{x} = (v_x cc - v_x ss) \\hat{x} + (v_x cs + v_x sc) \\hat{y} = (v_x(cos(\\theta) cos(\\theta) - sin(\\theta) sin(\\theta))) \\hat{x} + (v_x(cos(\\theta) sin(\\theta) + sin(\\theta) cos(\\theta))) \\hat{y} \\] <p>And then, with some trigonometry identities...</p> \\[ cos(\\theta + \\theta) = cos(\\theta) cos(\\theta) - sin(\\theta) sin(\\theta) \\] <p>\\(500\\) Lines!</p> \\[ sin(\\theta + \\theta) = cos(\\theta) sin(\\theta) + sin(\\theta) cos(\\theta) \\] \\[ (v_x cc) \\hat{x} + (v_x cs) \\hat{y} + (v_x sc) \\hat{y} - (v_x ss) \\hat{x} = (v_x cos(2 \\theta)) \\hat{x} + (v_x sin(2 \\theta)) \\hat{y} \\] \\[ (v_y cc) \\hat{y} - (v_y cs) \\hat{x} - (v_y sc) \\hat{x} - (v_y ss) \\hat{y} = (v_y cc - v_y ss) \\hat{y} - (v_y cs + v_y sc) \\hat{x} = (v_y(cos(\\theta) cos(\\theta) - sin(\\theta) sin(\\theta))) \\hat{y} - (v_y(cos(\\theta) sin(\\theta) + sin(\\theta) cos(\\theta))) \\hat{x} \\] \\[ (v_y cc) \\hat{y} - (v_y cs) \\hat{x} - (v_y sc) \\hat{x} - (v_y ss) \\hat{y} = (v_y cos(2 \\theta)) \\hat{y} - (v_y sin(2 \\theta)) \\hat{x} \\] \\[ (v_x cc) \\hat{x} + (v_x cs) \\hat{y} + (v_x sc) \\hat{y} - (v_x ss) \\hat{x} + (v_y cc) \\hat{y} - (v_y cs) \\hat{x} - (v_y sc) \\hat{x} - (v_y ss) \\hat{y} = (v_x cos(2 \\theta)) \\hat{x} + (v_x sin(2 \\theta)) \\hat{y} + (v_y cos(2 \\theta)) \\hat{y} - (v_y sin(2 \\theta)) \\hat{x} = (v_x cos(2 \\theta) - v_y sin(2 \\theta)) \\hat{x} + (v_y cos(2 \\theta) + v_x sin(2 \\theta)) \\hat{y} \\] \\[ (v_z cc) \\hat{z} + (v_z cs) \\hat{x} \\hat{y} \\hat{z} - (v_z sc) \\hat{x} \\hat{y} \\hat{z} + (v_z ss) \\hat{z} = (v_z cc) \\hat{z} + (v_z ss) \\hat{z} = (v_z(cc + ss)) \\hat{z} \\] <p>And then, with some more trigonometry identities...</p> \\[ (v_z cc) \\hat{z} + (v_z cs) \\hat{x} \\hat{y} \\hat{z} - (v_z sc) \\hat{x} \\hat{y} \\hat{z} + (v_z ss) \\hat{z} = (v_z) \\hat{z} \\] \\[ \\text{Also, } cos(2 \\theta) = C \\text{ and } sin(2 \\theta) = S \\] \\[ \\vec{v} \\prime \\prime = (v_x C - v_y S) \\hat{x} + (v_y C + v_x S) \\hat{y} + v_z \\hat{z} \\] \\[ e^{-\\hat{x} \\hat{y} \\theta} \\vec{v} e^{\\hat{x} \\hat{y} \\theta} = \\vec{v} \\prime \\prime \\] \\[ \\text{Yessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss! (Yes, that was } 100 \\text{ s's.) Now I have proved that it did rotate twice. There is a simple fix, divide the angle by } 2. \\] \\[ \\vec{v} \\prime = e^{-\\hat{x} \\hat{y} \\frac{\\theta}{2}} \\vec{v} e^{\\hat{x} \\hat{y} \\frac{\\theta}{2}} \\] \\[ \\text{And there it is, } \\vec{v} \\text{ in its prime.} \\]"},{"location":"geometric_algebra.html#maxwells-equation-singular-spoilers","title":"Maxwell's equation (singular) (#spoilers)","text":"\\[ \\text{First, I don't know what any of it means, but here's the un-simplified Maxwell's equations (in the order of Gauss's law, Ampere's law, Faraday's law, and, I dunno, Gauss's law for magnetism I guess.):} \\] \\[ \\vec{\\nabla} \\cdot \\vec{E} = \\frac{\\rho}{\\epsilon_0} \\] <p>And cut! This page has reached the maximum LaTeX limit, continue at Geometric algebra 2!</p>"},{"location":"geometric_algebra_2.html","title":"Geometric algebra 2","text":"\\[ \\text{Where we last left off, we were at Maxwell's equations.} \\] \\[ \\vec{\\nabla} \\cdot \\vec{E} = \\frac{\\rho}{\\epsilon_0} \\] \\[ \\vec{\\nabla} \\times \\vec{B} = \\mu_0 (\\vec{J} + \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}) \\] \\[ \\vec{\\nabla} \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t} \\] \\[ \\vec{\\nabla} \\cdot \\vec{B} = 0 \\] \\[ \\text{That is, in the order of: Gauss's law, Ampere's law, Faraday's law, } \\vec{\\nabla} \\cdot \\vec{B} = 0 \\text{'s law.} \\] \\[ \\text{That's it!} \\]"},{"location":"geometric_algebra_2.html#credits","title":"Credits","text":"\\[ \\text{A.P. (For privacy reasons, I won't say his full name.) He helped sometimes.} \\] \\[ \\text{Sudgy (that's just his username) who made a great introduction to geometric algebra.} \\] \\[ \\text{Silaspe (me) did everything else.} \\]"},{"location":"geometric_algebra_2.html#jk","title":"JK","text":"<p>It would be easier to go from Maxwell's equation (singular) to Maxwell's equations (plural) than to go the other way around. Here it is... After I combine some stuff. (By the way, \\(i = \\hat{x} \\hat{y} \\hat{z}\\) because it is \\(3d\\).)</p>"},{"location":"geometric_algebra_2.html#the-spacetime-gradient","title":"the spacetime gradient","text":"\\[ \\text{First, the differentials. There are two, to combine into into the spacetime gradient, the partial derivative with respect to time (} \\frac{\\partial}{\\partial t} \\text{), and the gradient with respect to space (} \\vec{\\nabla} \\text{) (which equals } \\frac{\\partial}{\\partial x} \\hat{x} + \\frac{\\partial}{\\partial y} \\hat{y} + \\frac{\\partial}{\\partial z} \\hat{z} \\text{) this spacetime gradient will be called } \\nabla \\text{. (As opposed to } \\vec{\\nabla} \\text{.)} \\] \\[ \\nabla = \\frac{\\partial}{\\partial t} + \\vec{\\nabla} \\] \\[ \\text{Actually, no. } \\] \\[ \\nabla = \\frac{1}{c} \\frac{\\partial}{\\partial t} + \\vec{\\nabla} \\] <p>Doing this may seem familiar if you've worked with realitivity enough.</p> \\[ \\text{This combination also makes sense because we now just have the sum of four derivatives (} \\nabla = \\frac{1}{c} \\frac{\\partial}{\\partial t} + \\frac{\\partial}{\\partial x} \\hat{x} + \\frac{\\partial}{\\partial y} \\hat{y} + \\frac{\\partial}{\\partial z} \\hat{z} \\text{), so in the end, it's pretty similar to the traditional gradient.} \\]"},{"location":"geometric_algebra_2.html#the-electromagnetic-source","title":"the electromagnetic source","text":"\\[ \\text{Next: combine the sources that create the electric and magnetic fields (to create the source that creates the electromagnetic field), there are two (again), the charge density (} \\rho \\text{), and the current (} \\vec{J} \\text{). (by the way, this source will be called } J \\text{, as opposed to } \\vec{J} \\text{)To combine these, just add them!} \\] \\[ J = \\rho + \\vec{J} \\] \\[ \\text{Actually, subtract them.} \\] \\[ J = \\rho - \\vec{J} \\] \\[ \\text{Don't worry about the minus sign.} \\] \\[ \\text{Actually, wrong again, but this can be fixed by adding a factor of } c \\text{ to the charge density.} \\] \\[ J = c \\rho - \\vec{J} \\] <p>Again, this is seen a lot in realitivity, so it's nothing new in physics.</p> \\[ \\text{It also gives an interesting new interpretation of charge density as a current that is moving through time and not space.} \\]"},{"location":"geometric_algebra_2.html#the-electromagnetic-field","title":"the electromagnetic field","text":"\\[ \\text{Finally, we need to combine the electric and magnetic fields(} \\vec{E} \\text{ and } \\vec{B} \\text{) into one electromagnetic field (} F \\text{). But unlike before, we can NOT just add them, the issue this time is that they are both vectors, so they're components will mix. But now's the time to kill two birds with one stone, you see, the magnetic field is traditionally defined with a cross product, and NOT a wedge product, but instead of re-defining the magnetic field, remember how the wedge product is just } i \\text{ times the cross product? Using that instead, we get this:} \\] \\[ F = \\vec{E} + i \\vec{B} \\] \\[ \\text{Actually, wrong again, again, but this can be fixed by adding a factor of } c \\text{ to the magnetic field.} \\] \\[ F = \\vec{E} + ic \\vec{B} \\] \\[ \\text{Some people actually call } c \\vec{B} \\text{ the magnetic field, so this factor of } c \\text{ is not too strangely placed.} \\] \\[ \\text{Are you ready to see Maxwell's equation? (Not to equations (plural), but equation (singular)).} \\] \\[ \\nabla F = J \\] \\[ \\text{That's it, this one simple equation describes all electromagnetic phenomena.} \\] \\[ \\text{Actually, I will admit, I cheated a bit} \\] \\[ \\nabla F = \\frac{J}{c \\epsilon_0} \\] \\[ \\text{I said it that way so that it would look better.} \\] \\[ \\text{To prove this formula, we need} \\]"},{"location":"geometric_algebra_2.html#maxwells-translation-i-just-came-up-with-that-name","title":"Maxwell's translation (I just came up with that name.)","text":"\\[ \\text{First, expand out all of the definitions that we made earlier.} \\] \\[ (\\frac{1}{c} \\frac{\\partial}{\\partial t} + \\vec{\\nabla}) \\text{ } (\\vec{E} + ic \\vec{B}) = \\frac{c \\rho - \\vec{J}}{c \\epsilon_0} \\] \\[ \\frac{1}{c} \\frac{\\partial \\vec{E}}{\\partial t} + \\vec{\\nabla} \\vec{E} + ic \\frac{1}{c} \\frac{\\partial \\vec{B}}{\\partial t} + ic \\vec{\\nabla} \\vec{B} = \\frac{c \\rho - \\vec{J}}{c \\epsilon_0} \\] \\[ \\frac{1}{c} \\frac{\\partial \\vec{E}}{\\partial t} + \\vec{\\nabla} \\cdot \\vec{E} + \\vec{\\nabla} \u2227 \\vec{E} + i \\frac{\\partial \\vec{B}}{\\partial t} + ic \\vec{\\nabla} \\cdot \\vec{B} + ic \\vec{\\nabla} \u2227 \\vec{B} = \\frac{c \\rho}{c \\epsilon_0} - \\frac{\\vec{J}}{c \\epsilon_0} \\] \\[ \\vec{\\nabla} \\cdot \\vec{E} + \\frac{1}{c} \\frac{\\partial \\vec{E}}{\\partial t} + ic \\vec{\\nabla} \u2227 \\vec{B} + \\vec{\\nabla} \u2227 \\vec{E} + i \\frac{\\partial \\vec{B}}{\\partial t} + ic \\vec{\\nabla} \\cdot \\vec{B} = \\frac{\\rho}{\\epsilon_0} - \\frac{\\vec{J}}{c \\epsilon_0} \\] <p>\\(100\\) Lines.</p> \\[ \\vec{\\nabla} \\cdot \\vec{E} = \\text{A scalar} \\] \\[ \\frac{1}{c} \\frac{\\partial \\vec{E}}{\\partial t} + ic \\vec{\\nabla} \u2227 \\vec{B} = \\text{a vector} \\] \\[ \\vec{\\nabla} \u2227 \\vec{E} + i \\frac{\\partial \\vec{B}}{\\partial t} = \\text{a bivector} \\] \\[ ic \\vec{\\nabla} \\cdot \\vec{B} = \\text{a trivector} \\] \\[ \\text{And on the right hand side:} \\] \\[ \\frac{\\rho}{\\epsilon_0} = \\text{A scalar} \\] \\[ \\frac{\\vec{J}}{c \\epsilon_0} = \\text{a vector.} \\] \\[ \\text{But then, you realize (or, at least, I realize) } 2 \\text{ things, } 1 \\text{: there are also bivector and trivector components on the right, but they are both } 0 \\text{, } 2 \\text{: for both to be equal, they must have the same scalar component, vector component, bivector component, and trivector component. So, we can derive four new equations:} \\] \\[ \\vec{\\nabla} \\cdot \\vec{E} = \\frac{\\rho}{\\epsilon_0} \\] \\[ \\frac{1}{c} \\frac{\\partial \\vec{E}}{\\partial t} + ic \\vec{\\nabla} \u2227 \\vec{B} = -\\frac{\\vec{J}}{c \\epsilon_0} \\] \\[ \\vec{\\nabla} \u2227 \\vec{E} + i \\frac{\\partial \\vec{B}}{\\partial t} = 0 \\] \\[ ic \\vec{\\nabla} \\cdot \\vec{B} = 0 \\] \\[ \\text{The first equation is Gauss's law!} \\] \\[ \\text{Second equation:} \\] \\[ \\frac{1}{c^2} \\frac{\\partial \\vec{E}}{\\partial t} + i \\vec{\\nabla} \u2227 \\vec{B} = -\\frac{\\vec{J}}{c^2 \\epsilon_0} \\] \\[ c = \\frac{1}{\\sqrt{\\mu_0 \\epsilon_0}} \\] \\[ c^2 = \\frac{1}{\\mu_0 \\epsilon_0} \\] \\[ \\frac{1}{c^2} = \\mu_0 \\epsilon_0 \\] \\[ \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t} + i \\vec{\\nabla} \u2227 \\vec{B} = -\\frac{1}{c^2} \\frac{\\vec{J}}{\\epsilon_0} \\] \\[ \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t} + i \\vec{\\nabla} \u2227 \\vec{B} = -\\mu_0 \\epsilon_0 \\frac{\\vec{J}}{\\epsilon_0} \\] \\[ \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t} + i \\vec{\\nabla} \u2227 \\vec{B} = -\\mu_0 \\vec{J} \\] \\[ \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t} + i^2 \\vec{\\nabla} \\times \\vec{B} = -\\mu_0 \\vec{J} \\] \\[ \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t} - \\vec{\\nabla} \\times \\vec{B} = -\\mu_0 \\vec{J} \\] \\[ -\\vec{\\nabla} \\times \\vec{B} = -\\mu_0 \\vec{J} - \\mu_0 \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t} \\] \\[ \\vec{\\nabla} \\times \\vec{B} = \\mu_0 (\\vec{J} + \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}) \\] \\[ \\text{We now have Ampere's law.} \\] \\[ \\text{Third equation:} \\] \\[ i \\vec{\\nabla} \\times \\vec{E} + i \\frac{\\partial \\vec{B}}{\\partial t} = 0 \\] \\[ \\vec{\\nabla} \\times \\vec{E} + \\frac{\\partial \\vec{B}}{\\partial t} = 0 \\] \\[ \\vec{\\nabla} \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t} \\] \\[ \\text{We now have Faraday's law.} \\] \\[ \\text{Finally, we have the fourth (and easiest equation after Gauss's law):} \\] \\[ \\vec{\\nabla} \\cdot \\vec{B} = 0 \\] <p>and, that proves it! (and in the original order as a bonus.) (Insert text here about how I had a lot of fun in this \\(3\\) month long journey (started on Apr \\(16\\)), and that I could go on for an entire page about using geometric algebra for group theory, number theory, modular arithmetic, and so on. And how, if only, this was the \\(500\\)'th commit to this website.) And, with that, the page has ended, thanks for watching! (Sorry, reading. I watch way too much youtube.)</p>"},{"location":"geometric_algebra_2.html#credits_1","title":"Credits","text":"\\[ \\text{I think that I already did this part.} \\] <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>\\(200\\) Lines.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p>"},{"location":"geometric_algebra_2.html#post-credit-scene","title":"post credit scene","text":"<p>even if Hestenes' law (I looked it up (on ChatGPT), that is the name of \\(\\nabla F = \\frac{J}{c \\epsilon_0}\\)) is more compressed, it is an equality of multivectors, and there should be a version that has four different equations, except they are about \\(k\\)-vectors (slightly less confusing). Also, if you don't know how to use \\(\\nabla\\), \\(J\\), \\(F\\), and the geometric product, then you shouldn't have to (the wedge product just makes sense). But still, a few things should be simplified (e.g. every cross product) (I will use the \\(\\rightarrow\\) sign to denote a re-definition), and I will be adding these definitions as I go.</p> \\[ \\text{First, } B = i \\vec{B} \\text{ is a must have.} \\] <p>Do you remember when we turned the equality of multivectors into four equalities of \\(k\\)-vectors that were turned into Maxwell's equations? That means that Maxwell's equations seem like a good starting point. Let's go from easiest to hardest, so, first, Gauss's law.</p> \\[ \\vec{\\nabla} \\cdot \\vec{E} = \\frac{\\rho}{\\epsilon_0} \\] \\[ \\text{Ok, it has an electric field (I approve), a dot product (I approve), and a bunch of other things that I approve of. In total, I approve.} \\] \\[ \\text{Next, Faraday's law.} \\] \\[ \\vec{\\nabla} \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t} \\] \\[ \\frac{\\vec{\\nabla} \u2227 \\vec{E}}{i} = \\frac{-\\frac{\\partial B}{\\partial t}}{i} \\] \\[ \\vec{\\nabla} \u2227 \\vec{E} = -\\frac{\\partial B}{\\partial t} \\] \\[ \\text{Next, } \\vec{\\nabla} \\cdot \\vec{B} = 0 \\text{'s law.} \\] \\[ \\vec{\\nabla} \\cdot \\vec{B} = 0 \\] \\[ \\text{You can't really just go } \\vec{\\nabla} \\cdot \\frac{B}{i} = 0 \\text{, } \\frac{\\vec{\\nabla} \\cdot B}{i} = 0 \\text{, } \\vec{\\nabla} \\cdot B = 0 \\text{, because that is not necessarily true, you see, } U \u2227 (VW) \\text{ does not necessarily equal } (U \u2227 V) W \\text{, Instead, you have to think about what } \\vec{\\nabla} \\cdot \\vec{B} = 0 \\text{ is trying to say.} \\] \\[ \\text{Actually, I checked, you can just go } \\vec{\\nabla} \\cdot \\frac{B}{i} = 0 \\text{, } \\frac{\\vec{\\nabla} \\cdot B}{i} = 0 \\text{, } \\vec{\\nabla} \\cdot B = 0 \\text{!} \\] \\[ \\vec{\\nabla} \\cdot B = 0 \\]"},{"location":"geometric_algebra_2.html#maxwells-alternative-equations","title":"Maxwell's alternative equations","text":"\\[ \\vec{\\nabla} \\cdot \\vec{E} = \\frac{\\rho}{\\epsilon_0} \\] \\[ \\vec{\\nabla} \\cdot B = 0 \\] \\[ \\vec{\\nabla} \u2227 \\vec{E} = -\\frac{\\partial B}{\\partial t} \\] \\[ \\text{And, I can't figure out Ampere's law. The most recent attempt where I expanded out Hesthenes' law with bivector } B \\text{ ended up with } 4 \\text{ really weird equations. If I eventually figure it out, or someone else solves an equivalent problem, I'll put it below this text.} \\]"},{"location":"geometric_algebra_2.html#amperes-law","title":"Ampere's law","text":"\\[ \\text{Actually, I checked, you can't just go } \\vec{\\nabla} \\cdot \\frac{B}{i} = 0 \\text{, } \\frac{\\vec{\\nabla} \\cdot B}{i} = 0 \\text{, } \\vec{\\nabla} \\cdot B = 0 \\text{. The true equation for } \\vec{\\nabla} \\cdot B \\text{ was called \"Ampere's law\" by, I forgot his name. Here's } \\vec{\\nabla} \\cdot \\vec{B} = 0 \\text{/Ampere's law anyways.} \\] \\[ \\vec{\\nabla} \\cdot B = \\mu_0 (\\vec{J} + \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}) \\] \\[ \\text{But then I realized, there should be } 5 \\text{ equations, one for } \\vec{\\nabla} \\cdot \\vec{E} \\text{, one for } \\vec{\\nabla} \u2227 \\vec{E} \\text{, one for } \\vec{\\nabla} \\cdot B \\text{, one for } \\vec{\\nabla} \u2227 B \\text{, and one for } \\vec{\\nabla} \u2218 B \\text{. (That I will name after the person who discovers it first.)} \\] \\[ \\text{Let me explain what this \"\u2218\" thing is. First, it is the \"surface product\" as opposed to dot product or wedge product (in between the inner and outer products). If } \\vec{\\nabla} \\cdot \\text{ is the divergence and } \\vec{\\nabla} \u2227 \\text{ is the curl, } \\vec{\\nabla} \u2218 \\text{ is the \"semicurl\".} \\] \\[ \\text{But what actually is \u2218? } \\Lambda \\text{wnser: (well, it's a thing that I came up with, but) } \\vec{U} \\vec{V} \\text{ for bivectors } \\vec{U} \\text{ and } \\vec{V} \\text{ is } \\vec{U} \\cdot \\vec{V} + \\vec{U} \u2227 \\vec{V} + \\vec{U} \u2218 \\vec{V} \\text{ (actually, } \\vec{U} \u2227 \\vec{V} \\text{ is a } 4 \\text{-vector, so the following statement only works in } 4 \\text{d or higher). You see, if you multiply } \\vec{U} \\text{ and } \\vec{V} \\text{ numerically, you get a scalar (the dot product), a } 4 \\text{-vector (the wedge product), and a bivector (the surface product). Actually, there is a handy formula for the geometric product of an } n \\text{-vector and a } k \\text{-vector, it's an } |n - k| \\text{-vector, plus an } |n - k| + 2 \\text{-vector, plus an } |n - k| + 4 \\text{-vector, all the way to an } n + k \\text{-vector. The } |n - k| \\text{-vector is the dot product, the } n + k \\text{-vector is the wedge product, and everything else is a surface product. But for bivectors, there is only one, and if there is more than one, the bivector one would be \u2218}_2 \\text{, the trivector one would be \u2218}_3 \\text{, the } 4 \\text{-vector one would be \u2218}_4 \\text{, and the } k \\text{-vector one would be \u2218}_k \\text{.} \\]"},{"location":"geometric_algebra_2.html#plan-of-attack","title":"plan of attack!","text":"\\[ \\text{OG Ampere's law:} \\] \\[ \\vec{\\nabla} \\times \\vec{B} = \\mu_0 (\\vec{J} + \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}) \\] \\[ \\text{I'll just focus on the left side of this equation for now.} \\] \\[ \\frac{\\vec{\\nabla} \u2227 \\vec{B}}{i} \\] \\[ \\frac{1}{i} (\\vec{\\nabla} \u2227 \\vec{B}) \\] \\[ \\frac{1}{i} (\\vec{\\nabla} \u2227 \\frac{B}{i}) \\] \\[ \\frac{1}{i} (\\vec{\\nabla} \u2227 ((\\frac{1}{i} B)) \\] \\[ i^2 = -1 \\] \\[ i i = -1 \\] \\[ -i i = 1 \\] \\[ (i)(-i) = 1 \\] \\[ \\frac{1}{i} = -i \\] \\[ \\frac{1}{i} (\\vec{\\nabla} \u2227 (-iB)) \\] \\[ (c \\vec{u}) \u2227 \\vec{v} = c (\\vec{u} \u2227 \\vec{v}) \\text{ (For scalar } c \\text{.)} \\] \\[ -\\frac{1}{i} (\\vec{\\nabla} \u2227 (iB)) \\] \\[ \\frac{1}{-i} (\\vec{\\nabla} \u2227 (iB)) \\] \\[ \\frac{1}{-i} (\\vec{\\nabla} \u2227 (iB)) = \\mu_0 (\\vec{J} + \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}) \\] <p>\\(300\\) Lines.</p> \\[ \\vec{\\nabla} \u2227 (iB) = -i \\mu_0 (\\vec{J} + \\epsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}) \\] \\[ \\text{So, the problem essentially boils down to finding } \\vec{u} \u2227 (iV) \\text{.} \\] \\[ \\vec{u} = u_1 \\hat{x} + u_2 \\hat{y} + u_3 \\hat{z} \\] \\[ V = V_1 \\hat{x} \\hat{y} + V_2 \\hat{x} \\hat{z} + V_3 \\hat{y} \\hat{z} \\] \\[ iV = (V_1 \\hat{x} \\hat{y} + V_2 \\hat{x} \\hat{z} + V_z \\hat{y} \\hat{z}) \\hat{x} \\hat{y} \\hat{z} = V_1 \\hat{x} \\hat{y} \\hat{x} \\hat{y} \\hat{z} + V_2 \\hat{x} \\hat{z} \\hat{x} \\hat{y} \\hat{z} + V_3 \\hat{y} \\hat{z} \\hat{x} \\hat{y} \\hat{z} = - V_1 \\hat{x} \\hat{x} \\hat{y} \\hat{y} \\hat{z} - V_2 \\hat{x} \\hat{x} \\hat{z} \\hat{y} \\hat{z} - V_3 \\hat{y} \\hat{z} \\hat{y} \\hat{x} \\hat{z} = - V_1 \\hat{z} - V_2 \\hat{z} \\hat{y} \\hat{z} + V_3 \\hat{y} \\hat{y} \\hat{z} \\hat{x} \\hat{z} = - V_1 \\hat{z} + V_2 \\hat{z} \\hat{z} \\hat{y} + V_3 \\hat{z} \\hat{x} \\hat{z} = - V_1 \\hat{z} + V_2 \\hat{y} - V_3 \\hat{z} \\hat{z} \\hat{x} = - V_1 \\hat{z} + V_2 \\hat{y} - V_3 \\hat{x} \\] \\[ iV = - V_3 \\hat{x} + V_2 \\hat{y} - V_1 \\hat{z} \\]"},{"location":"group_theory.html","title":"Group theory","text":"<p>I am finally making this page to celebrate the upcoming \\(2000\\)th line on the brainstorm page.</p> <p>Here's some great videos about the subject that should get at least some credit: Group theory, abstraction, and the 196,883-dimensional monster for the intuition, This playlist by Nemean for the proofs, the examples of things, and I would assume isomorphism examples (I decided not to watch the second video yet), and This playlist by Another Roof Which I don't think I'll take any of the same things from it.</p> <p>I'll start off with the textbook definition of a group:</p> <p>P.S. I don't have a group theory textbook.</p>"},{"location":"group_theory.html#textbook-definitions","title":"textbook definitions","text":"<p>A group is a set or collection of things together with a binary operation, e.g. addition or multiplication because they input two things and output one thing (I'll be writing this as \u2218) such that the following hold:</p> <p>\\(1\\). Closure (sometimes a given): If you have \\(a\\) and \\(b\\) in the group, then \\(a\\) \u2218 \\(b\\) is also in the group. This makes sure that your group is, well, closed.</p> <p>\\(2\\). Associativity: If you have \\(a\\), \\(b\\) and \\(c\\) in the group, then \\((a\\) \u2218 \\(b)\\) \u2218 \\(c\\) is equal to \\(a\\) \u2218 \\((b\\) \u2218 \\(c)\\)*. This makes sure both that addition, multiplication, function composition, and matrix multiplication all work, but also makes a lot of other things don't.</p> <p>\\(3\\). Identity (or neutral depending on where you're from): There must always be a term in the group (call it \\(e\\)) where if you have \\(a\\) in the group, then \\(a\\) \u2218 \\(e\\) is equal to \\(e\\) \u2218 \\(a\\) is equal to \\(a\\). This is like a \\(0\\) for addition or a \\(1\\) for multiplication. There must always be an option to do nothing.</p> <p>\\(4\\). Inverses: If you have \\(a\\) in the group, then there is also \\(a^{-1}\\) in the group where \\(a\\) \u2218 \\(a^{-1}\\) is equal to \\(e\\). This is like the negative for addition, or \\(1\\) divided by for multiplication. You must always be able to undo what you have done.</p> <p>Notice: there is no point where I say that the operation is commutative (i.e. \\(a\\) \u2218 \\(b = b\\) \u2218 \\(a\\)). If it is, the group is known as an Abelian group.</p> <p>Also, by the way, it is common to notate \\(a\\) \u2218 \\(a\\) \u2218 \\(a\\) \u2218 ... \u2218 \\(a\\) with \\(n\\) total a's as \\(a^n\\).</p> <p>A good way to think about what some groups actually represent is as sets of symmetries. This is because these \\(4\\) rules are exactly what you'd expect rotations and reflections to do with the operation of doing one after the other. Now the closure rule makes sense because if doing one thing preserves the symmetry, and doing another thing preserves the symmetry, then doing one after the other should also preserve the symmetry, the associativity rule makes sense because rotating, then flipping... then rotating should be the same as rotating... then flipping, then rotating, the Identity Rule makes sense because doing nothing should preserve the symmetry, and the inverses rule makes sense because if you rotate clockwise, then of course you should also be able to rotate counterclockwise.</p> <p>*For this reason, I will be denoting both as \\(a\\) \u2218 \\(b\\) \u2218 \\(c\\).</p>"},{"location":"group_theory.html#example-groups","title":"example groups","text":"<p>An example of a group is the integers with the operation of addition.</p> <p>Adding two integers would result in another integer. Addition is associative and there is only one \\(a + b + c\\), there is, of course, an identity element, of which is zero, and the inverse would of course be the negative.</p> <p>Thus, the integers form a group! And the symbol is \\(\u2124\\).</p> <p>Let's see what happens to the integers with an operation of multiplication!</p> <p>Multiplying two integers would result in another integer. There is only one \\(a \\times b \\times c\\). There is an identity element (i.e. \\(1\\)). But sadly, only \\(1\\) and \\(-1\\) have inverses.</p> <p>Thus, this is not a group :(</p> <p>This is also why it's written \\(\u2124\\) and not \\(\u2124\\) w/ addition.</p> <p>But this does hint that the rationals with the operation of multiplication form a group (as long as you exclude zero because it doesn't have an inverse), and that is correct!</p> <p>And the symbol is \\(\u211a^\\times\\).</p> <p>And if you follow through all of the steps yourself or you got a hint from that times superscript, you know that the rationals with an operation of addition also form a group: \\(\u211a^+\\).</p> <p>Let me introduce you to another type of group, a certain type of finite group: the dihedral groups! I'll use the example of \\(\\text{D}_3\\) (the dihedral group of order \\(3\\)). It is the group of all rotations and reflections of an equilateral triangle that leave it looking the same, with the operation of doing one after the other.</p> <p>The operations are: do nothing, rotate clockwise \\(120\\)\u00b0, rotate counterclockwise \\(120\\)\u00b0 reflect along the vertical axis, reflect on a \\(60\\)\u00b0 tilted axis, and reflect along a \\(60\\)\u00b0 tilted in the other direction axis.</p> <p>This group works similarly for other numbers, with the subscript being the amount of faces on the regular polygon.</p> <p>But there's a big debate over if the subscript should be \\(n\\) or \\(2n\\), because on the one hand, it should be \\(n\\) because that's the amount of sides the polygon has (the group is kind of based off of that), But on the other hand it's pronounced \"the dihedral group of order \\(n\\) or \\(2n\\)\" and order means size in the context of a group, so the dihedral group of order \\(2n\\) is what it should be called.</p> <p>Personally, I think that the subscript should be the amount of faces.</p> <p>There was this one time when I was talking to ChatGPT, and it said that \\(\\text{D}_4\\) was the dihedral group of order \\(8\\).</p> <p>There's also the integers with the operation of addition, but it loops back around once you reach a certain number, these groups are also known as the modular groups, and if that number is \\(n\\), i.e. the elements of the group are \\(0\\) all the way up to \\(n - 1\\), then the group it is denoted \\(\u2124_n\\) or \\(C_n\\).</p>"},{"location":"group_theory.html#proofs","title":"proofs","text":""},{"location":"group_theory.html#proof-number-1-theres-only-1-identity-element","title":"proof number \\(1\\): there's only \\(1\\) identity element.","text":"<p>This proof uses a proof by contradiction strategy. Let's say that there are more than \\(1\\) identity element. So I'm going to choose the first two being \\(e_1\\) and \\(e_2\\).</p> <p>Now, I ask of you, what is \\(e_1\\) \u2218 \\(e_2\\)? Because on the one hand, it should equal \\(e_2\\) because \\(e_1\\) times anything is that thing. But on the other hand, it should equal \\(e_1\\) because anything times \\(e_2\\) is that thing.</p> <p>Thus, because they are both equal to \\(e_1\\) \u2218 \\(e_2\\), they must themselves be equal. Thus there is only \\(1\\) identity element.</p> <p>And you can keep going with this logic, doing the same thing with the next one, and the next one, until there is only \\(1\\) left.</p> <p>QED!</p>"},{"location":"group_theory.html#proof-number-2-the-inverse-of-the-inverse-is-the-original","title":"proof number \\(2\\): The inverse of the inverse is the original","text":"<p>Every element has an inverse. So, by definition, the inverse has an inverse.</p> <p>Let's operate all of these together and see what happens.</p> \\[ a \u2218 a^{-1} \u2218 (a^{-1})^{-1} \\] <p>This should of course equal \\(a\\) because \\(a^{-1}\\) times its inverse should cancel out. But also this should equal \\((a^{-1})^{-1}\\) because \\(a\\) and its inverse should cancel out. Thus, because they are both equal to the same thing, they themselves must be equal.</p> <p>QED!</p>"},{"location":"group_theory.html#proof-number-3-the-inverse-can-cancel-out-from-either-side","title":"proof number \\(3\\): The inverse can cancel out from either side","text":"<p>The term \\(a^{-1}\\) \u2218 \\(a\\) can also be simplified. Because \\(a\\) is equal to \\((a^{-1})^{-1}\\), I can cancel \\(a^{-1}\\) with its inverse, resulting in the identity.</p> <p>\\(100\\) Lines.</p> \\[ a^{-1} \u2218 a = a^{-1} \u2218 (a^{-1})^{-1} = e \\] <p>QED!</p>"},{"location":"group_theory.html#proof-number-4-theres-only-1-inverse-for-a-given-term","title":"proof number \\(4\\): There's only \\(1\\) inverse for a given term","text":"<p>This one uses the same general strategy as proof number \\(1\\). Let's assume that there were multiple inverses, denoted \\(a^{-1}_1\\) and \\(a^{-1}_2\\). Then of course, \\(a\\) \u2218 \\(a^{-1}_1 = e\\).</p> <p>Let's see what happens when you \u2218 both sides on the left by \\(a^{-1}_2\\).</p> \\[ a^{-1}_2 \u2218 a \u2218 a^{-1}_1 = a^{-1}_2 \u2218 e \\] <p>Then \\(a^{-1}_2\\) and \\(a\\) would cancel out resulting in \\(a^{-1}_1\\) on the left. But on the other side, the identity element cancels out resulting in \\(a^{-1}_2\\). Thus, because they are both equal to the same thing, they themselves must be equal.</p> <p>QED!</p>"},{"location":"group_theory.html#proof-number-5-a2-1-a-12-and-they-can-both-be-denoted-as-a-2","title":"proof number \\(5\\): \\((a^2)^{-1} = (a^{-1})^2\\) and they can both be denoted as \\(a^{-2}\\)","text":"\\[ a^2 \u2218 (a^2)^{-1} = e \\] \\[ a^2 \u2218 (a^{-1})^2 = a \u2218 a \u2218 a^{-1} \u2218 a^{-1} = a \u2218 a^{-1} = e \\] <p>Because these are both the inverse of \\(a^2\\), and because of proof number four, they must both be the same.</p> <p>QED!</p> <p>Using the example of the integers, we have just proved that: there's only \\(1\\) zero, \\(-(-3) = 3\\), \\(3 + (-3) = (-3) + 3 = 0\\), and there's only one \\(-3\\)</p> <p>At the same time, we have just proved that: there's only one \\(1\\), \\(\\frac{1}{\\frac{1}{3}} = 3\\), \\(3 \\frac{1}{3} = \\frac{1}{3} 3 = 1\\), and there's only one \\(\\frac{1}{3}\\)</p> <p>They're all connected! They're all secretly the same! Group theory (or, more generally abstract algebra) really is the grand unified theory of algebraic systems!</p> <p>Now you can pretty much skip to any other chapter you want.</p>"},{"location":"group_theory.html#subgroups-cosets","title":"subgroups &amp; cosets","text":""},{"location":"group_theory.html#subgroups","title":"subgroups","text":"<p>Once again, I'll start with the textbook definition of a subgroup.</p>"},{"location":"group_theory.html#textbook-definitions_1","title":"textbook definitions","text":"<p>If you have a group and, taking just a portion of it, operating any of those elements together would result in another thing within that portion, then that portion is also known as a subgroup, as it is a group within a group.</p> <p>And yeah, I think that that's pretty intuitive of a definition, you can take just some of the elements of the group and they still form a group.</p>"},{"location":"group_theory.html#examples","title":"examples","text":"<p>Some examples of a subgroup are: the even integers for \\(\u2124\\), the integers for \\(\u211a^+\\), just the rotations from the dihedral groups (Which by the way are just the modular groups), or if the base is composite then the dihedral group from any of its factors, or the same thing for the modular group.</p> <p>But notice, the odd integers don't form a group because of the closure axiom, Adding two odd integers forms an even integer.</p> <p>But there are better ways to form and notate subgroups than to just guess and check. For example, the rotations can be formed by repeatedly applying the \\(120\\)\u00b0 rotation. And an example of a subgroup of a modular group is the \\(0\\), \\(3\\), \\(6\\) group within \\(\u2124_9\\) (which is isomorphic to \\(\u2124_3\\), but will get to isomorphism later).</p> <p>Something these all have in common is that they are formed by starting at the identity and then checking everything that is formed by repeatedly \u2218ing with a thing.</p> <p>There is a notation for this: \\(\u27e8a\u27e9\\) represents the sets of all powers of \\(a\\).</p> <p>The system is closed because a power of \\(a\\) \u2218 a power of \\(a\\) is a power of \\(a\\).</p> <p>And this is indeed a group even for finite groups. Because if it's a finite group then repeatedly operating \\(a\\) on it should eventually go through all of the elements and loop back around to itself. So the one before \\(a\\) has to be the identity.</p> <p>Also, if \\(e = a^8\\), then the inverse of \\(a^n\\) is just \\(a^{8 - n}\\).</p> <p>I know you might be thinking: why does it have to loop back around to the identity? What if \\(a^n\\) were equal to \\(a\\)? Well, then \u2218ing both sides by the inverse of \\(a\\), you would get \\(a^8\\) \u2218 \\(a^{-1}\\), which is of course equal to \\(a^7\\). However, on the right of the equation, you get \\(a\\) \u2218 \\(a^{-1}\\) which is of course the identity. So then \\(a^7\\) would be the identity.</p> <p>However, this argument doesn't necessarily work for integers as the group is infinitely large, and \\(\u27e82\u27e9\\) would only result in \\(2\\), \\(4\\), \\(6\\), \\(8\\), and so on, and not all of the even integers.</p> <p>Just going to say: this is still valid notation even if it isn't a group.</p> <p>But, this can be fixed! The true definition includes the identity element and the negative powers when needed, so \\(\u27e82\u27e9\\) is now a group!</p>"},{"location":"group_theory.html#cosets","title":"cosets","text":"<p>Cosets can be explained as shifted copies of subgroups.</p> <p>How you can more formally define cosets are as subgroups but each term is \u2218ed on the left (or on the right depending on the context) by a certain thing.</p> <p>Notice: it uses the word \"set\" because it isn't necessarily a group. For example, the odd numbers are a coset, but not a group.</p> <p>The notation for cosets is kind of what you'd expect. For example, the odd numbers are just notated \\(1 + \u27e82\u27e9\\) (This notation makes them seem more like arrays in coding than lists)</p> <p>Also, \\(3 + \u27e82\u27e9\\) or \\(5 + \u27e82\u27e9\\) or \\(6 + \u27e82\u27e9\\) would also work.</p> <p>Also, because of notation like \\(\u27e8a\u27e9\\) \u2218 \\(e\\), subgroups are a type of coset.</p> <p>Also, within the dihedral group, the reflections are cosets. The reason why is because if you do a reflection then a rotation, then it just rotates the reflection line.</p> <p>You can do the same thing the other way around with the identity and a flip with the shift being a rotation, and you get another case of perfect symmetry of a subgroup and its corresponding cosets perfectly filling up the group. I feel like this is building up to something big!</p>"},{"location":"group_theory.html#lagranges-theorem","title":"Lagrange's theorem","text":"<p>Lagrange's theorem (or at least something equivalent to it) states that if you pick a subgroup, then every element of the group will be covered once and only once by that subgroup and its cosets.</p> <p>Let's say that the group has \\(16\\) elements and that the elements of the subgroup are \\(e\\), \\(a\\), \\(b\\), and \\(c\\).</p> <p>If I have any element \\(x\\), then it is not too hard to cover it with a subgroup. Just shift it over by \\(x\\), and the corresponding term to the identity element will cover it as \\(x\\) \u2218 \\(e = x\\).</p> <p>\\(200\\) Lines.</p> <p>You can take this as either a subgroup or a proof that everything in the group is covered by at least one coset.</p> <p>Now, we have elements: \\(e\\), \\(a\\), \\(b\\), \\(c\\), \\(x\\), \\(x\\) \u2218 \\(a\\), \\(x\\) \u2218 \\(b\\), and \\(x\\) \u2218 \\(c\\)</p> <p>Also, now I would like to convince you that the cosets are of course the same size.</p> <p>Because if the code set is bigger than there must have been an extra term you \u2218 by to get to it.</p> <p>But it can't be any less because if \\(x\\) \u2218 \\(a = x\\) \u2218 \\(b\\), then \u2218ing both sides by \\(x^{-1}\\) (and using a little bit of proof number three), you get that \\(a = b\\), which would just mean that the original subgroup is smaller, not that the coset is smaller than the subgroup.</p> <p>But, you might still be thinking: what if the cosets overlap? Well, this is very complicated to prove. First: can a coset overlap with the original group? Well, then \\(x\\) \u2218 \\(a = b\\), so \\(x = b\\) \u2218 \\(a^{-1}\\)</p> <p>Because of the inverse property for the subgroup, \\(a^{-1}\\) is also within the subgroup. And because of the closure property for the subgroup, \\(b\\) \u2218 \\(a^{-1}\\) is in the subgroup.</p> <p>Thus, \\(x\\) must be within the subgroup. And shifting by something within the subgroup doesn't really change anything, so subgroups and cosets can't overlap.</p> <p>However, you can technically define \\(x\\) \u2218 \\(a\\) as \\((x\\) \u2218 \\(y^{-1})\\) \u2218 \\((y\\) \u2218 \\(a)\\), so you can convince yourself that they can't overlap.</p> <p>Now, we have elements: \\(e\\), \\(a\\), \\(b\\), \\(c\\), \\(x\\), \\(x\\) \u2218 \\(a\\), \\(x\\) \u2218 \\(b\\), \\(x\\) \u2218 \\(c\\), \\(y\\), \\(y\\) \u2218 \\(a\\), \\(y\\) \u2218 \\(b\\), \\(y\\) \u2218 \\(c\\), \\(z\\), \\(z\\) \u2218 \\(a\\), \\(z\\) \u2218 \\(b\\), and \\(z\\) \u2218 \\(c\\).</p> <p>We have just proven that you can neatly partition a group with a subgroup and its cosets, and from this, you can drive The following theorem:</p> <p>For any group \\(G\\) and subgroup \\(H\\), the size of \\(G\\) is divisible by the size of \\(H\\).</p> <p>This theorem is also known as Lagrange's theorem (Which Lagrange knew nothing about).</p> <p>We've been building up to this for the entire chapter! We have seen that the reflections and the rotations are both of size \\(n\\)! We have seen that the \\(0\\), \\(3\\), \\(6\\) group is of size \\(3\\) and \\(\u2124_9\\) is of size \\(9\\)! We... actually have seen no other examples for this because this only works for finite groups.</p> <p>Here's one way to think about it: if the size of a group is a prime number (like \\(7\\)), then the only possible size is of subgroups are of size \\(1\\) (which is clearly just the set containing the identity (yes, it is a group)) and the whole thing itself. So if you take any term \\(a\\) in the group and you keep \u2218ing by it, then you will eventually form a subgroup.</p> <p>Except, the size of the subgroup is either \\(1\\) or \\(7\\), and if it's \\(1\\), then clearly, \\(a\\) was just the identity.</p> <p>But if it's anything else, then the size has to be \\(7\\) as \\(a\\), not being the identity, wouldn't have an identity element. And if it's of size \\(7\\), that must be the whole group, so you can just label each term as \\(e\\), \\(a\\), \\(a^2\\), \\(a^3\\), \\(a^4\\), \\(a^5\\), and \\(a^6\\).</p> <p>Then what if we just label \\(e\\) as zero, \\(a\\) as \\(1\\), \\(a^2\\) as 2, and so on, also \u2218 as \\(+\\).</p> <p>Then we realize that every single group of which its size is prime number \\(p\\), the group is actually just \\(\u2124_p\\) in disguise!</p>"},{"location":"group_theory.html#isomorphic-groups-and-homomorphisms","title":"isomorphic groups and homomorphisms","text":""},{"location":"group_theory.html#isomorphic-groups","title":"isomorphic groups","text":"<p>First, let's just parse this term \"isomorphic\" (by the way, I think isomorphic and homomorphic mean the same thing. But isomorphic is used in group theory and category theory, while homeomorphic is used in topology) two things are isomorphic if they are related via homeomorphism (that's a direct quote from ChatGPT). homo means the same, and morphic means what you think it means. So homeomorphic means you can morph one into the other and they still look the same (as you do in topology, and it's kind of the same thing in group theory) and a homeomorphism is a method of doing just that.</p> <p>Long story short, two groups are isomorphic when they have the same general structure.</p> <p>I'll take the example of \\(\\text{V}_4\\) (what I called the \\(2x2\\) sudoku group (long story), then the rock paper scissors group, then it's formal name), then there's an identity element, if you take two non-identity ones that are themselves distinct and you \u2218 them, you get the third one, and if you take two of the same one and you \u2218 them, you get the identity.</p> <p>If a group \\(G\\) has the same structure, then we write:</p> \\[ G \\cong \\text{V}_4 \\] <p>(\\(G\\) is isomorphic to \\(\\text{V}_4\\))</p> <p>Now, how do we formally define this \"isomorphism\"? Well, I have a definition, except it's kind of long. It is:</p> <p>If you have groups \\(G\\) and \\(H\\) and \\(G\\) has operation \\(\\cdot\\) while \\(H\\) has operation \\(*\\), then they are isomorphic if you can find an isomorphism (which is a type of one to one correspondence) between them. But, it has to do the following: if you pick \\(a\\) and \\(b\\) that are within \\(G\\), then if \\(a \\cdot b\\) is equal to \\(c\\), then with the corresponding \\(E\\) (\\(e\\) was taken) and \\(f\\) in \\(H\\), you get that \\(E * f\\) is the corresponding term to \\(c\\).</p> <p>Okay, that definition was terrible. Here's a definition that was given to me by ChatGPT:</p> \\[ \\varphi : G \u2192 H \\] <p>(A mapping \\(\\varphi\\) from \\(G\\) to \\(H\\))</p> \\[ \\varphi (a \\cdot b) = \\varphi (a) * \\varphi (b) \\] <p>(If you pick \\(a\\) and \\(b\\) in \\(G\\), then \\(\\varphi (a \\cdot b) = \\varphi (a) * \\varphi (b)\\))</p> <p>It shouldn't be too hard to convince yourself that these two are the same. \\(\\varphi (a)\\) is \\(E\\), \\(\\varphi (b)\\) is \\(f\\), \\(a \\cdot b\\) is \\(c\\), \\(\\varphi (x)\\) is the corresponding term in \\(H\\) to the term \\(x\\) in \\(G\\), and then, if you substitute those in, you get \\(\\varphi (c) = E * f\\).</p> <p>And that's actually a Silas original definition, intuition, and example! (Well, apart from the ChatGPT part.)</p> <p>Now, let's look at another example of isomorphism: the real numbers with the operation of addition (\\(\u211d^+\\)), and the real numbers with the operation of multiplication (excluding zero) (\\(\u211d^\\times\\)) (This example was found in a three blue and brown video about group theory, except I don't remember which one. Just search \"three blue brown group theory\").</p> <p>If we're trying to find an isomorphism between \\(\u211d^+\\) and \\(\u211d^\\times\\), then what function would work?</p> \\[ f(x + y) = f(x)f(y) \\] <p>Does this equation look familiar to anyone? This function \\(f\\) not only exists, but it's an exponential function!</p> \\[ e^{x + y} = e^x e^y \\] <p>Thus, \\(\u211d^+ \\cong \u211d^\\times\\).</p> <p>Okay, now I'm going to watch the video by Nemean.</p>"},{"location":"group_theory.html#homomorphisms","title":"homomorphisms","text":"<p>I got \\(9\\) minutes and \\(46\\) seconds into the video and I already have so many ideas.</p> <p>First, I got the terminology wrong. ChatGPT didn't misspeak when it said \"two things are isomorphic if they're related via homomorphism\".</p> <p>But let's explore this idea of a homomorphism more. First, it might only take a few elements to define the homomorphism. Kind of like how a linear transformation only needs the two basis vectors to determine what it is for all outputs.</p> <p>In group theory, this (\\(\\varphi (a\\) \u2218 \\(b) = \\varphi (a)\\) \u2218 \\(\\varphi (b)\\)) relation of the homomorphism acts in kind of the same way, only needing a few outputs to define the whole thing. (In fact, I think that linear algebra is part of abstract algebra, of which group theory is a part of.)</p> <p>Also, homomorphisms can go between groups of different sizes, but I don't think that that means that they're isomorphic because they don't feel the same, they aren't bijections, they're surjections.</p> <p>\\(300\\) Lines.</p> <p>Here's an example: if I want to map from \\(\u2124_6\\) to \\(\u2124_3\\), I only need \\(1\\) output, which is \\(\\varphi (1)\\). Let me explain.</p> \\[ \\varphi (1) = 1 \\] \\[ \\varphi (2) = \\varphi (1 \\cdot 1) = \\varphi (1) * \\varphi (1) = 1 * 1 = 2 \\] \\[ \\varphi (2) = 2 \\] \\[ \\varphi (3) = \\varphi (2 \\cdot 1) = \\varphi (2) * \\varphi (1) = 2 * 1 = 0 \\] \\[ \\varphi (3) = 0 \\] \\[ \\varphi (4) = \\varphi (3 \\cdot 1) = \\varphi (3) * \\varphi (1) = 0 * 1 = 1 \\] \\[ \\varphi (4) = 1 \\] \\[ \\varphi (5) = \\varphi (4 \\cdot 1) = \\varphi (4) * \\varphi (1) = 1 * 1 = 2 \\] \\[ \\varphi (5) = 2 \\] \\[ \\varphi (0) = \\varphi (5 \\cdot 1) = \\varphi (5) * \\varphi (1) = 2 * 1 = 0 \\] \\[ \\varphi (0) = 0 \\] <p>If you know that \\(\\cdot\\) loops around every six and \\(*\\) loops around every three, this should make sense.</p> <p>More importantly, we just derived what all of the outputs should be for a given input using only \\(1\\) example.</p>"},{"location":"group_theory.html#parity","title":"parity","text":"<p>By the way, I came up with pretty much this entire sub-subchapter at the ice skating rink today.</p> <p>We can find more examples of homomorphism. For example, a homomorphism between \\(\u2124\\) and \\(\u2124_2\\).</p> <p>\\(\u2124_2\\) is a small group (in fact, the second smallest possible group), one containing only two elements, one where if you operate the non-identity element (that will henceforth be called \\(a\\)) with itself, you get the identity element.</p> <p>\\(a\\) to the power of any number \\(n\\) is \\(a\\) if \\(n\\) is odd and the identity elements if \\(n\\) is even. This can capture the idea of even or odd-ness, a.k.a. parity. So we can define the map \\(\\varphi\\) as just \\(a\\) raised to the power of the input number.</p> <p>No we get an important fact that I would rather show than tell:</p> \\[ \\varphi (n + k) = \\varphi (n) \u2218 \\varphi (k) \\] <p>What it was saying is that the parity of \\(n + k\\) is determined by the parity of \\(n\\) and the parity of \\(k\\) in exactly the way you'd expect. Even plus even is even, even plus odd is odd, odd plus even is odd, and odd plus odd is even. The same way how \u2218 works in \\(\u2124_2\\): \\(e\\) \u2218 \\(e = e\\), \\(e\\) \u2218 \\(a = a\\), \\(a\\) \u2218 \\(e = a\\), and \\(a\\) \u2218 \\(a = e\\).</p> <p>But there are more examples of a homomorphism from a group to \\(\u2124_2\\) (a.k.a. a parity homomorphism).</p> <p>Case in point: this one time I was talking to ChatGPT about all of the subgroups of \\(\\text{S}_4\\) (which is the group of all of the ways to arrange four things) (the reason why was because my code was so inefficient that after waiting for a solid \\(30\\) seconds, it only had the trivial group because it had to loop through every single combination of the \\(24\\) inputs which is very slow).</p> <p>The results given to me by ChatGPT were the things I expected: the trivial group, variations of \\(\u2124_2\\), variations of \\(\u2124_3\\), variations of \\(\u2124_4\\), even the group \\(\\text{V}_4\\) which was a pleasant surprise, and the group \\(\\text{D}_4\\).</p> <p>But there was one that I didn't expect (but in hindsight it makes sense): \\(\\text{A}_4\\), A group described by \\(3\\) blue \\(1\\) brown as permutations of four things that preserve a certain parity.</p> <p>This does kind of make sense as you could define a homomorphism \\(\\varphi\\) from the group \\(\\text{S}_4\\) to the group \\(\u2124_2\\). And if you plug in the numbers...</p> \\[ \\varphi (p \u2218 q) = \\varphi (p) \u2218 \\varphi (q) \\] <p>Suppose that both \\(p\\) and \\(q\\) have even parity. Then even parity \u2218 even parity would of course be even parity. Thus, \\(p\\) \u2218 \\(q\\) has even parity (this is explained better in the next sub-subchapter). So, this system is completely closed, and hence, this group has a name: \\(\\text{A}_4\\).</p> <p>By the way, the parity is if it takes an even or odd amount of \\(2\\)-swaps to get to it from the identity.</p> <p>Also by the way, Along with the whole group \\(\\text{S}_4\\), this completes ChatGPT's list.</p> <p>Also by the way, there's a great mathologer video about the parity of permutations.</p> <p>Now that I think about it, this should mean that there's a general theorem. The theorem is as follows:</p>"},{"location":"group_theory.html#theorem","title":"theorem","text":"<p>If you have a mapping \\(\\varphi\\) from a group \\(G\\) to a group \\(H\\), then all of the elements where the output of \\(\\varphi\\) is the identity element in \\(H\\), those elements themselves form a subgroup of \\(G\\).</p> <p>I actually just came up with this theorem while I was writing this, and I'm going to ask ChatGPT if this is a well-known theorem.</p> <p>The answer is of course yes, and the theorem is known as the Kernel theorem. The reason why is because the set of all elements where the output is the identity is known as the kernel of \\(\\varphi\\).</p> <p>ChatGPT also provided a sketch of a proof:</p> <p>\\(1\\). Closure. I've already done this proof, but I'll say it again.</p> \\[ a, b \\in \\text{ker} (\\varphi) \\] \\[ \\varphi (a) = \\varphi (b) = e \\] \\[ \\varphi (a \\cdot b) = \\varphi (a) * \\varphi (b) = e * e = e \\] \\[ \\varphi (a \\cdot b) = e \\] \\[ a \\cdot b \\in \\text{ker} (\\varphi) \\] <p>\\(2\\). Associativity. This one is a given as a subgroup has the same operation as the original, and thus, you don't need to prove that it is associative.</p> <p>\\(3\\). Identity. This one uses a proof by contradiction strategy. First, let's assume that \\(\\varphi (e) \\ne e\\)</p> \\[ \\varphi (a \\cdot e) = \\varphi (a) * \\varphi (e) = e * \\text{not } e = \\text{not } e \\] \\[ a \\cdot e = a \\] \\[ \\varphi (a) \\ne e \\] <p>\\(400\\) lines.</p> <p>Contradiction!</p> <p>\\(4\\). Inverses. As there is only one inverse for a given term, I'll have to prove that the one inverse is within \\(\\text{ker} (\\varphi)\\)</p> \\[ \\varphi (a) = \\varphi (a) \\] \\[ \\varphi (a \\cdot a) = \\varphi (a) * \\varphi (a) \\] \\[ \\varphi (a \\cdot a \\cdot a) = \\varphi (a) * \\varphi (a) * \\varphi (a) \\] \\[ \\vdots \\] \\[ \\varphi (a \\cdot a \\cdot a \\cdot \\dots \\cdot a) = \\varphi (a) * \\varphi (a) * \\varphi (a) * \\dots * \\varphi (a) \\] \\[ \\varphi (a^n) = \\varphi (a)^n \\] <p>Assuming that \\(G\\) is finite, I can plug \\(-1\\) into this equation.</p> \\[ \\varphi (a^{-1}) = (\\varphi (a))^{-1} = e^{-1} = e \\] \\[ \\varphi (a^{-1}) = e \\] <p>\\(420\\) Lines. (I'm a kid, can you blame me?)</p> \\[ a^{-1} \\in \\text{ker} (\\varphi) \\] <p>This completes the proof.</p> <p>I also found in the video a simple reproof that \\(\\varphi (e) = e\\):</p> \\[ \\varphi (a \\cdot e) = \\varphi (a) * \\varphi (e) \\] \\[ \\varphi (a \\cdot e) = \\varphi (a) \\] \\[ \\varphi (a) = \\varphi (a) * \\varphi (e) \\] <p>And as there's only one identity element, this completes the proof.</p> <p>I also found a proof in the video about the inverse rule:</p> \\[ a \\in \\text{ker} (\\varphi) \\] \\[ \\varphi (a \\cdot a^{-1}) = \\varphi (a) * \\varphi (a^{-1}) \\] \\[ \\varphi (a \\cdot a^{-1}) = \\varphi (e) = e \\] \\[ \\varphi (a) * \\varphi (a^{-1}) = e \\] <p>And as there's only one inverse element, this completes the proof.</p>"},{"location":"group_theory.html#isomorphisms","title":"isomorphisms","text":"<p>You know the thing I said about how homomorphisms sometimes mapping to smaller groups? Well, I got the terminology wrong again, and ChatGPT got the terminology wrong. If the homomorphism maps from one group to another group where they're the same size, then it's called an isomorphism.</p> <p>By the way, if there's an isomorphism \\(\\varphi\\) from a group \\(G\\) to a group \\(H\\), then there's an isomorphism \\(\\varphi^{-1}\\) from \\(H\\) to \\(G\\)</p> <p>He also said here that it was crazy that if you use multiplication for the modular groups instead of addition, then \\(\u2124^{\\times}_p = \u2124_{p - 1}\\), and I already knew this fact because one: computer, two: modular arithmetic \\(10\\) months ago.</p> <p>He also said here that there is no pattern, but it's literally just an exponential function.</p>"},{"location":"group_theory.html#automorphisms","title":"automorphisms","text":"<p>An automorphism is an isomorphism from a group to itself, and I remember calling this one a self-homomorphism.</p> <p>Here's an example: if you swap any of the non-identity elements in \\(\\text{V}_4\\), it's still the same.</p> <p>Also, right now we stumble upon a sort of meta group theory (which to me is more similar to category theory): if you have a given group, then all of its automorphisms form a group with the operation of composition.</p> <p>Now that I'm done with pretty much the entire page, I can finally copy over work that I did on the brainstorm page that never made it into the story.</p>"},{"location":"group_theory.html#misc","title":"misc","text":"<p>Because the story of the pages is already complete and you should already understand the concepts, this is where I'll put my group theory work from now on.</p>"},{"location":"group_theory.html#finite-groups","title":"finite groups","text":"<p>When I talk about all of these group categories, it really makes you think: what are all of the groups? (Or at least finite groups.) I will answer this question with at least the first few groups (ranked by size), and once I reach one of my favorites, I'll talk about it.</p> <p>At every step of the way. there is of course the cyclic group of that size, which is when you can repeatedly \u2218 by one term to go all the way around.</p> <p>These are kind of boring (at least to me).</p> <p>By the definition of a set (of which a group is a type of set), the amount of elements within it have to be a positive integer (including zero).</p> <p>So, let's start with...</p>"},{"location":"group_theory.html#groups-of-size-0","title":"groups of size \\(0\\)","text":"<p>As the set not having any elements doesn't have an identity element, this breaks the identity rule.</p> <p>By the way, the empty group is closed as not having elements means it's impossible to \u2218 two of them to get outside the group.</p> <p>It is associative, and I'll prove that to you by going through all of the possibilities:</p> <p>with all the elements, we need every element to have an inverse. But as there are no elements, we don't need to add extra inverses.</p> <p>But anyways, every group of size zero fumbles at the identity rule. Time for...</p> <p>\\(500\\) Lines.</p>"},{"location":"group_theory.html#groups-of-size-1","title":"groups of size \\(1\\)","text":"<p>Of course, because of the identity rule, the one element has to be the identity element, and this is a group as:</p> <p>\\(1\\). It is closed, as \\(e\\) \u2218 \\(e = e\\)</p> <p>\\(2\\). It is associative, and I'll prove that to you by going through all of the possibilities: \\((e\\) \u2218 \\(e)\\) \u2218 \\(e = e\\) \u2218 \\(e = e\\). But also: \\(e\\) \u2218 \\((e\\) \u2218 \\(e) = e\\) \u2218 \\(e = e\\). Because they're both equal to the identity, they themselves must be equal, and because we have comprehensively gone through all possibilities, this group must have an associative operation.</p> <p>\\(3\\). it obviously has an identity element</p> <p>\\(4\\) inverses. I'll go through each term along with its inverse in the following table:</p> element inverse \\(e\\) \\(e\\) <p>And yeah, that's kind of it, the group containing the identity is the only \\(1\\) element group, and all other \\(1\\) element groups are just isomorphic to it, with their own identity element and own operation.</p> <p>Time for...</p>"},{"location":"group_theory.html#groups-of-size-2","title":"groups of size \\(2\\)","text":"<p>As we have proved in the Lagrange's theorem subchapter, every group whose size is a prime number must be isomorphic to each other and to the corresponding cyclic group. Thus, the only group of size \\(2\\) is \\(\u2124_2\\), and as this is a cyclic group, I'll say that it's already proved that it is a group.</p> <p>I didn't really feel like doing this argument for the last sub-subchapter.</p> <p>Everything I liked about this group was discussed in the parity sub-subchapter.</p> <p>And yeah, that's kind of it, \\(\u2124_2\\) is the only \\(2\\) element group, and all other \\(2\\) element groups are just isomorphic to it, with their own operation.</p> <p>Time for...</p>"},{"location":"group_theory.html#groups-of-size-3","title":"groups of size \\(3\\)","text":"<p>By the same logic, there can only be one \\(3\\) element group.</p> <p>I never really thought about this group too hard.</p> <p>And yeah, I'm kind of tired of repeatedly copying and pasting the same thing. But \\(\u2124_3\\) is the only \\(3\\) element group.</p> <p>Time for...</p>"},{"location":"group_theory.html#groups-of-size-4","title":"groups of size \\(4\\)","text":"<p>What I would like to prove to you is that the only \\(4\\) element groups are \\(\u2124_4\\) and \\(\\text{V}_4\\).</p> <p>By the way, I thought of (almost) this whole proof last night.</p> <p>Let's say you pick an element \\(a\\) and you keep \u2218ing by it until you form a subgroup. By Lagrange's theorem, the size of the subgroup is either \\(1\\), \\(2\\) or \\(4\\).</p> <p>If it's \\(1\\), you clearly just picked the identity element. If it's \\(4\\), it's just the cyclic group, and the only other possibility is that it's \\(2\\). If it is \\(2\\), then \\(a^2\\) must be \\(e\\).</p> <p>Let's do the same with another term \\(b\\). Once again, if the size is \\(1\\), you clearly just picked the identity, and if the size is \\(4\\), it's clearly just the cyclic group. (And this is the only part that I didn't figure out last night.) But this is still consistent as \\(b^2\\) must equal \\(a\\), then that term squared must have been the identity.</p> <p>Anyways, the point is that \\(a^2\\) must be \\(e\\) if the group isn't just the cyclic group.</p> <p>Let's pick the final element \\(c\\). You know the drill by now, so I won't go over it again.</p> <p>But what's worth noting is that  this subgroup can't be of size \\(4\\), because if it was, then it would just be the cyclic group, but it can't be the cyclic group as the cyclic group only has one non-identity term that squares to the identity.</p> <p>Anyways, we now know that any cyclic group with non-identity elements \\(a\\), \\(b\\), and \\(c\\) has to have the following equality:</p> \\[ a^2 = b^2 = c^2 = e \\] <p>The proof is almost complete, I just have to ask the question: what is \\(a\\) \u2218 \\(b\\)? It can't be the identity because of the following, it can't be \\(a\\) because of the following after that, it can't be \\(b\\) because of the one after that, so the only possibility is that it is \\(c\\). The same argument works even when you swap out \\(a\\) and \\(b\\) for something else.</p> \\[ a \u2218 b = e \\] \\[ b \u2218 b = e \\] \\[ a \u2218 b = b \u2218 b \\] \\[ a \u2218 b \u2218 b^{-1} = b \u2218 b \u2218 b^{-1} \\] \\[ a \u2218 e = b \u2218 e \\] \\[ a = b \\] <p>.</p> \\[ a \u2218 b = a \\] \\[ a \u2218 e = a \\] \\[ a \u2218 b = a \u2218 e \\] \\[ a^{-1} \u2218 a \u2218 b = a^{-1} \u2218 a \u2218 e \\] \\[ e \u2218 b = e \u2218 e \\] \\[ b = e \\] <p>.</p> \\[ a \u2218 b = b \\] \\[ e \u2218 b = b \\] <p>\\(600\\) Lines.</p> \\[ a \u2218 b = e \u2218 b \\] \\[ a \u2218 b \u2218 b^{-1} = e \u2218 b \u2218 b^{-1} \\] \\[ a \u2218 e = e \u2218 e \\] \\[ a = e \\] <p>.</p> <p>Note: you can swap \\(a\\) and \\(b\\) in this argument, meaning that the operation is also commutative.</p> <p>Now, let's look at the evidence we have for what this non-cyclic group is: it's a \\(4\\) element group where every term squared is the identity, it has a commutative operation, and \u2218ing two non-identity terms results in the third.</p> <p>The only group that fits this description is \\(\\text{V}_4\\).</p> <p>QED!</p> <p>Because \\(\\text{V}_4\\) is my favorite group (and because this is the best time to talk about it), I would like to tell you why I gave it the nickname \"the \\(2x2\\) sudoku group\":</p> <p>One day many years ago I played a game of \\(2x2\\) sudoku with a completely blank board. I went through each term within a square, then throughout all four of the squares, always picking the smallest number that would still follow the rules. You quickly run into a contradiction with a board of size \\(3\\), but when I did it with a board of size \\(2\\), I always got the same pattern. Within any square, there was a repeating pattern, and it was the same for each row and each column. The pattern was \\(1\\) \\(2\\) \\(3\\) \\(4\\), \\(3\\) \\(4\\) \\(1\\) \\(2\\), \\(2\\) \\(1\\) \\(4\\) \\(3\\), and \\(4\\) \\(3\\) \\(2\\) \\(1\\). Time went by, and then I eventually asked \"do all of these swaps for my group?\" The answer is yes, and that is the story of why I always called it \"the \\(2x2\\) sudoku group\"</p> <p>By the way, \\(\\text{V}_4\\) is the only group that uses the letter \\(\\text{V}\\) because it's just that special.</p> <p>Also, \\(\\text{V}_4\\) is the first composite group. This is an actual definition that I just guessed one day well in a bath: \\(G \\times H\\) is a group where the elements of it are ordered pairs of things from \\(G\\) and \\(H\\). The operation is to just do it with each individual term.</p> <p>I just thought it would be cool for there to be a product of two groups, and then ChatGPT gave me a definition.</p> <p>If you're curious, \\(\\text{V}_4\\) is equal to \\(\u2124_2 \\times \u2124_2\\), kind of like how \\(4\\) is the first composite number because it's equal to \\(2 \\times 2\\)</p> <p>Time for...</p>"},{"location":"group_theory.html#groups-of-size-5","title":"groups of size \\(5\\)","text":"<p>By the same logic I used before, there can only be one \\(5\\) element group.</p> <p>And I'll stop here. Here's my table:</p> group size groups with that size \\(0\\) \\(1\\) \\(\\{ e \\}\\) \\(2\\) \\(\u2124_2\\) \\(3\\) \\(\u2124_3\\) \\(4\\) \\(\u2124_4\\), \\(\\text{V}_4\\) \\(5\\) \\(\u2124_5\\)"},{"location":"group_theory.html#misc_1","title":"misc","text":"<p>Also, because now is the best time to talk about it, I made a \u2218 table, but instead of symbols, I used colors. I first had this idea while I was trying to visualize the group \\(\\text{S}_4\\), but I made some in MS paint. One for the trivial group, one for \\(\\text{D}_3\\), one for \\(\u2124_2\\), and one for \\(\\text{V}_4\\).</p> <p>What I realized was that I saw the same pattern again in all of the rows: in the right order, then swap the first two along with the second two, then you swap the first two with the second two, then you put everything in reverse. The idea of the swaps being isomorphic to the original group does kind of make sense as these swaps encapsulate the idea of \u2218ing on the left by your thing.</p> <p>So of course \u2218ing on the left by \\(b\\), then by \\(a\\) is just \u2218ing on the left by their product. Thus, these swaps are isomorphic to the original.</p> <p>I also plan to make a T-shirt about group theory as I did for Modular Arithmeic and The Lambda Calculus, and this is just the thing for that!</p> <p>If you were wondering, here's the diagrams:</p> <p></p> <p></p> <p></p> <p>On the left column is the first term with each color corresponding to a term. On the top is the second term, and if you look at The corresponding pixel color to that row and that column, that can tell you what the product is.</p> <p>By the way, the background color is black.</p> <p>It's the first idea that came to mind when I thought about how to display the underlying structure of a group that would be the same for all isomorphic groups.</p> <p>Also, this is the first time I've imported an image into my website.</p>"},{"location":"group_theory.html#z_2-times-z_n","title":"\\(\u2124_2 \\times \u2124_n\\)","text":"<p>But being able to multiply modular groups does beg the question: what is \\(\u2124_2 \\times \u2124_n\\)? Well, it's the group of ordered lists of either \\(0\\) or \\(1\\) in the first position, and any number \\(0\\) to \\(n - 1\\) in the second. The group has operations you can probably guess.</p> <p>Note: the part where I was talking about \\(\\text{V}_4\\) earlier and the text here were originally in the same area, so that's why I would ask this question.</p> <p>So, what is, for example, the group \\(\u2124_2 \\times \u2124_3\\)? Last night I made a full \u2218 table, but then I just asked ChatGPT, and the answer was surprisingly \\(\u2124_6\\)*. I was first thinking why this is the case, but as you can see below, they are perfectly isomorphic.</p> <p>Also, because the operation in the modular groups is similar to addition, I'm going to replace the multiplicationy notation with more additiony notation.</p> \\[ 0(1, 1) = (0, 0) \\] \\[ 1(1, 1) = (1, 1) \\] \\[ 2(1, 1) = (1, 1) + (1, 1) = (1 + 1, 1 + 1) = (0, 2) \\] \\[ 3(1, 1) = 2(1, 1) + (1, 1) = (0, 2) + (1, 1) = (0 + 1, 2 + 1) = (1, 0) \\] \\[ 4(1, 1) = 3(1, 1) + (1, 1) = (1, 0) + (1, 1) = (1 + 1, 0 + 1) = (0, 1) \\] \\[ 5(1, 1) = 4(1, 1) + (1, 1) = (0, 1) + (1, 1) = (0 + 1, 1 + 1) = (1, 2) \\] \\[ 6(1, 1) = 5(1, 1) + (1, 1) = (1, 2) + (1, 1) = (1 + 1, 2 + 1) = (0, 0) \\] <p>\\(700\\) Lines. This page is officially long.</p> \\[ (0, 0) = 0(1, 1) \\] \\[ (0, 1) = 4(1, 1) \\] \\[ (0, 2) = 2(1, 1) \\] \\[ (1, 0) = 3(1, 1) \\] \\[ (1, 1) = 1(1, 1) \\] \\[ (1, 2) = 5(1, 1) \\] \\[ \\varphi (n(1, 1)) = n \\] <p>This same method of proof works as long as \\(n\\) is an odd number as you can see below. But if \\(n\\) is an even number, then I guess there's really no way to categorize them, other than that they are a composite group. Personally, I would denote them with the letter \\(\\text{V}\\).</p> \\[ 0(1, 1) = (0, 0) \\] \\[ 1(1, 1) = (1, 1) \\] \\[ 2(1, 1) = (1, 1) + (1, 1) = (1 + 1, 1 + 1) = (0, 2) \\] \\[ 3(1, 1) = 2(1, 1) + (1, 1) = (0, 2) + (1, 1) = (0 + 1, 2 + 1) = (1, 2) \\] \\[ \\vdots \\] \\[ n(1, 1) = (n - 1)(1, 1) + (1, 1) = (0, n - 1) + (1, 1) = (0 + 1, (n - 1) + 1) = (1, 0) \\] \\[ (n + 1)(1, 1) = n(1, 1) + (1, 1) = (1, 0) + (1, 1) = (1 + 1, 0 + 1) = (0, 1) \\] \\[ (n + 2)(1, 1) = (n + 1)(1, 1) + (1, 1) = (0, 1) + (1, 1) = (1 + 1, 1 + 1) = (1, 2) \\] \\[ (n + 3)(1, 1) = (n + 2)(1, 1) + (1, 1) = (1, 2) + (1, 1) = (1 + 1, 2 + 1) = (0, 3) \\] \\[ \\vdots \\] \\[ 2n(1, 1) = (2n - 1)(1, 1) + (1, 1) = (1, 2n - 1) + (1, 1) = (1 + 1, (n - 1) + 1) = (0, 0) \\] \\[ (x, y) = \\begin{Bmatrix} x + y = 2k: y(1, 1) \\\\ x + y = 2k + 1: (y + n)(1, 1) \\\\ \\end{Bmatrix} \\] \\[ \\varphi (n(1, 1)) = n \\]"},{"location":"group_theory.html#finite-groups-part-2","title":"finite groups part \\(2\\)","text":"<p>Also, I asked ChatGPT to make a list of all of the groups of size \\(10\\) and under, and it delivered.</p> <p>Also, now I can finally answer that asterisk:</p> <ul> <li>This ChatGPT conversation was the one where it said that the answer was \\(\u2124_6\\), except what it said was that the only groups of size \\(6\\) are \\(\u2124_6\\) and \\(\\text{S}_3\\) / \\(\\text{D}_3\\). But in \\(\u2124_6\\), \\(0\\), \\(2\\), and \\(4\\) cubed- times \\(3\\) results in the identity. But also, in \\(\u2124_2 \\times \u2124_3\\), \\((0, 0)\\), \\((0, 1)\\), and \\((0, 2)\\) all cube to the identity. But also, \\(0\\) and \\(2\\) all square- times \\(2\\) is the identity, while \\((0, 0)\\) and \\((1, 0)\\) do so as well. Also, in \\(\\text{D}_3\\), there are three things that square to the identity, while in \\(\u2124_2 \\times \u2124_3\\), there are only two. From all this, I concluded that \\(\u2124_2 \\times \u2124_3 \\cong \u2124_6\\)*</li> </ul> <p>*I actually only did the \\(3(0) = 3(2) = 3(4) = 0\\) / \\(3(0, 0) = 3(0, 1) = 3(0, 2) = (0, 0)\\) and \\(2(0) = 2(3) = 0\\) / \\(2(0, 0) = 2(1, 0) = (0, 0)\\) tests.</p> <p>Also, you're probably wondering about the ChatGPT thing, and here's the table!:</p> group size groups with that size \\(0\\) \\(1\\) \\(\\{ e \\}\\) \\(2\\) \\(\u2124_2\\) \\(3\\) \\(\u2124_3\\) \\(4\\) \\(\u2124_4\\), \\(\u2124_2 \\times \u2124_2\\) \\(5\\) \\(\u2124_5\\) \\(6\\) \\(\u2124_6\\), \\(\\text{S}_3\\) \\(7\\) \\(\u2124_7\\) \\(8\\) \\(\u2124_8\\), \\(\u2124_4 \\times \u2124_2\\), \\(\u2124_2 \\times \u2124_2 \\times \u2124_2\\), \\(\\text{D}_4\\), \\(\\text{Q}_8\\) \\(9\\) \\(\u2124_9\\), \\(\u2124_3 \\times \u2124_3\\) \\(10\\) \\(\u2124_{10}\\), \\(\\text{D}_5\\) <p>Now, I can see why people don't call \\(\u2124_2 \\times \u2124_n\\) by the shorter \\(\\text{V}_{2n}\\)</p> <p>Also, you're probably wondering the same thing I was wondering at the time: what is \\(\\text{Q}_8\\)? Answer: it's a group of eight quaternions being \\(1\\), \\(-1\\), \\(i\\), \\(-i\\), \\(j\\), \\(-j\\), \\(k\\), and \\(-k\\) with the operation of multiplication.</p> <p>I remember thinking one day at the playground \\(1.5\\) or \\(2.5\\) years ago that these eight are closed under multiplication, but I never really thought they would be a group. I even made a times table!</p> <p>\\(777\\) Lines.</p> <p>Also, here's the full \u2218 table:</p> \\[ \\begin{matrix} \u2218 &amp; 1 &amp; -1 &amp; i &amp; -i &amp; j &amp; -j &amp; k &amp; -k \\\\ 1 &amp; 1 &amp; -1 &amp; i &amp; -i &amp; j &amp; -j &amp; k &amp; -k \\\\ -1 &amp; -1 &amp; 1 &amp; -i &amp; i &amp; -j &amp; j &amp; -k &amp; k \\\\ i &amp; i &amp; -i &amp; -1 &amp; 1 &amp; k &amp; -k &amp; -j &amp; j \\\\ -i &amp; -i &amp; i &amp; 1 &amp; -1 &amp; -k &amp; k &amp; j &amp; -j \\\\ j &amp; j &amp; -j &amp; -k &amp; k &amp; -1 &amp; 1 &amp; i &amp; -i \\\\ -j &amp; -j &amp; j &amp; k &amp; -k &amp; 1 &amp; -1 &amp; -i &amp; i \\\\ k &amp; k &amp; -k &amp; j &amp; -j &amp; -i &amp; i &amp; -1 &amp; 1 \\\\ -k &amp; -k &amp; k &amp; -j &amp; j &amp; i &amp; -i &amp; 1 &amp; -1 \\\\ \\end{matrix} \\] <p>Here's an analogy for the group \\(\u2124_2\\):</p> ...my friend is... ...my enemy is... The friend of... ...my friend ...my enemy The enemy of... ...my enemy ...my friend"},{"location":"group_theory.html#something-cool-about-the-first-few-groups","title":"something cool about the first few groups","text":"<p>\\(800\\) Lines.</p> <p>How does subchapter will work is I go over every group that is isomorphic to the corresponding group of the chapter, and I give each element a name, and figure out how that group should work. This should give me what I wanted from group theory when I started studying it.</p> <p>Also, when I refer to \"This group\", what I really mean is \"This group considered up to isomorphism\".</p> <p>Also, \\(|a|\\) means \"The smallest non-zero number \\(n\\) where \\(a^n = e\\)\".</p> <p>Also, \\(x\\) and \\(y\\) are in \\(G\\).</p>"},{"location":"group_theory.html#g-cong-e","title":"\\(G \\cong \\{ e \\}\\)","text":"<p>Boring!</p> <p>Next!</p>"},{"location":"group_theory.html#g-cong-z_2","title":"\\(G \\cong \u2124_2\\)","text":"<p>I feel like I said all that needs to be said about this group in the start of the parity sub-subchapter and a couple lines ago. But I feel like it captures the idea of a negative times a negative is a positive, odd plus odd is an even, and so on.</p> <p>Next!</p>"},{"location":"group_theory.html#g-cong-z_3","title":"\\(G \\cong \u2124_3\\)","text":"<p>Last time I went over this, I kind of just brushed it off as \"I never really thought about this group too hard\", but I feel like that isn't what this group deserves.</p> <p>Anyways, here's the part I've been waiting to put on this website for \\(5\\) days!</p> <p>As \\(G\\) is the only group of order \\(3\\), it has identity element \\(e\\), as well as two non-identity elements \\(a\\) and \\(b\\), and here are their properties:</p> \\[ x \u2218 y = y \u2218 x \\] \\[ x \u2218 e = x \\] \\[ a^2 = b \\] \\[ b^2 = a \\] \\[ a \u2218 b = b \u2218 a = e \\] <p>And here's those plus some additional ones that you can derive from them.</p> \\[ x \u2218 y = y \u2218 x \\] \\[ x \u2218 e = x \\] \\[ |a| = 3 \\] \\[ |b| = 3 \\] \\[ a^2 = b \\] \\[ b^2 = a \\] \\[ a \u2218 b = e \\] <p>For all these reasons, I don't know if the elements of \\(\u2124_2\\) or \\(a\\) and \\(b\\) are more like yin and yang.</p> <p>Next!</p>"},{"location":"group_theory.html#g-cong-z_4","title":"\\(G \\cong \u2124_4\\)","text":"<p>I've been waiting to do this one for \\(4\\) days! (I added the previous chapter \\(4\\) days ago.)</p> <p>\\(G\\) has identity element \\(e\\), as well as two non-identity elements \\(a\\), \\(b\\), and \\(c\\), and here are their properties:</p> \\[ x \u2218 y = y \u2218 x \\] \\[ x \u2218 e = x \\] \\[ a^2 = c \\] \\[ c \u2218 a = b \\] \\[ b \u2218 a = e \\] <p>And here's those plus some additional ones that you can derive from them.</p> \\[ x \u2218 y = y \u2218 x \\] \\[ x \u2218 e = x \\] \\[ |a| = 4 \\] \\[ |b| = 4 \\] \\[ c^2 = e \\] \\[ a^2 = c \\] \\[ b^2 = c \\] \\[ a \u2218 c = b \\] \\[ b \u2218 c = a \\] \\[ a \u2218 b = e \\] <p>If you were wondering how I came up with these, replace the \u2218 with times, \\(e\\) with \\(1\\), \\(a\\) with \\(i\\), \\(c\\) with \\(-1\\), and \\(b\\) with \\(-i\\).</p> <p>\\(900\\) Lines.</p> <p>But if it makes more sense, replace the \u2218 with doing one after the other, \\(e\\) with a \\(0\\)\u00b0 rotation, \\(a\\) with a \\(90\\)\u00b0 clockwise rotation, \\(c\\) with a \\(180\\)\u00b0 rotation, and \\(b\\) with a \\(90\\)\u00b0 counterclockwise rotation.</p>"},{"location":"group_theory.html#group-theory-card-game","title":"group theory card game","text":"<p>I came up with the rules of this game in about \\(10\\) minutes and it has not been playtested or even played before. You have been warned.</p> <p>First, you pick a deck. Each deck corresponds to a different group and the cards represent the elements of the group. There may or may not be duplicate cards depending on how big or small the group is.</p> <p>Then you deal one of each card out, get out an object, and place it on the identity element.</p> <p>If there's only one of each card in the deck, then get out another deck for doing this.</p> <p>The card that this object sits on should be viewed by every player.</p> <p>Then you deal \\(5\\) cards to each player, deal a card into what will soon become the pile, and move the object from the identity element to that card.</p> <p>Note: you can choose to not do this and just deal \\(5\\) cards to each player and start playing from here with the object still on the identity.</p> <p>Then you choose someone to go first. That player plays a card from their hand onto the pile, then they take the card that the object is currently sitting on and \u2218s it by the card that was dealt, and of course recards.</p> <p>Play continues clockwise until the marker goes back on the identity, then the person who dealt the last card gets the pile.</p> <p>Then you do the same process again, but that person who got the pile plays the first card.</p> <p>Play continues until you run out of cards in the deck, then the current pile goes to no one and the game ends.</p> <p>Then the player with the most cards across all of the decks wins.</p>"},{"location":"group_theory.html#my-assignment-this-week-well-two-weeks","title":"my assignment this week (well, two weeks)","text":"<p>It all starts with having a finite group \\(G\\) and it's center \\(C\\), but often we'll just take \\(C\\) to be an Abelian group.</p> <p>The assignment is, given that the size of \\(C\\) is divisible by prime number \\(p\\), to prove that there's an element of \\(C\\) of order \\(p\\).</p> <p>He already partially completed this problem, and I'll work through that.</p> <p>He said: just pick an element at random. Call it \\(x\\).</p> <p>Maybe you get lucky and the order of \\(x\\) is \\(p\\).</p> <p>Maybe you'll get kind of lucky and the order of \\(x\\) is at least divisible by \\(p\\).</p> <p>Or maybe you'll get unlucky and the order of \\(x\\) isn't even divisible by \\(p\\).</p> <p>In the first case, the problem is solved. In the second, for reasons I do not fully understand, the problem is also solved. But the third, there's something very interesting you can do.</p> <p>Let's call the subgroup of all powers of \\(x\\) by the name \\(H\\).</p> <p>Then the size (or commonly called the order) of \\(H\\) is the order of \\(x\\) is not divisible by \\(p\\). It was at this point that I realized why the two senses of the word are used.</p> <p>Here's some terminology that I'm going to have to define at some point, so I'll define it here. \\(G / H\\) is pronounced \"\\(G\\) mod \\(H\\)\" and means \"the set of all cosets of H\".</p> <p>You might think that that's just \\(G\\) because there's \\(1\\) for each element. But no, there are only \\(2\\) cosets of the even numbers for example (the evens and the odds).</p> <p>This notation is valid only for normal subgroups. I know what you're thinking: what is a normal subgroup? Well, normal subgroups are subgroups where the set of all things of form \\(ghg^{-1}\\) for \\(g\\) in \\(G\\) and \\(h\\) in \\(H\\) is \\(H\\) itself, just maybe with its elements shuffled.</p> <p>Notice how, three paragraphs ago, I said \"the set of all cosets of \\(H\\)\". Because, as you should figure out for yourself, it's the same if you do it from the left or the right.</p> <p>Also, fun fact! \\(\u2124\\) mod all of the integers divisible by \\(n\\) is actually equal to \\(\u2124_n\\). Or written more formally, it looks like this:</p> \\[ \u2124_n : = \u2124 / \u27e8n\u27e9 \\] <p>Also, taking the coset formed by an element of the subgroup itself doesn't change it. That also means that \u2218ing the thing you're taking a coset with by an element of the subgroup, it doesn't change the coset.</p> <p>A consequence of this is that the size of \\(G / H\\) is the size of \\(G\\) divided by the size of \\(H\\).</p> <p>Also, if the group is Abelian, then every subgroup is normal.</p> <p>I can't believe I haven't said this before so I'll say it now. The size of a group is denoted with absolute value signs.</p> <p>With all of this in mind, realize that the size of \\(C / H\\) times the size of \\(H\\) is equal to the size of \\(C\\) (\\(|C / H| \\cdot |H| = |C|\\)). But remember, at the start I said that the size of \\(C\\) is divisible by \\(p\\), so if the factor of \\(p\\) definitely didn't come from \\(H\\), then it definitely came from \\(C / H\\)!</p> <p>You might not know why this is important (and neither do I), but this statement is really close to solving the problem, and for reasons that I do not fully understand, this means that \\(C / H\\) has an element of order \\(p\\). Now the only thing left for me to prove this week is that if \\(C / H\\) has an element of order \\(p\\), then so does \\(C\\).</p> <p>I solved the problem around \\(20\\) minutes after I wrote that down.</p> <p>What happened was I analyzed the coset itself. That is, the one of order \\(p\\).</p> <p>I'll just take a random element of this coset and call it \\(c\\). By definition, if you \u2218 each element of \\(H\\) by \\(c\\) a total of \\(p\\) times (that is, once by \\(c^p\\)), then you just shuffle around the elements of \\(H\\).</p> <p>But realize: that shuffling action can be described perfectly with an element of \\(\\text{S}_{|H|}\\). That is, a permutation of \\(H\\)'s elements. Call it \\(\\sigma\\).</p> <p>But because \\(\\sigma\\) is a permutation of a finite number of things, it by definition has an order, and you can keep doing it \\(|\\sigma|\\) times to get back where you started.</p> <p>But this means that if you \u2218 an element of \\(H\\) by \\(c^p\\) a total of \\(|\\sigma|\\) times (that is, once by \\({(c^p)}^{|\\sigma|}\\)) then you get back to where you started, which means you \u2218ed by the identity. Thus, \\({(c^p)}^{|\\sigma|}\\) is the identity.</p> <p>But by using the properties of exponents, we realize that \\({(c^p)}^{|\\sigma|} = c^{p |\\sigma|} = c^{|\\sigma| p} = {(c^{|\\sigma|})}^p\\), so \\({(c^{|\\sigma|})}^p\\) is the identity.</p> <p>Because \\(p\\) by definition has no factors, while multiplying by \\(c^{|\\sigma|}\\) it can't have hit an earlier term and have that term as the order.</p> <p>Thus, \\(c^{|\\sigma|}\\) is an element of \\(C\\) of order \\(p\\).</p> <p>QED!</p> <p>Clarification: the thing I said two paragraphs ago would mean that \\({(c^{|\\sigma|})}^p\\) is the identity because \\(p\\) is divisible by that smaller exponent, i.e. you would just repeatedly hit the identity on every multiple of said smaller exponent including \\(p\\).</p>"},{"location":"harmonic.html","title":"Harmonic","text":""},{"location":"harmonic.html#harmonic","title":"Harmonic","text":"<p>i'm going to start with the alternating harmonic series \\(1 - \\frac{1}{2} + \\frac{1}{3} - \\frac{1}{4} + \\frac{1}{5}...\\)</p> <p>then, for example \\(- \\frac{1}{4} = \\frac{1}{4} - \\frac{1}{2}\\)</p> <p>then, the original series is equal to \\(1 + \\frac{1}{2} - 1 + \\frac{1}{3} + \\frac{1}{4} - \\frac{1}{2} ...\\)</p> <p>Also, I will call the total sum \"?\"</p> <p>now, let me define \\(f(n)\\) as \\(f(1) = 1\\) and \\(f(2) = 1 + \\frac{1}{2} - 1 + \\frac{1}{3} = \\frac{1}{2} + \\frac{1}{3}\\) than \\(f(3)\\) equals the next few terms, so \\(\\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5}\\)</p> <p>in general, \\(f(n)\\) = \\(\\frac{1}{n} + \\frac{1}{n + 1} + ...\\) with n terms OR \\(\\frac{1}{n} + \\frac{1}{n} + ... + \\frac{1}{2n - 1}\\)</p> <p>but than \\(f(\\infty) = ?\\)</p> <p>\\(f(n,x) = \\frac{1}{n} + \\frac{1}{n + 1} + ... + \\frac{1}{xn - 1}\\) \\(x\\) as \\(\\frac{\\text{integer}}{n}\\) for integer n</p> <p>\\(f(n,x + \\frac{1}{n}) - f(n,x) = \\frac{1}{xn}\\)</p> <p>\\(n(f(n,x + \\frac{1}{n}) - f(n,x)) = \\frac{1}{x}\\)</p> <p>as \\(n \\to \\infty\\) \\(\\frac{1}{n} \\to 0\\) and \\(x\\) could be anything AND the previous expression equals \\(\\frac{df(\\infty,x)}{dx}\\)</p> <p>but \\(ln \\prime(x) = \\frac{1}{x}\\) and \\(ln(1) = 0\\) AND \\(f(\\infty,1) = \\frac{1}{1 + \\infty} = 0\\)</p> <p>therefore \\(f(\\infty,x) = ln(x)\\)</p> <p>but \\(f(n,2) = f(n)\\) and \\(? = f(\\infty)\\)</p> <p>so, \\(? = f(\\infty,2) = ln(2)\\)</p> <p>so \\(1 - \\frac{1}{2} + \\frac{1}{3} - \\frac{1}{4} + \\frac{1}{5}\\) FOREVER \\(= ln(2)\\)</p> <p>yay!</p>"},{"location":"harmonic.html#regular-harmonic","title":"regular harmonic","text":"<p>Now I'll start with a series like \\(1 + \\frac{1}{2} + \\frac{1}{3} - 1 + \\frac{1}{4} + \\frac{1}{5} + \\frac{1}{6} - \\frac{1}{2}\\) is \\(f(\\infty, 3)\\), so ln(3)</p> <p>the simpler \\(1 + \\frac{1}{2} + \\frac{1}{3} - 1 - \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{5} + \\frac{1}{6} - \\frac{1}{3} - \\frac{1}{4}\\), if you don't get it, its 3 positive 2 negative.</p> <p>for the same reasons as before, this equals (remember that \\(n \\to \\infty\\) in the following) \\(\\frac{1}{2n} + \\frac{1}{2n + 1} + ... + \\frac{1}{3n - 1}\\), so \\((\\frac{1}{n} + \\frac{1}{n + 1} + ... + \\frac{1}{3n - 1}) - (\\frac{1}{n} + \\frac{1}{n + 1} + ... + \\frac{1}{2n - 1})\\), so \\(f(n, 3) - f(n, 2)\\), so \\(f(\\infty, 3) - f(\\infty, 2)\\), so \\(ln(3) - ln(2)\\), so \\(ln(\\frac{3}{2})\\), now I will define \\(g(m, n)\\) as m posataves, n negataves, so \\(g(m, n) = ln(\\frac{m}{n})\\)</p> <p>now, with the power of \\(g\\) on our side, we can solve for \\(1 + \\frac{1}{2} + \\frac{1}{3}...\\) (which by the way is the point of this chapter) as 1 positive, 0 negatives, so \\(ln(1) - ln(0)\\), but \\(ln(0)\\) is undefined, so time to define it!</p>"},{"location":"harmonic.html#ln0","title":"\\(ln(0)\\)","text":"<p>\\(ln(0) = u\\) (u for undefined)</p> <p>\\(e^u = 0\\)</p> <p>\\(\\frac{1}{e^{-u}} = 0\\)</p> <p>\\(e^{\\infty} = \\infty\\)</p> <p>\\(\\frac{1}{\\infty} = 0 \\Rightarrow u = - \\infty \\quad \\And \\quad ln(1) = 0 \\Rightarrow 1 + \\frac{1}{2} + \\frac{1}{3}... = \\infty\\)</p>"},{"location":"harmonic.html#frac1infty","title":"\\(\\frac{1}{\\infty}\\)","text":"<p>I'll just say it, I spent a long time trying to figure it out a way to prove that \\(1 + \\frac{1}{2} + \\frac{1}{3}... = \\infty\\), and I realized that \\(e^{-\\infty} = 0\\) and \\(\\frac{1}{\\infty} = 0\\) are true by convention, and the only proofs like \"if you divide \\(1\\) by \\(e\\) enough times, it eventually has to hit zero\" and \"\\(\\frac{1}{\\infty}\\) 'approaches ' zero\" are nor rigorous because the word \"approach\" proveably doesn't have rigorous definition, so you just have to trust the conclusion.</p>"},{"location":"harmonic.html#conclusion","title":"conclusion","text":"\\[ 1 - \\frac{1}{2} + \\frac{1}{3} - \\frac{1}{4} + \\frac{1}{5} ... = ln(2) \\] \\[ \\text{m positives, n negatives} = ln(\\frac{m}{n}) \\] \\[ 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5}... = \\infty \\]"},{"location":"how.html","title":"How","text":"<p>how did you even find this? Probably the hyperbolic page. Here's \\(10,000\\) q's</p> <p>qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq</p>"},{"location":"hyperbolic.html","title":"Hyperbolic","text":""},{"location":"hyperbolic.html#hyperbolic-arcs","title":"Hyperbolic Arcs","text":"<p>If you manged to scroll down, here's a chosse your own adventure: if you found this in the brainstorm page, go to page 1 either from that link, or in the dropdown menu, if you found this page by pure chance, go to page 2 (once again, either from that link, or in the dropdown menu)</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p>"},{"location":"hyperbolic.html#page-1","title":"page 1","text":"<p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p>"},{"location":"hyperbolic.html#page-2","title":"page 2","text":"<p>Here's an easter egg!</p>"},{"location":"jacobian.html","title":"Jacobian","text":""},{"location":"jacobian.html#linear-algebra","title":"linear algebra","text":"<p>learn more here, and here</p> \\[ \\begin{bmatrix}  a \\\\   b\\end{bmatrix} = a + bi \\] \\[ \\hat{I} = \\begin{bmatrix}  1 \\\\   0\\end{bmatrix} = 1 \\] \\[ \\hat{j} = \\begin{bmatrix}  0 \\\\   1\\end{bmatrix} = i \\] \\[ \\begin{bmatrix} a \\quad b \\\\ c \\quad d \\\\ \\end{bmatrix} \\begin{bmatrix}  e \\\\ f \\end{bmatrix} = \\begin{bmatrix}  ae + bf \\\\ ce + df\\end{bmatrix}\\] \\[ A = \\begin{bmatrix}  ? \\quad ? \\\\ ? \\quad ?\\end{bmatrix} \\] \\[ A \\hat{I} = \\begin{bmatrix}  a \\\\ c\\end{bmatrix} \\] \\[ A \\hat{j} = \\begin{bmatrix}  b \\\\ d\\end{bmatrix} \\] \\[ A = \\begin{bmatrix}  a \\quad b \\\\ c \\quad d\\end{bmatrix} \\] \\[ a + bi = \\begin{bmatrix}  ? \\quad ? \\\\ ? \\quad ?\\end{bmatrix} \\] \\[ (a + bi) \\hat{I} = (a + bi) 1 = a + bi = \\begin{bmatrix}  a \\\\ b\\end{bmatrix} \\] \\[ (a + bi) \\hat{j} = (a + bi) i = -b + ai = \\begin{bmatrix}  -b \\\\ a\\end{bmatrix} \\] \\[ a + bi = \\begin{bmatrix} a &amp; -b \\\\ b &amp; a\\end{bmatrix} \\]"},{"location":"jacobian.html#complex-functions","title":"complex functions","text":"\\[ z = x + yi =\\begin{bmatrix}  x \\\\ y\\end{bmatrix} \\] \\[ f(z) = u + vi = \\begin{bmatrix}  u \\\\ v\\end{bmatrix} \\] \\[ dz = dx + dyi = \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ df = du + dvi = \\begin{bmatrix}  du \\\\ dv\\end{bmatrix} \\] \\[ f\\prime (z) = \\frac{df}{dz} \\] \\[ df = f\\prime (z) dz \\] \\[ \\begin{bmatrix}  du \\\\ dv\\end{bmatrix} = f\\prime (z) \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ df = f(z + dz) - f(z) \\] \\[ \\frac{\\partial}{\\partial x} f(x, t) = : \\frac{f(x + dx, t) - f(x, t)}{dx} \\] \\[ \\partial f(x, t)_x = : f(x + dx, t) - f(x, t) \\] \\[ f(z + dx \\hat{I}) = f(z) + \\begin{bmatrix}  \\partial u_x \\\\ \\partial v_x\\end{bmatrix} \\] \\[ f(z + dy \\hat{j}) = f(z) + \\begin{bmatrix}  \\partial u_y \\\\ \\partial v_y\\end{bmatrix} \\] \\[ f(z + dx + dyi) = f(z + dz) = f(z) + \\begin{bmatrix}  \\partial u_x \\\\ \\partial v_x\\end{bmatrix} + \\begin{bmatrix}  \\partial u_y \\\\ \\partial v_y\\end{bmatrix} \\] \\[ f(z + dz) - f(z) = df = \\begin{bmatrix}  \\partial u_x \\\\ \\partial v_x\\end{bmatrix} + \\begin{bmatrix}  \\partial u_y \\\\ \\partial v_y\\end{bmatrix} = \\partial u_x \\hat{I} + \\partial v_x \\hat{j} + \\partial u_y \\hat{I} + \\partial v_y \\hat{j} = \\frac{\\partial u}{\\partial x} dx \\hat{I} + \\frac{\\partial v}{\\partial x} dx \\hat{j} + \\frac{\\partial u}{\\partial y} dy \\hat{I} + \\frac{\\partial v}{\\partial y} dy \\hat{j} = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} dx + \\frac{\\partial u}{\\partial y} dy  \\\\ \\frac{\\partial v}{\\partial x} dx + \\frac{\\partial v}{\\partial y} dy\\end{bmatrix} = \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ \\begin{bmatrix}  du \\\\ dv\\end{bmatrix} = \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ f\\prime (z) = \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\]"},{"location":"jacobian.html#clarification-and-cauchy-riemann-equations","title":"clarification and cauchy-riemann equations","text":"<p>technically a term like \\(\\partial u_y\\) seems like the change in \\(u\\) when \\(z\\) adds \\(dy\\), but actually it is the change in \\(u\\) when \\(z\\) moves up by \\(dyi\\), but this is just \\(x + yi + dyi\\), so \\(x + (y + dy)i\\) which means that it is a change in \\(u\\) when \\(y\\) increases</p> <p>in short, \\(\\partial u_y\\) is not the change in \\(u\\) when \\(z\\) adds \\(dy\\), but the change of \\(u\\) when you increase \\(y\\)</p> <p>on another note, the equation or jacobian matrix is a matrix, but as you know, every complex number has a corresponding matrix but not every matrix has a corresponding complex number, so to  find out if the jacobian matrix is a complex number or just a matrix, or said another way, if complex function \\(f(z)\\) has a derivative, we need the cauchy-riemann equations, lets go derive them!</p> <p>so, if the jacobian matrix is a complex number \\(a + bi\\) which I have been saving for something like this, than the corresponding matrix is:</p> \\[ \\begin{bmatrix} a &amp; -b \\\\ b &amp; a\\end{bmatrix} \\] <p>so that means that</p> \\[ \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} = \\begin{bmatrix} a &amp; -b \\\\ b &amp; a\\end{bmatrix} \\] <p>Well, what makes a matrix of that form?</p> <p>for one, the top left equals the bottom right equals the real part</p> <p>and for another, the top right equals the negative of the bottom left equals the negative of the imaginary part</p>"},{"location":"jacobian.html#in-conclusion","title":"in conclusion...","text":"\\[ f\\prime (z) = \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\] <p>and to test if this is a matrix or a complex number</p> \\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y} \\] \\[ -\\frac{\\partial u}{\\partial y} = \\frac{\\partial v}{\\partial x} \\]"},{"location":"jacobian.html#examples","title":"examples","text":"\\[ f(z) = z^2 \\] \\[ f(x +yi) = (x^2 - y^2) + (2xy)i \\] \\[ u = x^2 - y^2 \\] \\[ v = 2xy \\] \\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial}{\\partial x} (x^2 - y^2) = 2x \\] \\[ \\frac{\\partial u}{\\partial y} = \\frac{\\partial}{\\partial y} (x^2 - y^2) = -2y \\] \\[ \\frac{\\partial v}{\\partial x} = \\frac{\\partial}{\\partial x} (2xy) = 2y \\] \\[ \\frac{\\partial v}{\\partial y} = \\frac{\\partial}{\\partial y} (2xy) = 2x \\] \\[ 2x = 2x \\] \\[ 2y = 2y \\] \\[ f\\prime (z) = 2x + 2yi = 2z \\] <p>another one!</p> \\[ f(z) = \\text{ccong} (z) \\] \\[ f(x + yi) = x - yi \\] \\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial}{\\partial x} x = 1 \\] \\[ \\frac{\\partial u}{\\partial y} = \\frac{\\partial}{\\partial y} x = 0 \\] \\[ \\frac{\\partial v}{\\partial x} = \\frac{\\partial}{\\partial x} (-y) = 0 \\] \\[ \\frac{\\partial v}{\\partial y} = \\frac{\\partial}{\\partial y} (-y) = -1 \\] \\[ 1 \\ne -1 \\] \\[ 0 = 0 \\] \\[ f\\prime (z) = \\begin{bmatrix}  1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\] <p>In conclusion, \\(z^2\\) has a derivative, and \\(\\text{ccong} (z)\\) does not.</p>"},{"location":"lambda.html","title":"Lambda calculus","text":"<p>Ok, so, lambda calculus, I'll be following along with the first part, at least for now, and I haven't seen this in a year, so we'll be reacting to it together.</p>"},{"location":"lambda.html#chapter-1-introduction","title":"Chapter \\(1\\): Introduction","text":"<p>Ok, so, it looks like he's starting off with a function (written as \\(I\\)) that he calls \"the identity\". Now, what do you think \\(I(1)\\) is? * Man in the background says \" same value! \", and that is correct, \\(I(1) = 1\\), \\(I(2) = 2\\), what about \\(I(I)\\)? * No one answers, yeah, it's \\(I\\). (By the way, functions in lambda calculus are also called \"combinators\", each one has a function name (e.g. \\(I\\)), a more descriptive function name (e.g. identity), and an alias (which is always a bird for some reason) (e.g. Idiot bird.)) (Also by the way, I'll be copying what he's saying for the rest of this page, and putting my thoughts in between parentheses.) And then he uses some fancy notation \" \\(\\lambda a. a\\) \". It takes in (\\(\\lambda\\)) a value (\\(a\\)), and outputs (\\(.\\)) that same value (\\(a\\)). So, \\(I(x) = x\\) for any \\(x\\). As you saw, you can use functions as arguments (which is why they are called \"combinators\". Also, to me, this is the heart of lambda calculus), erbs are nouns and nouns are erbs. </p>"},{"location":"lambda.html#chapter-2-what-is-this-lambda","title":"chapter \\(2\\): What is this \\(\\lambda\\)?","text":"<p>Ok, so, what was this \\(\\lambda\\) stuff from earlier? (Finally he explains it.) So, lambda is a signafier, it's a way to indicate that we are starting the definition of a function. So, we can read this (this \\(\\lambda a. a\\)) as: we're starting the definition of a function (\\(\\lambda\\)), which takes a single input (\\(a\\)), or parameter (The \\(.\\) signifies that we are no longer writing down the parameters), and returns some expression (\\(a\\)). This whole thing is called a lambda abstraction in lambda calculus.</p>"},{"location":"lambda.html#chapter-3-what-is-lambda-calculus","title":"chapter \\(3\\): What is Lambda Calculus?","text":"<p>Lambda calculus is a really tiny symbol manipulation framework, a calculus is just a way of moving around symbols on a page (I actually didn't know that), the subject that you may have learnd in school called \"calculus\" is a specific calculus for things like differentials and integrals and stuff like that, derivatives. But this calculus is about something else, this calculus is about evaluating and defining functions.</p>"},{"location":"lambda.html#chapter-4-im-tired-of-this-heres-a-crash-course-on-lambda-calculus","title":"chapter \\(4\\): I'm tired of this, here's a crash course on lambda calculus.","text":"<p>Functions (aka combinators) act on other functions, each one has a single letter abbreviation, and an alias. For the \\(2\\) example functions, they are written \" \\(M\\) \" and \" \\(K\\) \", and called \"The Mockingbird\" and \"The Kestrel\". Now, how it's defined in lambda calculus is \" \\(M = : \\lambda a. aa\\) \" and \" \\(K = : \\lambda x. \\lambda y. x\\) \". Now, this is where it starts to get complicated (that's a family meme). First, there's a \\(\\lambda\\) (aka lambda), which means: defining inputs (functions). Then, it defines an input (\\(a\\)), then, there's a \\(.\\), which means: stop defining inputs. (Right here in the other one, there's another \\(\\lambda\\), so, define inputs again.) Then, there's an \\(aa\\) (which is actually \\(a(a)\\), it's just auto-parenthesized). The \\(a\\) being a function is explaind by when I said \"inputs (functions)\", but then, there's another question: why is \\(a\\) also an input? One or the other, right? Well, no, it is explaind by when I said \"Functions act on other functions\". Putting it all together, The Mockingbird function takes in one input, and evaluates it on itself, and The Kestrel takes in two inputs, and just does the first one.</p>"},{"location":"lambda.html#chapter-5-big-numbers-and-transfinite-ordinals","title":"chapter \\(5\\): Big numbers and Transfinite ordinals","text":"<p>This chapter doesn't have much lambda calculus, it's just what I have been doing for the past week. This, this, and this were my only evidence (in that order) (you should watch them anyways, they're really cool videos), I knew enough about the fast-growing hierarchies, but very little about the infinite ordinals.</p> <p>Also, that second video had a pretty cool premise of \"biggest textable number\" (the video went into more detail on that), so I'm gonna steal it.</p> \\[ f_0 (x) = : x + 1 \\] \\[ f_{n + 1} (x) = : f_n(f_n(f_n(...(f_n(x)))))... = f_n^x(x) \\] \\[ f_1 (x) = 2x \\] \\[ f_2 (x) = x \\cdot 2^x \\] \\[ f_3 (3) &gt; 10^{100} \\times \\text{The american national debt.} \\] \\[ f_\\omega (x) = : f_x (x) \\] \\[ \\text{Yes, that omega is a transfinite ordinal, but there's nothing infinite about it, it just grows faster than any finite number would (e.g. } f_{1000} (x) &lt; f_\\omega (x) \\text{ because for all } x &gt; 1000 \\text{, the subscript would be bigger, thus it would grow faster), it's just notational shorthand, because if the input is really long to write, you would have to write it twice. But there is a connection between transfinite ordinals and these functions (which have a connection to big numbers, hence the name: Big numbers and Transfinite ordinals) that I will explain after I get to the largest ordinal that I know.} \\] \\[ \\text{For ordinals, there are three rules, } 1 \\text{: you start with omega (} \\omega \\text{), } 2 \\text{: you can add } 1 \\text{, } 3 \\text{: you can repeat 'til infinity*} \\] \\[ \\text{*only an infinity that you have already reached} \\] \\[ \\omega \\] \\[ \\omega + 1 \\] \\[ \\omega + 2 \\] \\[ \\omega + 3 \\] \\[ \\vdots \\] \\[ \\omega + \\omega \\] \\[ 2 \\omega \\] \\[ 2 \\omega + 1 \\] \\[ 2 \\omega + 2 \\] \\[ 2 \\omega + 3 \\] \\[ \\vdots \\] \\[ 2 \\omega + \\omega \\] \\[ 3 \\omega \\] \\[ \\vdots \\] \\[ \\omega \\omega \\] \\[ \\omega^2 \\] \\[ \\omega^\\omega \\] \\[ \\omega^{\\omega^\\omega} \\] \\[ \\omega^{\\omega^{\\omega^{\\omega^{.^{.^.}}}}} = : \\epsilon_0 = \\text{}^\\omega \\omega \\] \\[ \\text{Where }^y x \\text{ is pronounced \"} x \\text{ tetrated to } y \\text{\" and equals } x^{x^{x^{x^{.^{.^.}}}}} y \\text{ times. (Also, it's parenthesized from the right to the left.)} \\] \\[ \\text{Also, } \\epsilon_0 = \\omega^{\\epsilon_0} \\text{, so } \\epsilon_0 \\text{ is a \"fixed point\" of } \\omega \\text{.} \\] \\[ \\epsilon_{n + 1} = : \\epsilon_n^{\\epsilon_n^{\\epsilon_n^{\\epsilon_n^{.^{.^.}}}}} \\] \\[ \\epsilon_\\omega \\] \\[ \\epsilon_{\\epsilon_0} \\] \\[ \\epsilon_{\\epsilon_{\\epsilon_{\\epsilon_\\ddots}}} = : \\zeta_0 \\] \\[ \\text{IMPORTANT! This is only } \\zeta_0 \\text{ because at the end of that tower* of epsilons, there is a } 0 \\text{.} \\] \\[ \\text{*The term \"tower\" is exclusive to exponents, the term for subscripts is \"dungeon\".} \\] \\[ \\zeta_n^{\\zeta_n^{\\zeta_n^{\\zeta_n^{.^{.^.}}}}} = : \\epsilon_{\\zeta_n + 1} \\] <p>\\(100\\) lines.</p> \\[ \\zeta_{n + 1} = : \\epsilon_{\\epsilon_{\\epsilon_{\\epsilon_\\ddots}}} \\text{ Where the final epsilon is an } \\epsilon_{\\zeta_n + 1} \\text{.} \\] \\[ \\zeta_\\omega \\] \\[ \\zeta_{\\epsilon_0} \\] \\[ \\zeta_{\\zeta_0} \\] \\[ \\zeta_{\\zeta_{\\zeta_{\\zeta_\\ddots}}} = : \\eta_0 = : \\varphi_3 (0) \\]"},{"location":"lambda.html#chapter-6-the-veblen-hierarchy","title":"chapter \\(6\\): THE VEBLEN HIERARCHY","text":"<p>First, if there's any confusion, the veblen hierarchy is this \\(\\varphi\\) thing.</p> \\[ \\varphi_0 (\\alpha) = : \\omega^\\alpha \\] \\[ \\varphi_1 (\\alpha) = : \\epsilon_\\alpha \\] \\[ \\varphi_2 (\\alpha) = : \\zeta_\\alpha \\] \\[ \\varphi_3 (\\alpha) = : \\eta_\\alpha \\] \\[ \\varphi_\\omega (0) = \\varphi_{\\varphi_0 (1)} (0) \\] \\[ \\varphi_{\\varphi_{\\varphi_0 (1)} (0)} (0) \\] <p>\\(2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2\\) Lines!</p> \\[ \\varphi_{\\varphi_{\\varphi_{\\varphi_\\ddots (0)} (0)} (0)} (0) = : \\Gamma_0 = \\varphi_{\\Gamma_0} (0) \\] \\[ \\text{Ok, here's the connection: Let's say that we want to turn an infinite ordinal (} \\epsilon_0 \\text{) into a function. First, the function will be } f_{\\epsilon_0} (x) \\text{, but for even more notational shorthand, it is written } \\epsilon_0 (x) \\text{. But here's how it's defined: step } 1 \\text{: write the ordinal in terms of omega (so }^\\omega \\omega \\text{), maybe easier said than done, step } 2 \\text{: replace every omega with an } x \\text{ (so, }^x x \\text{), step } 3 \\text{: make it a subscript for } f \\text{ and evaluate it on } x \\text{ (so } f_{^x x} (x) \\text{).} \\] <p>Now, you can go over to my repo to see how to do it in python.</p>"},{"location":"lambda.html#logic","title":"logic","text":"<p>Feel free to skip this part if you just want to see the big number in lambda calculus.</p> \\[ K = \\text{Kestrel} = \\lambda x. \\lambda y. x = : \\text{True} = \\text{T} \\] \\[ Ki = \\text{Kite} = \\lambda x. \\lambda y. y = : \\text{False} = \\text{F} \\] \\[ \\text{And} = : \\lambda pq. p(q(\\text{T}, \\text{F}), q(\\text{F}, \\text{F})) = \\lambda pq. p(q, \\text{F}) = \\lambda pq. p(q, p) \\] \\[ \\text{And} = \\lambda pq. pqp \\] \\[ \\text{Or} = : \\lambda pq. p(q(\\text{T}, \\text{T}), q(\\text{T}, F)) = \\lambda pq. p(\\text{T}, q) = \\lambda pq. p(p, q) \\] \\[ \\text{Or} = \\lambda pq. ppq \\] \\[ \\text{Not} = : \\lambda p. p(\\text{F}, \\text{T}) \\] \\[ \\text{Not} = \\lambda p. p \\text{F} \\text{T} \\]"},{"location":"lambda.html#chapter-7-currying-operations-numbers-arithmetic-operations-groupings-subtraction-is-0-inequalities-equalities-and-the-biggest-codable-number","title":"chapter \\(7\\): currying operations, numbers, arithmetic operations, groupings, subtraction, is \\(0\\), inequalities, equalities, and the biggest (codable) number","text":"\\[ \\text{By the way, everything else that I had learned (other than this page's main plot of the biggest (codable) number) while writing this (currying operations, arithmetic operations, groupings, and subtraction) (that I learned via actually watching through the videos) was going to be in a post credit scene, but they were actually required for the code (e.g. I needed an exponent function for the `for i in range(x - 1): result = x ** result` on lines $4$ - $5$ of the epsilon subscript function).} \\] \\[ \\text{First: currying. How I think of it is like this (this entire next line):} \\] \\[ \\text{A function can have as many (or as few) inputs as possible. If the function is, say, The Kestrel (} 2 \\text{ inputs), you can easily define the } 2 \\text{ input version, and the no input version would just always return itself. Three is harder, but doable, and everything bigger has the same idea (one of two ideas both referred to as currying), it requires the first half of currying: } f(a, b) \\text{ for single input function } f \\text{ is equal to } (f(a))(b) \\text{. But for } 1 \\text{ input, it is much harder. New function: addition (yes, number addition) (specifically the } 2 \\text{ input one). If you only give it } 1 \\text{ input (} 1 \\text{ for example), you get the add } 1 \\text{ function. So that's the second rule of currying!} \\] \\[ \\text{So basically, currying is when you give a function the incorrect amount of inputs, and it either waits for more, or evaluates the function on the next input.} \\] \\[ \\text{you can also combine these to get } f(a, b) = f(a)(b) \\text{ for } 2 \\text{ input function } f \\text{, making the comma redundant.} \\] \\[ \\text{Next: numbers (church numerals).} \\] \\[ n0 = \\lambda f. \\lambda x. x \\text{ } ( = \\text{False}) \\] \\[ n1 = \\lambda f. \\lambda x. f x \\] \\[ n2 = \\lambda f. \\lambda x. f(f x) \\] \\[ n3 = \\lambda f. \\lambda x. f(f(f x)) \\] \\[ \\text{So, } 1 \\text{ equals once, } 2 \\text{ equals twice, } 3 \\text{ equals thrice, } 4 \\text{ equals fourfold, and so on.} \\] \\[ \\text{Next: successor!} \\] \\[ \\text{Successor} (n2) = n3 \\] \\[ \\text{Successor} (f(f x)) = f(f(f x)) \\] \\[ \\text{Successor} (n) = f(n) \\] \\[ Succ = \\lambda nfx. f(n(f, x)) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Succ = \\lambda nfx. f(nfx) \\] \\[ \\text{Next: addition!} \\] \\[ \\text{Addition} (n2, n3) = n5 \\] \\[ \\text{Addition} (2, 3) = 2 + 1 + 1 + 1 = 3( + 1, 2) \\] <p>\\(200\\) Lines.</p> \\[ Add = \\lambda nk. n(Succ, k) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Add = \\lambda nk. n \\text{ } Succ \\text{ } k \\] \\[ \\text{Next: NOT subtraction, multiplication instead!} \\] \\[ \\text{because subtraction is so hard, it's going into its own subchapter.} \\] \\[ Mult \\text{ } n2 \\text{ } n3 = n6 \\] \\[ \\text{Before I expand that, I would like to say that } f f f f f f x \\text{ is interpreted as } f(f, f, f, f, f, x) \\text{, because of currying, so I'll say that } (f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f) x \\text{ is equal to } f(f(f(f(f(f x))))) \\text{.} \\] \\[ Mult \\text{ } n2 \\text{ } n3 \\text{ } f \\text{ } x = (f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f) (x) \\] \\[ \\text{By the way, that's a curried function on the left.} \\] \\[ \\text{But, composition is associative (Which is a word that he used } 6 \\text{ times) (a function like } + \\text{ is associative if and only if } a + (b + c) = (a + b) + c \\text{), so you can write it like this:} \\] \\[ Mult \\text{ } n2 \\text{ } n3 \\text{ } f \\text{ } x = ((f \u2218 f \u2218 f) \u2218 (f \u2218 f \u2218 f)) (x) \\] \\[ \\text{you can kinda replace the } f \u2218 f \u2218 f \\text{ with an } n3(f) \\text{, but be careful, it's a curried function, so it will wait for an input, and then do } (f \u2218 f \u2218 f) (x) \\text{. But the function is kinda already waiting for an input, so} \\] \\[ Mult \\text{ } n2 \\text{ } n3 \\text{ } f \\text{ } x = ((n3(f)) \u2218 (n3(f)) (x) \\] \\[ \\text{And we can use the same trick now to get} \\] \\[ Mult \\text{ } n2 \\text{ } n3 \\text{ } f \\text{ } x = n2(n3(f)) (x) \\] \\[ \\text{But, because it's a curried function on the left, the } x \\text{ on the right, and the fact that if } f(x) = g(x) \\text{, than } f = g \\text{, I can say that} \\] \\[ Mult \\text{ } n2 \\text{ } n3 \\text{ } f = n2(n3(f)) \\] \\[ Mult = \\lambda nkf. n(k(f)) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Mult = \\lambda nkf. n(kf) \\] \\[ \\text{Now, I'm just gonna say, multiplication is just function composition, represented with The Bluebird combinator.} \\] \\[ B = \\lambda fgx. f(gx) \\] \\[ Mult = B \\] \\[ \\text{Next: exponents!} \\] \\[ Pow \\text{ } n2 \\text{ } n4 = n16 \\] \\[ Pow \\text{ } n2 \\text{ } n4 \\text{ } f \\text{ } x = (f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f) (x) \\] \\[ Pow \\text{ } n2 \\text{ } n3 \\text{ } f = f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f \u2218 f = (((f \u2218 f) \u2218 (f \u2218 f)) \u2218 ((f \u2218 f) \u2218 (f \u2218 f))) \u2218 (((f \u2218 f) \u2218 (f \u2218 f)) \u2218 ((f \u2218 f) \u2218 (f \u2218 f))) = ((n2(f) \u2218 n2(f)) \u2218 (n2(f) \u2218 n2(f))) \u2218 ((n2(f) \u2218 n2(f)) \u2218 (n2(f) \u2218 n2(f))) = (n2(n2(f)) \u2218 n2(n2(f))) \u2218 (n2(n2(f)) \u2218 n2(n2(f))) = n2(n2(n2(f))) \u2218 n2(n2(n2(f))) = n2(n2(n2(n2(f))))) = n4(n2, f) = n4(n2)(f) \\] <p>\\(2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 2\\) Lines!</p> \\[ Pow \\text{ } n2 \\text{ } n4 = n4(n2) \\] \\[ Pow = \\lambda nk. k(n) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Pow = \\lambda nk. kn \\] \\[ \\text{Next: groupings!} \\] \\[ \\text{This subchapter is about the smallest data structure in lambda calculus, The Vireo (V), here's it's definition:} \\] \\[ \\text{V} = \\lambda abf. f(a, b) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ \\text{V} = \\lambda abf. fab \\] \\[ \\text{This function mainly works with the second rule of currying, you see, The Vireo can do this:} \\] \\[ \\text{V} (a, b) (f) = f(a, b) \\] \\[ \\text{but what happens to the V} (a, b) \\text{ before the } f \\text{? Solution: } a \\text{ and } b \\text{ are paired together.} \\] \\[ \\text{By the way, because it is used so much (and this just makes sense), V} (a, b) = (a, b) \\text{.} \\] \\[ \\text{Now, when you want to evaluate a regular function } f \\text{ on a pair } p \\text{, than that would be } p(f) \\text{, but if you wanted a function that evaluates more like } f(p) \\text{ than } p(f) \\text{, then you need a pairwise function (not to be confused with piecewise function, as it is the only result when you google \"pairwise function\", and piecewise functions don't actually exist in lambda calculus) (e.g. a function that inputs a pair, and outputs the first thing in that pair) actually, that second thing would be really useful, I'm gonna try to derive it (as well as a function that inputs a pair, and outputs the second thing in that pair).} \\] \\[ Fst((a, b)) = a \\] \\[ \\text{And because this is the first pairwise function that I'm gonna derive, it has to use regular functions in its definition, so} \\] \\[ Fst((a, b)) = (a, b)(f) = f(a, b) = a \\] \\[ \\text{Hmm... } f(a, b) = a \\text{, sounds like The Kestrel!} \\] \\[ Fst = \\lambda p. p (\\text{T}) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Fst = \\lambda p. p \\text{ } \\text{T} \\] <p>\\(300\\) Lines.</p> \\[ \\text{And, because } \\text{F} \\text{ is like the opposite of } \\text{T} \\text{ (in that it does the second and not the first), } Snd \\text{ should just be } \\lambda p. p \\text{ } \\text{F} \\text{!} \\] \\[ \\text{Ok, fine, I'll prove it (the } Snd = \\lambda p. p \\text{ } \\text{F} \\text{ thing).} \\] \\[ (a, b)(\\text{F}) = \\text{F}(a, b) = b \\] \\[ \\text{And that's exactly what you'd expect (from a function that inputs a pair, and outputs the second thing in that pair), proof complete!} \\] \\[ \\text{Next: (it might not seem obvious why, but) I want to tell you about The Phi Combinator!} \\] \\[ \\Phi = \\lambda p. \\text{V} (Snd(p))(Succ(Snd(p))) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ \\Phi = \\lambda p. \\text{V} (Snd \\text{ } p)(Succ(Snd \\text{ } p)) \\] \\[ \\text{And the pair notation one:} \\] \\[ \\Phi = \\lambda p. (Snd(p), Succ(Snd(p))) \\] \\[ \\text{And the auto-parenthesized and pair notation one:} \\] \\[ \\Phi = \\lambda p. (Snd \\text{ } p, Succ(Snd \\text{ } p)) \\] \\[ \\Phi((\\text{doesn't matter what the first thing is}, n0)) = (n0, n1) \\] \\[ \\Phi((n0, n1)) = \\Phi(\\Phi((\\text{doesn't matter what the first thing is}, n0))) = (n1, n2) \\] \\[ \\Phi((n1, n2)) = \\Phi(\\Phi(\\Phi((\\text{doesn't matter what the first thing is}, n0)))) = (n2, n3) \\] \\[ \\vdots \\] \\[ \\Phi((n6, n7)) = \\Phi(\\Phi(\\Phi(\\Phi(\\Phi(\\Phi(\\Phi(\\Phi((\\text{doesn't matter what the first thing is}, n0))))))))) = n8(\\Phi, (\\text{doesn't matter what the first thing is}, n0)) = (n7, n8) \\] \\[ Fst(n8(\\Phi, (\\text{doesn't matter what the first thing is}, n0))) = Fst((n7, n8)) \\] \\[ Fst(n8(\\Phi, (\\text{doesn't matter what the first thing is}, n0))) = n7 \\] \\[ Pred = \\lambda n. Fst(n(\\Phi, \\text{V} (\\text{doesn't matter what the first thing is}, n0))) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Pred = \\lambda n. Fst(n \\text{ } \\Phi \\text{ } (\\text{V} \\text{ doesn't matter what the first thing is } n0)) \\] \\[ \\text{And the pair notation one:} \\] \\[ Pred = \\lambda n. Fst(n(\\Phi, (\\text{doesn't matter what the first thing is}, n0))) \\] \\[ \\text{And the auto-parenthesized and pair notation one:} \\] \\[ Pred = \\lambda n. Fst(n \\text{ } \\Phi \\text{ } (\\text{doesn't matter what the first thing is}, n0)) \\] \\[ \\text{Before we do subtraction, what is } Pred(n0) \\text{?} \\] \\[ Pred(n0) = Fst(n0(\\Phi, (\\text{doesn't matter what the first thing is}, n0))) \\] \\[ n0(\\Phi, (\\text{doesn't matter what the first thing is}, n0)) = (\\text{doesn't matter what the first thing is}, n0) \\] \\[ Fst((\\text{doesn't matter what the first thing is}, n0)) = \\text{doesn't matter what the first thing is} \\] \\[ \\text{I'll just choose } n0 \\text{.} \\] \\[ \\text{This also means that I have to redo the predecessor function.} \\] \\[ Pred = \\lambda n. Fst(n(\\Phi, \\text{V} (n0, n0))) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Pred = \\lambda n. Fst(n \\text{ } \\Phi \\text{ } ( \\text{V } n0 \\text{ } n0)) \\] \\[ \\text{And the pair notation one:} \\] \\[ Pred = \\lambda n. Fst(n(\\Phi, (n0, n0))) \\] \\[ \\text{And the auto-parenthesized and pair notation one:} \\] \\[ Pred = \\lambda n. Fst(n \\text{ } \\Phi \\text{ } (n0, n0)) \\] \\[ \\text{Next: subtraction!} \\] \\[ Sub = \\lambda nk. n(Pred, k) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Sub = \\lambda nk. n \\text{ } Pred \\text{ } k \\] \\[ \\text{Next: is } 0 \\text{!} \\] \\[ f(n0) = \\text{T} \\] \\[ f(n) = \\text{F } (n &gt; 0) \\] \\[ f(n) = n(g, x) \\] \\[ f(n0) = n0(g, x) = x = \\text{T} \\] \\[ x = \\text{T} \\] <p>\\(400\\) Lines.</p> \\[ f(n) = n(g, \\text{T}) \\] \\[ f(n1) = n1(g, \\text{T}) = g(\\text{T}) = \\text{F} \\] \\[ g(\\text{T}) = \\text{F} \\] \\[ g = \\text{Not???} \\] \\[ f(n2) = n2(g, \\text{T}) = g(g(\\text{T})) = g(\\text{F}) = \\text{F} \\] \\[ g(\\text{F}) = \\text{F} \\] \\[ g(p) = \\text{F} \\] \\[ g = h(y) \\] \\[ h(y)(p) = \\text{F} \\] \\[ h(y, p) = \\text{F} \\] \\[ y = \\text{F} \\] \\[ h = K \\] \\[ g = K(\\text{F}) \\] \\[ f(n) = n(K(\\text{F}), \\text{T}) \\] \\[ is0 = \\lambda n. n(K(\\text{F}))(\\text{T}) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ is0 = \\lambda n. n \\text{ } (K \\text{F}) \\text{ } \\text{T} \\] \\[ \\text{Next: I'll keep it a mystery, 'cause the question is:} \\] \\[ \\text{When is } n - k \\text{ equal to } 0 \\text{? (Well, } n0 \\text{ actually.)} \\] \\[ \\text{Well, there's an obvious answer of when } n \\text{is equal to } k \\text{, but remember when I said that the predecessor of } n0 \\text{ is } n0 \\text{? So, any time } n \\text{ is less than } k \\text{, } n - k = n0 \\text{. So} \\] \\[ Leq = \\lambda nk. is0(Sub(n, k)) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Leq = \\lambda nk. is0(Sub \\text{ } n \\text{ } k)) \\] \\[ \\text{Next: } \u2265 \\text{!} \\] \\[ Geq = \\lambda nk. is0(Sub(k, n)) \\] \\[ \\text{And the auto-parenthesized one:} \\] \\[ Geq = \\lambda nk. is0(Sub \\text{ } k \\text{ } n)) \\] \\[ \\text{Next: } &gt; \\text{!} \\] \\[ Gt = \\text{Not} (Leq) \\] \\[ \\text{Next: } &lt; \\text{!} \\] \\[ Lt = \\text{Not} (Geq) \\] \\[ \\text{Next: } = \\text{!} \\] \\[ Eq = \\text{And} (Leq, Geq) \\] \\[ \\text{Next: } \\ne \\text{!} \\] \\[ Neq = \\text{Not} (Eq) \\] \\[ \\text{before we do the biggest (codable) number, let's write down every function that we know in one line.} \\] \\[ I \\text{ } M \\text{ } K / \\text{T} \\text{ } Ki / \\text{F} \\text{ } \\text{And} \\text{ } \\text{Or} \\text{ } \\text{Not} \\text{ } n0 \\text{ } n1 \\text{ } n2 \\text{ } n3 \\text{ } Succ \\text{ } Add \\text{ } Mult \\text{ } B \\text{ } Pow \\text{ } \\text{V} \\text{ } Fst \\text{ } Snd \\text{ } \\Phi \\text{ } Pred \\text{ } Sub \\text{ } is0 \\text{ } Leq \\text{ } Geq \\text{ } Gt \\text{ } Lt \\text{ } Eq \\text{ } Neq \\] \\[ \\text{Finally, the thing you've all been waiting for (final boss music starts playing), the biggest (codable) number} \\] \\[ Fgh = \\lambda nx. is0(n)(Succ(x), Eq(n, n1)(Mult(n2, x), Eq(n, n2)(Mult(x, Pow(2, x)), x(Fgh(Pred(n)), x)))) \\] \\[ \\text{Next: I'm skipping omega, because there's already an omega combinator (even though it's a capital omega), and it's equivalent to } M(M) \\text{. So, epsilon next} \\] \\[ Epss = \\lambda xn. is0(n)(Pred(x)(Pow(x), x), Pred(x)(Pow(Epss(x, Pred(n))), Epss(x, Pred(n)))) \\] \\[ Eps = \\lambda xn. Fgh(Epss(x, n), x) \\] \\[ \\varphi_s = \\lambda nxy. is0(n)(Pow(x, y), Eq(n, n1)(Epss(x, y), is0(y)(x(\\varphi_s (Pred(n), x), 0), Pred(x)(Pow(\\varphi_s (n, x, Pred(y))), x)))) \\]"},{"location":"lcr.html","title":"Lambda Calculus revisited","text":""},{"location":"lcr.html#coders-perspective","title":"coder's perspective","text":""},{"location":"lcr.html#introduction","title":"Introduction","text":"<p>Let's start out with a function \\(I\\) equal to \\(\\lambda x. x\\). If I plug in \\(1\\) to this function, what do I get back? What is \\(\\text{I} (1)\\)? Yeah, \\(\\text{I} (1) = 1\\), \\(\\text{I} (2) = 2\\), what about \\(\\text{I} (\\text{I})\\)? Yeah, it's \\(\\text{I}\\).</p> <p>So, the identity combinator takes in an input \\(x\\), and returns an output, also \\(x\\). So the identity of any \\(x\\) is \\(x\\), and as seen before, we can use functions as arguments, verbs are nouns and nouns are verbs, so the identity of identity is itself.</p>"},{"location":"lcr.html#what-is-this-lambda","title":"What is this \\(\\lambda\\)?","text":"<p>So, you might've seen this lambda stuff (\\(\\lambda x. x\\)) earlier and wondered what it ment, so I'll break it down into small parts. Lambda (\\(\\lambda\\)) is a signifier, a notation that I'm gonna use, to indicate that we're starting the definition of a function.</p> <p>So we can read this as: we're starting to define a function (\\(\\lambda\\)), that takes a single input (\\(x\\)), or parameter, and returns some expression, some body (\\(x\\)) (the part after rhe dot). This whole thing is called a lambda abstraction in the lambda calculus, but it just means that it's a unary (takes a single input) anonymous function</p>"},{"location":"lcr.html#what-is-the-lambda-calculus","title":"What is The Lambda Calculus?","text":"<p>A purely functional programming language. It's called \"calculus\" because the term \"calculus\" just means a method of moving around symbols on a page, but instead of the calculus of the very small and the very large, it's the calculus of evaluating and defining functions.</p>"},{"location":"lcr.html#syntax","title":"syntax","text":"<p>So in the lambda calculus, we have variables (which are kinda boring), we have expressions (applying a function to an argument), and we have lambda abstractions. In total, lambda terms are of the Backus-Naur Form:</p> \\[ M, N ::= x \\text{ } | \\text{ } M(N) \\text{ } | \\text{ } \\lambda x. M \\] <p>I gotta go do kid stuff now.</p> <p>It's been \\(20\\) minutes.</p> <p>Oh, right, there's also parenthesis.</p> <p>And that's the entire lambda calculus.</p>"},{"location":"lcr.html#examplescomparisons","title":"examples/comparisons","text":"<p>Let's start with the Backus-Naur Form...</p>"},{"location":"lcr.html#x","title":"\\(x\\)","text":"<p>There's pretty much one thing to say about variables in the lambda calculus: they are immutable, they can not be changed after the fact, there's no concept of \"assignment\" per say, in the lambda calculus (there is binding, i.e. You can assign a variable to a value, but only once. It becomes a \"bound variable\"), if a variable is bound to a value, that's its value for now and forever more.</p> <p>On the other hand, the Backus-Naur Form \\(M(N)\\) is slightly more interesting</p>"},{"location":"lcr.html#mn","title":"\\(M(N)\\)","text":"<p>In the lambda calculus, application is just a space, there's no parens for invocation. So, correction:</p> \\[ M, N ::= x \\text{ } | \\text{ } M \\text{ } N \\text{ } | \\text{ } \\lambda x. M \\] <p>This seems weird at first, but in reality, it ends up removing a lot of noise from our expressions.</p> The Lambda Calculus Code \\(f\\) \\(x\\) <code>f(x)</code> \\(f\\) \\(x\\) \\(y\\) <code>f(x)(y)</code>!! <p>I just wanted to have a multi-input function, but now, I have to interrupt this to explain...</p>"},{"location":"lcr.html#currying","title":"currying","text":"<p>In the lambda calculus, all functions are unary. This (\\(f\\) \\(x\\) \\(y\\)) is really a curried function, \\(f\\) takes in \\(x\\), and outputs a new function that takes in \\(y\\). A classic example of a curried function is the curried addition function, <code>add(1)</code> is the add \\(1\\) function, and if I apply it to \\(2\\), I get \\(3\\) (<code>add(1)(2) == 3</code>).</p> <p>So \\(f\\) \\(x\\) \\(y\\) can be read as \\(f\\) first taking in an \\(x\\), then a \\(y\\), and if there were more inputs, they would also be fed in one by one from the left to the right.</p>"},{"location":"lcr.html#back-to-examplescomparisons","title":"back to examples/comparisons","text":""},{"location":"lcr.html#m-n","title":"\\(M\\) \\(N\\)","text":"<p>You could make \\(f\\) \\(x\\) \\(y\\) more clear by saying \\((f\\) \\(x)\\) \\(y\\), but that dosen't do anything because application is left associative.</p> <p>But you can use parens to force it to be evaluated in a different order. So \\(f\\) \\((x\\) \\(y)\\) is actually a different expression from \\((f\\) \\(x)\\) \\(y\\), it means first apply \\(x\\) to \\(y\\), then the result of that to \\(f\\).</p> The Lambda Calculus Code \\(f\\) \\(x\\) <code>f(x)</code> \\(f\\) \\(x\\) \\(y\\) <code>f(x)(y)</code> \\((f\\) \\(x)\\) \\(y\\) <code>(f(x))(y)</code> \\(f\\) \\((x\\) \\(y)\\) <code>f(x(y))</code> <p>On the third hand, let's look at the Backus-Naur Form...</p>"},{"location":"lcr.html#lambda-x-m","title":"\\(\\lambda x. M\\)","text":"<p>Let's go from simple to complicated.</p> \\[ \\lambda a. b \\] <p>(Or <code>a =&gt; b</code>.)</p> <p>Takes in an input \\(a\\), throws it away, and outputs whatever \\(b\\) is.</p> \\[ \\lambda a. b \\text{ } x \\] <p>(Or <code>a =&gt; b(x)</code>.)</p> <p>Takes in an input \\(a\\), throws it away, and outputs whatever \\(b\\) is, but applied to whatever \\(x\\) is.</p> <p>Footnote: the lambda abstraction swallows up as many things to the right as it can (use parens to stop the lambda term).</p> <p>\\(100\\) Lines!</p> \\[ \\lambda a. (b \\text{ } x) \\] <p>(Or <code>a =&gt; (b(x))</code>.)</p> <p>But that's the same thing.</p> \\[ (\\lambda a. b) \\text{ } x \\] <p>(Or <code>(a =&gt; b)(x)</code>.)</p> <p>Which is actually a different thing, it's the function that takes an input, throws it away, and outputs whatever \\(b\\) is. That is, applied to \\(x\\)</p> \\[ \\lambda x. \\lambda y. x \\] <p>(Or <code>x =&gt; y =&gt; x</code>.)</p> <p>This is an example of a curried function. In particular, it does the first of two things.</p> \\[ \\lambda x. (\\lambda y. x) \\] <p>(Or <code>x =&gt; (y =&gt; x)</code>.)</p> <p>But that's the same thing.</p> The Lambda Calculus JS py \\(\\lambda a. b\\) <code>a =&gt; b</code> <code>lambda a: b</code> \\(\\lambda a. b \\text{ } x\\) <code>a =&gt; b(x)</code> <code>lambda a: b(x)</code> \\(\\lambda a. (b \\text{ } x)\\) <code>a =&gt; (b(x))</code> <code>lambda a: (b(x))</code> \\((\\lambda a. b) \\text{ } x\\) <code>(a =&gt; b)(x)</code> <code>(lambda a: b)(x)</code> \\(\\lambda x. \\lambda y. x\\) <code>x =&gt; y =&gt; x</code> <code>lambda x: lambda y: x</code> \\(\\lambda x. (\\lambda y. x)\\) <code>x =&gt; (y =&gt; x)</code> <code>lambda x: (lambda y: x)</code>"},{"location":"lcr.html#beta-reduction-and-others","title":"\\(\\beta\\)-reduction and others","text":"<p>Beta reduction is literally just plugging arguments into functions.</p> \\[ (\\lambda a. a)(\\lambda b. \\lambda c. b)(x)(\\lambda e. f) \\] <p>Plugging the argument into the parameter, and replacing the parameter in the body</p> \\[ (\\lambda b. \\lambda c. b)(x)(\\lambda e. f) \\] <p>Plugging the argument into the parameter, and replacing the parameter in the body</p> \\[ (\\lambda c. x)(\\lambda e. f) \\] <p>Plugging the argument into the parameter, and replacing the parameter in the body</p> \\[ x \\] <p>And now, it is in \\(\\beta\\)-normal form (that is, it has no redexes (that is, a thing that can be reduced (that is, an application where the function part starts with a \\(\\lambda\\))))</p> <p>All in an honest days work (I've added the entire page up to this point in one Oct. \\(15\\)'th)</p> <p>There's also bound and free variables: bound ones are the ones that start out in between a \\(\\lambda\\) and a \\(.\\) and free variables are the other ones.</p> <p>There's also \\(\\alpha\\)-equivalence: two things are alpha equivalent if they differ only in the name of a bound variable.</p> <p>There's extensionally vs intentionally equal: extensionally equal means giving the same inputs gives the same outputs, and intentionally equal means the same formula.</p> <p>You can abbreviate \\(\\lambda x. \\lambda y. \\lambda z.\\) as \\(\\lambda xyz.\\), which you can interpret as a ternary (takes three inputs) function, but don't be fooled, as every function in the lambda calculus is unary.</p> <p>And finally, I know what you might be thinking: you keep using that word 'combinator', I don't think it means what you think it means. To that, I say: yes, I do know what it means, other than function in the lambda calculus, it means function that has no free variables. I'm gonna give some examples, and you try to figure out which ones are combinators.</p> \\[ \\lambda b. b \\] \\[ \\lambda b. a \\] \\[ \\lambda ab. a \\] \\[ \\lambda a. a \\text{ } b \\] \\[ \\lambda abc. c(\\lambda e. b) \\] <p>First one: a combinator, nothing left undefined.</p> <p>Second one: not a combinator, what is \\(a\\)?</p> <p>Third one: a combinator, doesn't matter if \\(b\\) isn't used.</p> <p>Fourth one: not a combinator, what is \\(b\\)?</p> <p>Fifth one: a combinator.</p> Function Combinator \\(\\lambda b. b\\) Yes \\(\\lambda b. a\\) No \\(\\lambda ab. a\\) Yes \\(\\lambda a. a\\) \\(b\\) No \\(\\lambda abc. c\\) \\(\\lambda e. b\\) Yes"},{"location":"lcr.html#formalism","title":"formalism","text":"<p>Ok, to any mathematicians reading this (which is my general audience), you can stop holding your breath, I'm gonna rigorously define all of this.</p> <p>\\(200\\) Lines!</p>"},{"location":"lcr.html#extensional-vs-intensional-view-of-functions","title":"extensional vs. intensional view of functions","text":"<p>What is a function? Probably that \"functions as graphs\" thing: every function (call it \\(f\\)) has a fixed domain (call it \\(X\\)) and codomain (call it \\(Y\\)). We often say \\(f: X \u2192 Y\\) to denote this (pronounced \\(f\\) maps \\(X\\) to \\(Y\\)). The function is a subset of \\(X \\times Y\\) (the set of all ordered pairs of size \\(2\\) where the first term is in \\(X\\) and the second in \\(Y\\)) such that if \\(x \\in X\\) there exists one and only one \\(y \\in Y\\) such that the ordered pair \\((x, y) \\in f\\) (Side note! If both \\(X\\) and \\(Y\\) were the set of all real numbers, the ordered pair \\((x, y)\\) would not just look like a point, but would be the (well, at least, my) definition of a point. End of side note). Two functions \\(f, g: X \u2192 Y\\) are considered equal if they are the same set, that is, giving the same inputs gives the same outputs, a.k.a. An extensional perspective.</p> <p>Alternitavely, there's the \"functions as rules\" paradigm, commonlly used in the \\(19\\)th century: each function has a rule (e.g. the square function is a rule, and you can write this rule like \\(x^2 = x \\cdot x\\), or \\(^2 = x \u21a6 x \\cdot x\\)). Yeah, that's it. But it is as intensional view.</p> <p>Pop quiz/famous unsolved problem: are \\(x \u21a6 x\\) and \\(y \u21a6 y\\) intensionally equal?</p> <p>In this notion, you don't need to know the exact domain and codomain of a function. e.g. \\(f(x) = x\\) has domain and codomain \\(X\\) for any set \\(X\\).</p> <p>In most of mathematics, the \"functions as graphs\" paradigm is the most elegant and appropriate way of dealing with functions. Graphs define a more general class of functions, because it includes functions that are not necessarily given by a rule.</p> <p>On the other hand, in computer science, the \u201cfunctions as rules\u201d paradigm is often more appropriate. Think of a computer program as defining a function that maps input to output. Most computer programmers (and users) do not only care about the extensional behavior of a program (which inputs are mapped to which outputs), but also about how the output is calculated: How much time does it take? How much memory and disk space is used in the process? How much communication bandwidth is used? These are intensional questions having to do with the particular way in which a function was defined.</p>"},{"location":"lcr.html#the-lambda-calculus","title":"The lambda calculus","text":"<p>... Is a method for defining functions as rules.</p> <p>Time for arihmetic! Arithmetic expressions are made up from variables (\\(x\\), \\(y\\), \\(z\\)...), numbers (\\(1\\), \\(2\\), \\(3\\),...), and operators (\\(+\\), \\(-\\), \\(\\times\\) etc.). An expression such as \\(x + y\\) stands for the result of an addition (as opposed to an instruction to add, or the statement that something is being added). The great advantage of this language is that expressions can be nested without any need to mention the intermediate results explicitly (e.g. \"\\(A = (x + y) \\times z^2\\)\" and not \"let \\(w = x + y\\), then let \\(u = z^2\\), then let \\(A = w \\times u\\)\").</p> <p>The lambda calculus extends the idea of an expression language to include functions. Where we normally write \"Let \\(f\\) be the function \\(x \u21a6 x^2\\). Then consider A = f(5)\", in the lambda calculus we just write \\(A = (x \u21a6 x^2)(5)\\). But, because I want it to look more fancy, it is written \\(A = (\\lambda x. x^2)(5)\\). The expression \\(\\lambda x. x^2\\) stands for the function that maps \\(x\\) to \\(x^2\\) (as opposed to the statement that \\(x\\) is being mapped to \\(x^2\\)). As in arithmetic, we use parentheses to group terms.</p> <p>It is understood that the variable \\(x\\) is a local variable in the term \\(\\lambda x. x^2\\). Thus, it does not make any difference if we write \\(\\lambda y. y^2\\) instead. A local variable is also called a bound variable.</p> <p>One advantage of the lambda notation is that it allows us to easily talk about higher-order functions, i.e., functions whose inputs and/or outputs are themselves functions. An example is the operation \\(f \u21a6 f \u2218 f\\) in mathematics, which takes a function \\(f\\) and maps it to \\(f \u2218 f\\), the composition of f with itself. In the lambda calculus, \\(f \u2218 f\\) is written as \\(\\lambda x. f(f(x))\\), and the operation that maps \\(f\\) to \\(f \u2218 f\\) is written as \\(\\lambda f. \\lambda x. f(f(x))\\).</p>"},{"location":"lcr.html#just-read-the-paper","title":"just read the paper.","text":""},{"location":"lcr.html#coders-perspective_1","title":"coder's perspective","text":""},{"location":"linear.html","title":"Linear algebra","text":""},{"location":"linear.html#linear-algebra","title":"linear algebra","text":""},{"location":"linear.html#part-1-choose-your-fighte-vector-space","title":"part \\(1\\): choose your fighte- vector space!","text":"<p>As you might know, the main thing in linear algebra is the vector. so, to make this as general as possible, I'm gonna let you make your own vector space (space in which vectors live). Something important that defines a vector is that there's a sense of vector \\(\\vec{u} + \\vec{v}\\) (for vectors \\(\\vec{u}\\) and \\(\\vec{v}\\)) and there's a sense of vector \\(c \\vec{v}\\) (for vector \\(\\vec{v}\\) and scalar (real number) \\(c\\)). But, you cannot (necessarily) multiply vectors or add vectors and scalars (unless you use geometric algebra). But, for something to quallify as a vector space, there are some more rules/axioms it has to follow: (assume that your vector space is denoted as \\(\\text{V}\\) with vectors \\(\\vec{u}\\), \\(\\vec{v}\\), \\(\\vec{w}\\) and scalars \\(x\\), \\(y\\), and \\(z\\))</p> <p>Rule #\\(1\\):</p> <p>\\(\\vec{u} + (\\vec{v} + \\vec{w}) = (\\vec{u} + \\vec{v}) + \\vec{w}\\)</p> <p>Rule #\\(2\\):</p> <p>\\(\\vec{u} + \\vec{v} = \\vec{v} + \\vec{u}\\)</p> <p>Rule #\\(3\\) with words:</p> <p>There is a vector \\(\\vec{0}\\) aka \"the zero vector\" such that \\(\\vec{v} + \\vec{0} = \\vec{v}\\) for all \\(\\vec{v}\\)</p> <p>Rule #\\(3\\) with set theory:</p> <p>\\(\\exists \\vec{0} \\in \\text{V}. \u2200 \\vec{v} \\in \\text{V}. \\vec{v} + \\vec{0} = \\vec{v}\\)</p> <p>Rule #\\(4\\) with words:</p> <p>For any \\(\\vec{v}\\) there is a \\(-\\vec{v}\\) such that \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\) for all \\(\\vec{v}\\)</p> <p>Rule #\\(4\\) with set theory:</p> <p>\\(\u2200 \\vec{v} \\in \\text{V}. \\exists -\\vec{v} \\in \\text{V}. \\vec{v} + (-\\vec{v}) = \\vec{0}\\)</p> <p>Rule #\\(5\\):</p> <p>\\(x(y \\vec{v}) = (xy) \\vec{v}\\)</p> <p>Rule #\\(6\\):</p> <p>\\(1 \\vec{v} = \\vec{v}\\)</p> <p>Rule #\\(7\\):</p> <p>\\(x(\\vec{u} + \\vec{v}) = x \\vec{u} + x \\vec{v}\\)</p> <p>Rule #\\(8\\):</p> <p>\\((x + y) \\vec{v} = x \\vec{v} + y \\vec{v}\\)</p> <p>The vector space that (at least to me) makes all of the intuition click is arrows in space where it's the same if it has the same length and direction (hence the little arrow over every vector). The result of adding two of them is putting the base of the second on the tip of the first and drawing a new arrow from the base of the first to the tip of the second. The result of multiplying one of these by a number is scaling the length by a factor of the number (hence the name) and flipping the vector and scaling the length by a factor of the absolute value of the number if it is negative. You can convince yourself that this is a vector space. Also, these sorts of vectors are usually rooted at the origin.</p> <p>another commonly used definition of a vector is that of lists of numbers. The result of adding two of them is adding them term by term and the result of multiplying one of these by a number is multiplying each term by said number. You can convince yourself that this is a vector space.</p> <p>You can convert from the first definition to the second by making a list of the vector's coordinates and doing the opposite to convert from a list of numbers to an arrow.</p> <p>Now that you have chosen a vector space, we can now move on to...</p>"},{"location":"linear.html#part-2-linear-combinations-span-and-basis-vectors","title":"part \\(2\\): linear combinations, span, and basis vectors","text":"<p>In \\(2d\\) (arrows of length \\(1\\) or list of two numbers) there are vectors that will prove to be very important. The first being called \\(\\hat{x}\\) (x hat), the unit vector pointing to the right (in the direction in the \\(x\\) axis) or the list of numbers \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix}\\) (vectors that are of length \\(1\\) are denoted with a hat) and \\(\\hat{y}\\) (y hat) the unit vector pointing up (in the direction in the \\(y\\) axis). AKA the list of numbers \\(\\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}\\).</p> <p>If you think about it, any \\(2d\\) vector \\(\\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix}\\) can be written in terms of \\(\\hat{x}\\) and \\(\\hat{y}\\) (i.e. \\(x \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} x \\\\ 0 \\\\ \\end{bmatrix}\\), \\(y \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ y \\\\ \\end{bmatrix}\\), \\(\\begin{bmatrix} x \\\\ 0 \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ y \\\\ \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix}\\)).</p> <p>Alternatively (and you might need a grid for this), you can take the unit vector in the x direction and scale it until it's tip is on the same vertical line as the tip of your vector and the same thing with the y direction. And, when you add them up, (you can deduce that) you get your original vector. This can be used as another way to go back and forth between the two definitions of a vector.</p> <p>By the way, this is called a linear combination of \\(\\hat{x}\\) and \\(\\hat{y}\\) (linear because if you fix one and vary the other, it traces out a line if you look at the tip of the result).</p> <p>For this reason that every \\(2d\\) vector can be made out of \\(\\hat{x}\\) and \\(\\hat{y}\\), they are called the basis vectors.</p> <p>Also, Every Vector that can be formed by adding and scaling \\(\\hat{x}\\), $\\hat{y}, and any other vector formed in this way is of the form \\(a \\hat{x} + b \\hat{y}\\), and the reason why is because \\((a \\hat{x} + b \\hat{y}) + c \\hat{x} + d \\hat{y} = (a + c) \\hat{x} + (b + d) \\hat{y}\\) and \\(c(a \\hat{x} + b \\hat{y}) = (a) \\hat{x} + (cb) \\hat{y}\\).</p> <p>Also by the way, \\(a \\vec{u} + b \\vec{v}\\) is called a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\).</p> <p>But this begs the question: we could've used any other two basis vectors and we would've gotten another completely sensible way of going back and forth between the two definitions of a vector. That is, of course, unless the two vectors that are aligned with each other (or are both the zero vector).</p> <p>By the way, the set of all the vectors that can be made with a linear combination of two vectors is called the span of those two vectors. This idea of using different basis vectors, aka a different basis, is something that I'll go much more in detail about later.</p> <p>Also, if you have just one vector, think of it as an arrow, but if you have many vectors, think of each of them as a point where the point lies at the tip of the vector.</p> <p>But, things get more interesting in \\(3d\\), now \\(a \\vec{u} + b \\vec{v}\\) is a flat sheet cutting through the origin.</p> <p>But what about the span of \\(3\\) vectors in \\(3d\\), Now it's \\(a \\vec{u} + b \\vec{v} + c \\vec{w}\\) for scalars \\(a\\), \\(b\\), \\(c\\). And if the third is in the span of the other two, it doesn't change the span and it's still a flat sheet cutting through the origin.</p> <p>You can imagine the first two forming a plane, and then the third one moving the plane around, sweeping it through space. Another intuition is that you're using all three scalars to your advantage, you can't replace one of them with the other two.</p> <p>Whenever you can remove a vector without changing its span it is also known as linearly dependent, but \\(\\hat{z}\\) signed the declaration of independence ~\\(250\\) years ago, so they span all of \\(3d\\) space.</p> <p>So, the more formal definition of a basis is a set of linearly independent vectors that span all of space.</p> <p>Some of you might have seen the definition of linear independence as \"The only solution to \\(a \\vec{u} + b \\vec{v} + c \\vec{w} = 0\\) is \\(a = b = c = 0\\)\". But I think you should instead think of it as \"There isn't any \\(a\\), \\(b\\) such that \\(\\vec{w} = a \\vec{u} + b \\vec{v}\\)\"</p> <p>\\(100\\) Lines.</p>"},{"location":"linear.html#part-3-matrices-and-linear-transformations","title":"part \\(3\\): matrices and linear transformations","text":"<p>This is probably the most important part in this course.</p> <p>Let's start off this part with a quote:</p> <p>No one really understands The Matrix, you just have to see for yourself.</p> <p>-Morpheus</p> <p>Jokes Aside, for this part I'm going to be talking about linear transformations.</p> <p>Transformation is just a fancy word for function (In this context, it's a function that inputs and outputs vectors), but what makes it linear is that it preserves the two operations of vector addition and scalar multiplication.</p> <p>That is, \\(L(\\vec{u} + \\vec{v}) = L(\\vec{u}) + L(\\vec{v})\\), and \\(L(c \\vec{v}) = c L(\\vec{v})\\) (I'll explain why the word linear is used later).</p> <p>You can picture moving every vector to the corresponding output vector. And, if you remember what I told you the last part, that should mean transforming the grid itself, stretching and morphing it.</p> <p>But linear algebra limits itself to a very specific type of transformation: transformations of space that keep the grid lines parallel and evenly spaced with the origin remaining fixed.</p> <p>You can convince yourself that these two are the same.</p> <p>But, if you were given one of these guys, how would you describe it numerically? What is \\(L(\\vec{v})\\)?</p> <p>Well, describe \\(\\vec{v}\\) as a linear combination of \\(\\hat{x}\\) and \\(\\hat{y}\\), so \\(v_x \\hat{x} + v_y \\hat{y}\\)</p> \\[ L(\\vec{v}) = L(v_x \\hat{x} + v_y \\hat{y}) = L(v_x \\hat{x}) + L(v_y \\hat{y}) = v_x L(\\hat{x}) + v_y L(\\hat{y}) \\] <p>This is why it's called a linear transformation, \\(L(\\vec{v})\\) is a linear combination of \\(L(\\hat{x})\\) and \\(L(\\hat{y})\\)</p> <p>So, literally all you need to define a (\\(2d\\)) linear transformation is where \\(\\hat{x}\\) and \\(\\hat{y}\\) each go.</p> <p>Here's a concrete example: let's say that the transformation applied to \\(\\hat{x}\\) is \\(\\begin{bmatrix} 1 \\\\ -2 \\\\ \\end{bmatrix}\\) and the transformation applied to \\(\\hat{y}\\) is \\(\\begin{bmatrix} 3 \\\\ 0 \\\\ \\end{bmatrix}\\), then the transformation applied to \\(-1 \\hat{x} + 2 \\hat{y}\\) should be \\(-1 \\begin{bmatrix} 1 \\\\ -2 \\\\ \\end{bmatrix} + 2 \\begin{bmatrix} 3 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} (-1)(1) + (2)(3) \\\\ (-1)(-2) + (2)(0) \\\\ \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 2 \\\\ \\end{bmatrix}\\)</p> <p>Ok, got all that?</p> <p>In general, this transformation applied to \\(\\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix}\\) is \\(\\begin{bmatrix} 1x + 3y \\\\ -2x + 0y \\\\ \\end{bmatrix}\\). You give me any vector and I tell you the output vector.</p> <p>What I'm saying is that the linear transformation \\(L\\) is completely determined by four numbers: the \\(x\\) coordinate of the transformed \\(\\hat{x}\\), the y coordinate of the transformed \\(\\hat{x}\\), the \\(x\\) coordinate of the transformed \\(\\hat{y}\\), and the y coordinate of the transformed \\(\\hat{y}\\).</p> <p>Usually how you write a linear transformation is with a \\(2x2\\) group of numbers, also called a called a \\(2x2\\) matrix. You can read off the first column as where \\(\\hat{x}\\) goes, and the second as where \\(\\hat{y}\\) goes.</p> <p>By the way, linear transformations are usually denoted with an \\(L\\) (\\(L\\) for linear), but matrices are usually denoted as capital letters.</p> <p>Also by the way, if you're applying a linear transformation to a vector and the linear transformation is written as a matrix, \\(A(\\vec{v})\\) is usually just denoted as \\(A \\vec{v}\\)  and called \"matrix vector multiplication\".</p> <p>If you're given a matrix describing a linear transformation, and you're also given some specific vector, and you want to compute the linear transformation evaluated on said vector, you multiply the coordinates of the vector by the columns of the matrix and adding up the results.</p> <p>Here's a concrete example:</p> \\[ \\begin{bmatrix} 3 &amp; 2 \\\\ -2 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 7 \\\\ \\end{bmatrix} = 5 \\begin{bmatrix} 3 \\\\ -2 \\\\ \\end{bmatrix} + 7 \\begin{bmatrix} 2 \\\\ 1 \\\\ \\end{bmatrix} \\] <p>What about the most general possible example of matrix vector multiplication:</p> \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} = x \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + y \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix} = \\begin{bmatrix} ax \\\\ cx \\\\ \\end{bmatrix} + \\begin{bmatrix} by \\\\ dy \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} = \\begin{bmatrix} ax + by \\\\ cx + dy \\\\ \\end{bmatrix} \\] <p>You could even use this formula as a definition. And then you could teach it to high schoolers worldwide and not teach them the key intuition that makes it intuitive (\\(x \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + y \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix}\\))</p> <p>Isn't it better to think of the columns of the matrix as where \\(\\hat{x}\\) and \\(\\hat{y}\\) each go? And the result of multiplying a matrix by a vector as the appropriate linear combination?</p> <p>How would you describe a linear transformation like a 90\u00b0 counterclockwise rotation? (Yes, that is a linear transformation.) Well, \\(\\hat{x}\\) gets shifted up towards \\(\\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}\\) (\\(\\hat{y}\\)), and \\(\\hat{y}\\) gets rotated down towards \\(\\begin{bmatrix} -1 \\\\ 0 \\\\ \\end{bmatrix}\\) (\\(-\\hat{x}\\)). So the result should be the matrix \\(\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}\\), and if you want to rotate any vector clockwise by 90 degrees, just multiply it by the matrix \\(\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}\\).</p> <p>On the other hand, if the two columns are linearly dependent, the transformation squishes all of space onto one line, the span of the two linearly dependent columns.</p> <p>Summary:</p> <p>\\(200\\) Lines.</p> <p>linear transformations are those that preserve the operations of vector addition and scalar multiplication, of which you can think of as transformations of space that keep the grid lines parallel and evenly spaced with the origin remaining fixed. But to describe your linear transformation, you only need a handful of numbers: the coordinates of where the basis vectors land. matrices give us a language for linear transformations: just read off the columns and you'll know where the basis vectors land. And matrix vector multiplication just tells you what the linear transformation does to a given vector.</p> <p>Ok, got all that?</p>"},{"location":"linear.html#part-4-matrix-multiplication","title":"part \\(4\\): matrix multiplication","text":"<p>What happens if you want to apply more than one linear transformation to a given vector? How do you do that?</p> <p>If you want to apply \\(\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}\\) (aka rotate the plane 90\u00b0 counterclockwise), then \\(\\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix}\\) (commonly called a shear (?)) You get a new matrix, commonly called the composition of the other two.</p> <p>Here's a quick proof that the result is indeed a linear transformation:</p> \\[ C(\\vec{v}) = L(M(\\vec{v})) \\] \\[ C(\\vec{u} + \\vec{v}) = L(M(\\vec{u} + \\vec{v})) = L(M(\\vec{u}) + M(\\vec{v})) = L(M(\\vec{u})) + L(M(\\vec{v})) = C(\\vec{u}) + C(\\vec{v}) \\] \\[ C(c \\vec{v}) = L(M(c \\vec{v})) = L(c M(\\vec{v})) = c L(M(\\vec{v})) = c C(\\vec{v}) \\] <p>Side note!</p> <p>if you apply \\(A\\) to \\(\\vec{v}\\), then you apply \\(B\\) to that, it's actually written as \\(BA \\vec{v}\\) (because \\(B(A \\vec{v})\\)).</p> <p>This stems from how functions are written, because if you have something like \\(f(g(x))\\), then was really saying is apply \\(g\\) first, then \\(f\\), you have to read it right to left.</p> <p>Good for the Hebrew eaters (?), bad news for the rest of us.</p> <p>End of side note.</p> <p>We can find the columns of this new composition matrix like this:</p> \\[ A \\hat{x} = \\begin{bmatrix} ? &amp; ? \\\\ ? &amp; ? \\\\ \\end{bmatrix} \\hat{x} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} (\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\hat{x}) : = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} (1)(0) + (1)(1) \\\\ (0)(0) + (1)(1) \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} ? &amp; ? \\\\ ? &amp; ? \\\\ \\end{bmatrix} \\hat{x} = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} \\] \\[ A \\hat{y} = \\begin{bmatrix} 1 &amp; ? \\\\ 1 &amp; ? \\\\ \\end{bmatrix} \\hat{y} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} (\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\hat{y}) : = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} -1 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} (1)(-1) + (1)(0) \\\\ (0)(-1) + (1)(0) \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} 1 &amp; ? \\\\ 1 &amp; ? \\\\ \\end{bmatrix} \\hat{y} = \\begin{bmatrix} -1 \\\\ 0 \\\\ \\end{bmatrix} \\] \\[ A = \\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\] <p>Here's a new way to think about the composition matrix: if you were to take your vector and apply the rotation to it, and then the shear. The long way of doing this is to first multiply it on the left by the rotation matrix, and then by the shear. But the short way of doing it is just multiplying it by this new composition matrix.</p> <p>This should work no matter what the initial vector was, because this new matrix should be able to capture the idea of the \"rotation then shear\" action</p> <p>\\(\\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} (\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\vec{v}) = \\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\vec{v}\\)</p> <p>And if I'm assuming that matrix vector multiplication is associative (i.e. \\(x^\\star (y^\\star z) = (x^\\star y)^\\star z\\)), then \\(A(B \\vec{v}) = (AB) \\vec{v}\\).</p> <p>For this reason, the composition of \\(A\\) and \\(B\\) is usually denoted as \\(AB\\).</p> <p>Imagine this new composition matrix of \\(A\\) and \\(B\\) as transforming all of space with the \\(B\\) transformation, and then transforming it again with the \\(A\\) transformation after that.</p> <p>Let's look at another example (but this time I'll be explaining it): let's say that \\(M_1 = \\begin{bmatrix} 1 &amp; -2 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}\\) and \\(M_2 = \\begin{bmatrix} 0 &amp; 2 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}\\). The composition of \\(M_1\\) and \\(M_2\\) should be a new matrix (call it \\(M_c\\)), let's find it!</p> <p>To find the first column of \\(M_c\\), we need to figure out what happens when we apply it to \\(\\hat{x}\\).</p> <p>But \\(M_c\\) of \\(\\hat{x}\\) must just be \\(M_2 M_1 \\hat{x}\\).</p> <p>But, because \\(M_1\\) is the \\(\\begin{bmatrix} 1 &amp; -2 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}\\) matrix, \\(M_1 \\hat{x}\\) is defined to equal \\(\\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}\\)</p> <p>Now, the question becomes: what is \\(M_2 \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}\\)?</p> <p>Well, \\(M_2 \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 2 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} = 1 \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} + 1 \\begin{bmatrix} 2 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} + \\begin{bmatrix} 2 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} + \\begin{bmatrix} 2 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\\\ \\end{bmatrix}\\)</p> <p>\\(303\\) Lines.</p> <p>So, \\(M_c = \\begin{bmatrix} 2 &amp; ? \\\\ 1 &amp; ? \\\\ \\end{bmatrix}\\)</p> <p>To find the second column of \\(M_c\\), we need to figure out what happens when we apply it to \\(\\hat{y}\\).</p> <p>But \\(M_c\\) of \\(\\hat{y}\\) must just be \\(M_2 M_1\\) \\(\\hat{y}\\).</p> <p>But, because \\(M_1\\) is the \\(\\begin{bmatrix} 1 &amp; -2 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix}\\) matrix, \\(M_1\\) \\(\\hat{y}\\) is defined to equal \\(\\begin{bmatrix} -2 \\\\ 0 \\\\ \\end{bmatrix}\\)</p> <p>Now, the question becomes: what is \\(M_2 \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}\\)?</p> <p>Well, \\(M_2 \\begin{bmatrix} -2 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 2 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} -2 \\\\ 0 \\\\ \\end{bmatrix} = -2 \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix} + 0 \\begin{bmatrix} 2 \\\\ 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\\\ \\end{bmatrix}\\)</p> <p>So, \\(M_c = \\begin{bmatrix} 2 &amp; 0 \\\\ 1 &amp; -2 \\\\ \\end{bmatrix}\\)</p> <p>Let's drive the most general possible formula for \\(2x2\\) matrix multiplication.</p> \\[ A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\] \\[ B = \\begin{bmatrix} e &amp; f \\\\ g &amp; h \\\\ \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} ? &amp; ? \\\\ ? &amp; ? \\\\ \\end{bmatrix} \\] \\[ (AB) \\hat{x} = A(B \\hat{x}) \\] \\[ B \\hat{x} : = \\begin{bmatrix} e \\\\ g \\\\ \\end{bmatrix} \\] \\[ A(B \\hat{x}) = A \\begin{bmatrix} e \\\\ g \\\\ \\end{bmatrix} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} e \\\\ g \\\\ \\end{bmatrix} = e \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + g \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix} = \\begin{bmatrix} ae + bg \\\\ ce + dg \\\\ \\end{bmatrix} \\] \\[ (AB) \\hat{y} = A(B \\hat{y}) \\] \\[ B \\hat{y} : = \\begin{bmatrix} f \\\\ h \\\\ \\end{bmatrix} \\] \\[ A(B \\hat{y}) = A \\begin{bmatrix} f \\\\ h \\\\ \\end{bmatrix} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{bmatrix} \\begin{bmatrix} f \\\\ h \\\\ \\end{bmatrix} = f \\begin{bmatrix} a \\\\ c \\\\ \\end{bmatrix} + h \\begin{bmatrix} b \\\\ d \\\\ \\end{bmatrix} = \\begin{bmatrix} af + bh \\\\ cf + dh \\\\ \\end{bmatrix} \\] \\[ AB = \\begin{bmatrix} ae + bg &amp; af + bh \\\\ ce + dg &amp; cf + dh \\\\ \\end{bmatrix} \\] <p>We just derived the formula that you (probably) learned in school. Isn't that satisfying? </p>"},{"location":"logic.html","title":"Set theory II","text":"<p>This page doesn't require seeing the set theory page, but you can see it anyways.</p> <p>The idea behind this page is that you can derive all of set theory from logic, so \\(\\text{T}\\) is true, and \\(\\text{F}\\) is false.</p> <p>Tools: \\(\\text{T}\\), \\(\\text{F}\\), \\(=\\)*, and another thing that will be defined later.</p> <p>*Returns true if the two things are the same, and false if they are not.</p>"},{"location":"logic.html#boolean-operations","title":"boolean operations","text":"<p>I can derive the and operation like this:</p> \\[ \\text{F} \u2229 \\text{F} = \\text{F} \\] \\[ \\text{F} \u2229 \\text{T} = \\text{F} \\] \\[ \\text{T} \u2229 \\text{F} = \\text{F} \\] \\[ \\text{T} \u2229 \\text{T} = \\text{T} \\] <p>and the or operation like this:</p> \\[ \\text{F} \u2228 \\text{F} = \\text{F} \\] \\[ \\text{F} \u2228 \\text{T} = \\text{T} \\] \\[ \\text{T} \u2228 \\text{F} = \\text{T} \\] \\[ \\text{T} \u2228 \\text{T} = \\text{T} \\] <p>and the not operation like this:</p> \\[ \u00ac \\text{F} = \\text{T} \\] \\[ \u00ac \\text{T} = \\text{F} \\] <p>and the less famous implies operation like this:</p> \\[ \\text{F} \u2192 \\text{F} = \\text{T} \\] \\[ \\text{F} \u2192 \\text{T} = \\text{T} \\] \\[ \\text{T} \u2192 \\text{F} = \\text{F} \\] \\[ \\text{T} \u2192 \\text{T} = \\text{T} \\] \\[ (p \\iff q) = (p = q) \\] \\[ (p = q) \\iff (p \u2192 q) \u2229 (q \u2192 p) \\] \\[ \\text{Order of operations: } \u00ac &gt; \u2229 &gt; \u2228 &gt; = &gt; \u2192 &gt; \\iff \\text{.} \\]"},{"location":"logic.html#extensions","title":"extensions","text":"\\[ \\{ x| p(x) \\} = \\text{ The extension of } p(x) \\text{ (and the secret } 4 \\text{th tool) (curly braces didn't work for rendering reasons), the set of all } x \\text{ such that } p(x) \\text{ (is true), and this is how I'm going to define sets. This function } p(x) \\text{ inputs, well, anything, and outputs a boolean (true or false) (e.g. is } x \\text{ an odd number?).} \\] \\[ \\text{The extension of that particular function is the set of all odd numbers.} \\]"},{"location":"logic.html#other-things-from-set-theory","title":"other things from set theory","text":"<p>You can also derive the membership or element sign like this:</p> \\[ x \\in \\{ y| p(y) \\} \\iff p(x) \\text{ (} = \\text{T)} \\] <p>which goes somewhere in between and and not in the order of operations.</p> <p>And just for fun, I'll also derive the for any and there exists signs like this:</p> \\[ \\text{But first, } p(x) \u2261 \\text{T} \\text{ Means } p(x) = \\text{T} \\text{ for all values of } x \\text{, same thing for } p(x) \u2261 \\text{F} \\text{ and } p(x) \u2261 q(x) \\text{.} \\] \\[ \\text{And it goes in between or and equals in the order of operations.} \\] \\[ \\text{I'll start with the general form for the for any sign: } \u2200(x) \\cdot p(x) \\text{ (} = \\text{T)}: q(x) \\text{ (} = \\text{T)} \\] \\[ \\text{And that example in logic: } p(x) \u2192 q(x) \\text{, at least, I think.} \\] \\[ \\text{Yeah, it doesn't work, different values for } x \\text{ would give different outputs.} \\] \\[ \\text{But, there's a simple trick, take the and over all values for } x \\text{, so } (p(x) \u2192 q(x)) \u2261 \\text{T} \\text{.} \\] \\[ \\text{And the general form for the there exists sign: } \\exists (x) \\cdot p(x) \\text{ (} = \\text{T)} \\] \\[ \\text{And that example in logic: } \u00ac(p(x) \u2261 \\text{F}) \\] \\[ \\text{Maybe I'll use it.} \\] \\[ \\text{There's also the highlander function, it's general form is} \\] \\[ \\exists ! (x) \\cdot p(x) \\text{ (} = \\text{T)}  \\] \\[ \\text{And that example in logic: } \u00ac(p(x) \u2261 \\text{F}) \u2229 (p(x) \u2192 (\u00ac(y = x) \u2192 p(y) = \\text{F})) \\] \\[ \\text{Alternative: } \u00ac(p(x) \u2261 \\text{F}) \u2229 (p(x) \u2229 \u00ac(y = x) \u2192 p(y) = \\text{F}) \\]"},{"location":"logic.html#numbers","title":"numbers","text":"\\[ 0 = \u00d8 = \\{ x| \\text{F} \\} \\] \\[ \\text{succ} (x) = \\{ y| y = x \\} \\] <p>\\(100\\) Lines.</p> \\[ \\text{succ} (0) = 1 \\] \\[ \\text{succ} (1) = 2 \\] \\[ \\text{succ} (2) = 3 \\] \\[ 0 \\in \u2115 \\] \\[ x \\in \u2115 \u2192 \\text{succ} (x) \\in \u2115 \\] \\[ \u2115 = \\{ x| (x = 0) \u2228 \\exists (y) \\cdot (\\text{succ} (y) = x) \u2229 y \\in \u2115 \\} = \\{ x| (x = 0) \u2228 \u00ac(((\\text{succ} (y) = x) \u2229 y \\in \u2115) \u2261 \\text{F}) \\} \\]"},{"location":"logic.html#russells-paradox","title":"Russell's paradox","text":"\\[ R = \\{ x| \u00ac(x \\in x) \\} \\] \\[ \\text{Now, the question is, is } R \\in R \\text{? Because if } \u00ac(R \\in R) \\text{, than } \u00ac(x \\in x) \\text{ would be true (for } x \\text{ equal to } R \\text{), but then, } R \\text{ would be an element of } R \\text{, but if } R \\in R \\text{, than } \u00ac(x \\in x) \\text{ would be false (for } x \\text{ equal to } R \\text{), but then, } R \\text{ wouldn't be an element of } R \\text{, paradox!)} \\] \\[ x \\in \\{ y| p(y) \\} = \\text{N (} \\ne \u2115 \\text{)} \\iff (\u00ac(x \\in \\{ y| p(y) \\}) \u2192 p(x) = \\text{T}) \u2229 ((x \\in \\{ y| p(y) \\}) \u2192 p(x) = \\text{F}) \\] \\[ \\text{N} \u2229 \\text{Anything} = \\text{N} \\] \\[ \\text{N} \u2228 \\text{Anything} = \\text{N} \\] \\[ \u00ac \\text{N} = \\text{N} \\] \\[ R \\in R = \\text{N} \\] <p>Now, I'll define an infinite set of things \\(x1, x2, x3,...\\) that are all in \\(\\{ x| \\text{T} \\}\\).</p> <p>And define \\(x1, x2, x3 \\in S\\) as \\(x1 \\in S \u2229 x2 \\in S \u2229 x3 \\in S\\).</p> <p>And that, if a statement involves \\(x1\\) or \\(x2\\) or \\(x3\\) and so on, then it's true for any value \\(x1, x2, x3,...\\).</p>"},{"location":"logic.html#addition","title":"addition","text":"\\[ \\text{I already defined the successor, what about addition?} \\] \\[ x1, x2 \\in \u2115 \u2192 x1 + succ(x2) = succ(x1 + x2) \\] \\[ x1 \\in \u2115 \u2192 x1 + 0 = x1 \\]"},{"location":"logic.html#mappings","title":"mappings","text":"<p>A mapping \\(f\\) is, well, first pick a domain \\(X\\) and codomain \\(Y\\). Then, um, I gotta get out the set theory textbook.</p> <p>Ok, I got out The real number system, in an algebraic setting, by J. B. Roberts. A gift from my grandpa.</p> <p>A mapping \\(f\\) is kinda like a function or a dictionary in python, (a mapping \\(f\\) from domain \\(X\\) and codomain \\(Y\\) is denoted as \\(f: X \u2192 Y\\)) if you pick an \\(x\\) in \\(X\\), is it \\(tied\\) \\(to\\) some \\(y\\) in \\(Y\\). The particular value of \\(y\\) for given \\(x\\) is denoted as \\(f(x)\\), but for a given \\(y\\) in \\(Y\\), there could be \\(0\\), \\(1\\), or more values of \\(x\\) (e.g. The absolute value function). How you represent a mapping would be something like \\(f(x) = x^2\\), \\(x \u21a6 x^2\\), or \\(\\lambda x. x^2\\) .. \\(f(x)\\) for \\(\u00ac(x \\in X)\\) is \\(\\text{N}\\) (\\(\\text{N}\\) stands for neither, but it can also br interpreted an NoneType from python). And finally, the image of \\(f\\) is (\\(I(f)\\)):</p> \\[ I(f) = \\{ y| y \\in Y \u2229 \\exists (x) \\cdot x \\in X \u2229 f(x) = y \\} \\] <p>So</p> \\[ I(f) = \\{ y| y \\in Y \u2229 \u00ac((x \\in X \u2229 f(x) = y) \u2261 \\text{F}) \\} \\] <p>But if you don't like domains and codomains, here's the versions without them:</p> \\[ I(f) = \\{ y| \u00ac(y = \\text{N}) \u2229 \\exists (x) \\cdot f(x) = y \\} \\] \\[ I(f) = \\{ y| \u00ac(y = \\text{N}) \u2229 \u00ac((f(x) = y) \u2261 \\text{F}) \\} \\]"},{"location":"logic.html#cardinality","title":"cardinality","text":"\\[ \\text{The cardinality of } S \\text{ is denoted as hashtag } S \\text{, but hashtags aren't allowed in git, so it's } C(S) \\text{ instead.} \\] \\[ (\\exists (n) \\cdot n \\in \u2115 \u2229 (S = \\{ k| k &lt; n \\})) \u2192 C(S) = n \\] \\[ C(S) = C(T) \\iff \\exists (f: S \u2192 T) \\cdot \u2200(t) \\cdot t \\in T: \\exists ! (s) \\cdot f(s) = t \\]"},{"location":"logic.html#mappings-again","title":"mappings (again)","text":"<p>A definition that I thought of for mappings (that dosent imply a domain or codomain) uses a new thing to define axiomatioly (i.e. A thing that I have to define that it exists, and hasn't been seen before (e.g. How sets and extensions are not booleans)) (e.g. The \\(\\text{N}\\) in the Russell's paradox section): \\(\u2192\\) (Yes, another arrow (that makes \\(4\\) (boolean arrow \\(\u2192\\), mapping arrow \\(\u2192\\), fancy functional arrow \\(\u21a6\\), and new arrow \\(\u2192\\)))). I'll explain with an example: the function \\(x \u21a6 x^2\\) would be denoted as the set containing \\(0 \u2192 0\\), \\(1 \u2192 1\\), \\(2 \u2192 4\\), \\(3 \u2192 9\\), \\(4 \u2192 16\\), \\(5 \u2192 25\\), and so on. (\\((x \u2192 y)[1] = x\\), And \\((x \u2192 y)[2] = y\\).)</p> <p>You can also define ordered sets as mappings from natural numbers greater than \\(0\\) (or from all natural numbers if you want it to be confusing like in every single programming language exept fortran) (this is why I didn't use ordered lists last time, because, they are defined with mappings) (I gotta go ice skating now, bye!) (I'm back and it's been \\(2\\) hours).</p> \\[ f(x) = \\begin{Bmatrix} \u00ac \\exists(t) \\cdot t \\in f \u2229 (t[1] = x): \\text{N} \\\\ \\exists(t) \\cdot t \\in f \u2229 (t[1] = x): t[2] \\\\ \\end{Bmatrix} \\]"},{"location":"logic.html#other-random-stuff","title":"other random stuff","text":"\\[ \\{ x| p(x)] \u2228 \\{ x| q(x)] = \\{ x| p(x) \u2228 q(x)] \\] \\[ \\{ x| p(x)] \u2229 \\{ x| q(x)] = \\{ x| p(x) \u2229 q(x)] \\] \\[ S \\times T \\text{ is pronounced \"the cartesian product of } S \\text{ and } T \\text{\".} \\] \\[ \\{ x| p(x)] \\times \\{ x| q(x)] = \\{ l| l \\text{ is a list of size } 2 \u2229 (p(l(1)) \u2229 q(l(2)))] \\] \\[ S \\times T = \\{ l| l \\text{ is a list of size } 2 \u2229 (l(1) \\in S \u2229 l(2) \\in T)] \\] <p>The reason why it's called \"the cartesian product\" is (at least, I think) because \\(\u211d \\times \u211d\\) (\\(\u211d\\) is the set of all real numbers) is the set of all ordered pairs of real numbers. That is, the set of all points on the cartesian plane.</p> <p>The actual axiom of choice:</p> <p>\\(200\\) Lines.</p> \\[ \u2200(S) \\cdot S \\text{ Is a set of sets}: \\exists (f: S \u2192 \\{ x| \\exists(T) \\cdot x \\in T \u2229 T \\in S]) \\cdot \u2200(T) \\cdot T \\in S: f(T) \\in T \\] \\[ \\text{There's also compositions denoted as } f \u2218 g \\text{, but that does } g \\text{ first, then } f \\text{ (} (f \u2218 g)(x) = f(g(x)) \\text{). So I'll say that } f^* g \\text{ does } f \\text{ first, then } g \\text{ (} (f^* g)(x) = g(f(x)) \\text{).} \\] <p>Here's a diagram of sets \\(A\\), \\(B\\), \\(C\\), mappings \\(f: A \u2192 B\\) and \\(g: B \u2192 C\\), \\(f^* g: A \u2192 C\\), and \\(I: \\{ x| \\text{T}] \u2192 \\{ x| \\text{T}] = x \u21a6 x\\) : (Just replace \\(\\text{id}\\) with \\(I\\), and \\(g \u2218 f\\) with \\(f^* g\\))</p>"},{"location":"logic.html#the-8-axioms-of-zermelofraenkel-set-theory-zfc","title":"the \\(8\\) axioms of Zermelo\u2013Fraenkel set theory (ZFC)","text":"<p>\\(1\\): The Axiom of Extensionality:</p> \\[ \u2200(S, T): (\u2200(x): x \\in S \\iff x \\in T \u2192 S = T) \\] <p>\\(2\\): The Axiom of Pairing:</p> \\[ \u2200(S, T): \\exists (U) \\cdot U = \\{ x| x = S \u2228 x = T] \\] <p>\\(3\\): The Axiom of Union:</p> \\[ \u2200(S, T): \\exists (U) \\cdot U = \\{ x| x \\in S \u2228 x \\in T] \\] <p>\\(4\\): The Axiom of the Power Set:</p> \\[ \u2200(S): \\exists (T) \\cdot T = \\{ x| x \u2286 S] \\] <p>\\(5\\): The Axiom of Infinity:</p> \\[ \\exists (S) \\cdot S = \\{ x| x = \\{ y| \\text{F}] \u2228 \\exists (y) \\cdot y \\in S \u2229 x = \\{ z| z \\in y \u2228 z = y]] \\] <p>\\(6\\): The Axiom of Schema of Replacement:</p> \\[ \u2200(f): \\exists (S) \\cdot S = \\{ y| \\exists (P) \\cdot P \\in f \u2229 p[2] = y] \\] <p>\\(7\\): The Axiom of Regularity:</p> \\[ \u2200(A): (A \\ne \\{ x| \\text{F}] \u2192 \\exists (B) \\cdot (B \\in (A \u2229 B) \u2229 A = \\{ x| \\text{F}])) \\] <p>\\(8\\): The Axiom of Schema of Separation:</p> \\[ \u2200(S): \\exists (T) \\cdot \u2200(x) \\cdot x \\in T: x \\in S \\]"},{"location":"logic.html#the-secret-9th-axiom-of-zermelofraenkel-set-theory-zfc","title":"the secret \\(9\\)'th axiom of Zermelo\u2013Fraenkel set theory (ZFC)","text":"<p>The Axiom of Choice:</p> \\[ \u2200(S): \\exists (f) \\cdot \u2200(T) \\cdot T \\in S \u2229 T \\ne \\{ x| \\text{F}]: f(T) \\in T \\]"},{"location":"logic.html#comparosins","title":"comparosins","text":"My old notation Standard notation My new notation \\(\u2200(x) \\cdot p(x): q(x)\\) \\(\u2200x. p(x) \u2192 q(x)\\) \\((p(x) \u2192 q(x)) =_x \\text{T}\\) \\(\u2200(x): p(x)\\) \\(\u2200x. p(x)\\) \\(p(x) =_x \\text{T}\\) \\(\\exists (x) \\cdot p(x)\\) \\(\\exists x. p(x)\\) \\(\u00acp(x) =_x \\text{F}\\) \\(\u2200(A): \\exists (f) \\cdot \u2200(B) \\cdot B \\in A \u2229 \u00acB = \\{ x \\| \\text{F}]: f(B) \\in B\\) \\(\u2200A. \\exists f. \u2200B. B \\in A \u2229 B \\ne \u00d8 \u2192 f(B) \\in B\\) \\(((B^S \\in A^S \u2229 \u00acB = \\{ x \\| \\text{F}] \u2192 f(B) \\in B) =_f \\text{F}) =_{A, B} \\text{F}\\)"},{"location":"logic.html#my-new-notation","title":"my new notation","text":"<p>The \\(=_x\\) sign is pronounced \"is equal to, for all values of \\(x\\)\". Also, I (kinda) wrote \\(\\exists (x) \\cdot p(x)\\) as \\(\u00ac(\u2200(x): p(x) = \\text{F}\\)), because the statement \"there exists an \\(x\\) where \\(p(x)\\) is true\" is just the inversion of the statemment \"\\(p(x)\\) is, for all values of \\(x\\), equal to false\". Then I realized that the for any sign takes the and over all values of \\(x\\) and that the there exists sign takes the or over all values of \\(x\\). So that \\(\u00ac(\u2200(x): p(x) = \\text{F})\\) thing is just DeMorgan's laws: \\(\u00ac(\u00acp \u2229 \u00acq) = p \u2228 q\\) and \\(\u00ac(\u00acp \u2228 \u00acq) = p \u2229 q\\). So the equuivalent statement for the for any sign is \\(\u00ac(\\exists (x) \\cdot p(x) = \\text{F})\\)</p>"},{"location":"logic.html#my-newer-notation","title":"my newer notation","text":"<p>All the symbols we need are: \\(\u2227\\) (and), \\(\u2228\\) (or), \\(\u00ac\\) (not), \\(\u2192\\) (implies), \\(\\iff\\) (if and only if/precisely when), \\((\\) (open parentheses), \\()\\)(close parentheses), \\(\u2200\\) (for any/all), \\(\u2203\\) (there exists), \\(\\in\\) (is an element) of, a separator (such as a dot), and none of the above (so literally anything else, I'll choose \\(x\\)). These are all of the symbols you need to describe all of set theory, and hence, all of modern mathematics.</p> <p>Order of operations: \\(() &gt; \\in &gt; \u00ac &gt; \u2229 &gt; \u2228 &gt; \u2192 &gt; \\iff\\), and if you have a for any or there exists sign, it keeps going as far to the right as possible, kind of like a lambda.</p> <p>Also, the statement \\(x \\in S\\) is the only possible way to make a statement that might be true and might be false, you can even think of sets as boolean functions that might be true and might be false depending on the input.</p> <p>Also also, the reason why I'm using a dot as in for any \\(x\\) and then a dots is because all of the variables are just strings of \\(x\\)'s, so for any \\(xx\\), \\(xxx\\) and then something, if I didn't have that separator, it would just be written as one block of five \\(x\\)'s, and it would not be certain what it actually meant.</p> <p>Also also also, instead of writing out the full thing with lots of \\(x\\)'s, I'm usually just going to write \\(x\\) subscript and then the amount of \\(x\\)'s there should be.</p> <p>An example of a line of logic is: \"\\(\u2200x.x \\ne \u00d8 \u2192 \u2203xx.xx \\in x\\), \\(\u2200x.x \\ne \u00d8 \u2192 \u2203xx.\\) Double \\(x\\) is wihin \\(x\\)\", \"\\(\u2200x.x \\ne \u00d8\\) implies that there exists double \\(x\\) where double \\(x\\) is wihin \\(x\\)\", \"For all \\(x\\), x not being epty implies that there exists double \\(x\\) where double \\(x\\) is wihin \\(x\\)\"</p>"},{"location":"logic.html#1-1","title":"\\(1 + 1\\)","text":"<p>Before I start, none of this was scripted.</p> <p>What I want to do here is prove that \\(1 + 1 = 2\\). But most of the time, you find yourself in a loop of defining things. For example: what is \\(1\\)? \\(1\\) is the successor of \\(0\\). What is the successor operation? The successor operation is the function that-</p> <p>\"Stop right there!\" Said person #\\(2\\), \"What is a function?\".</p> <p>\"Okay, fine!\" Said person #\\(1\\), \"I'll instead say that \\(1\\) is the set that contains \\(0\\).\" (written \\(\\{ 0 \\}\\).)</p> <p>\"That's better, but can you express \\(\\{ 0 \\}\\) more formally?\" Said person #\\(2\\).</p> <p>\"Well, what I mean by that is: \\(\u2200x.x \\in \\{ 0 \\} \\iff x = 0\\).\" Said person #\\(1\\). (For any/all \\(x\\), \\(x\\) is within \\(\\{ 0 \\}\\) precisely when \\(x = 0\\).)</p> <p>\"But what is \\(=\\)?\" Said person #\\(2\\).</p> <p>\"Axiom #\\(1\\) of ZFC: the axiom of extensionality\" said person #\\(1\\), \"it states that \\(S = T\\) precisely when for any \\(s \\in S\\), \\(s \\in T\\), and for any \\(t \\in T\\), \\(t \\in S\\)\"</p> <p>\"Okay, so what you're saying is that \\(\u2200x.x \\in 1 \\iff (\u2200y.y \\in x \u2192 y \\in 0) \u2227 (\u2200y.y \\in 0 \u2192 y \\in x)\\).\" Said person #\\(2\\).</p> <p>\"Yes. Is there anything else left undefined?\" Said person #\\(1\\).</p> <p>\"Yes, always!\" Said person #\\(2\\). \"What is \\(0\\)?\"</p> <p>\\(300\\) Lines.</p> <p>\"Are you really gonna make me answer that?\" Said person #\\(1\\).</p> <p>\"Yes!\" Said person #\\(2\\).</p> <p>\"Okay, fine!\" Person #\\(1\\) said with frustration. \"\\(0\\) is \u00d8\" (the empty set) \"is the set with nothing in it, so \\(\u00ac\u2203x.x \\in \u00d8\\).\"</p> <p>\"So, what you really meant by \\(\u2200x.x \\in 1 \\iff (\u2200y.y \\in x \u2192 y \\in 0) \u2227 (\u2200y.y \\in 0 \u2192 y \\in x)\\) was \\(\u2200x.x \\in 1 \\iff \u00ac\u2203y.y \\in x\\).\" Said person #\\(2\\)</p> <p>\"Yes!\" Said person #\\(1\\).</p> <p>\"So \\(1\\) is the set of all empty sets, of which there are only one\" Said person #\\(2\\) \"am I understanding this correctly?\" Said person #\\(2\\)</p> <p>\"Yes!\" Said person #\\(1\\).</p> <p>I'm tired of this conversation between a mathematician and probably a mathematical snob who only accepts the truest logical statements crafted from pure mathematical set theory.</p> \\[ \u2200x_1.(\u2200x_2.x_2 \\in x_1 \\iff \u00ac\u2203x_3.x_3 \\in x_2) \u2192 x_1 + x_1 = 2 \\] \\[ \\text{\"For any variable (call it } x_1 \\text{), that variable being } 1 \\text{ implies that adding it to itself results in } 2 \\text{, that is, there is not an } x_3 \\in x_2 \\text{\"} \\] \\[ \u2200x_1.x_1 \\in 2 \\iff (\u2200x_2.x_2 \\in x_1 \\iff \u00ac\u2203x_3.x_3 \\in x_2) \\] \\[ \\text{\"For any variable (call it } x_1 \\text{), } x_1 \\in 2 \\text{ implies that } x_1 = 1 \\text{, that is, for any variable (call it } x_2 \\text{), } x_2 \\in 1 \\text{ precisely when } x_2 = 0 \\text{, that is, there is not an } x_3 \\in x_2 \\text{\"} \\] \\[ \u2200x_1.\u2200x_2.(\u2200x_3.x_3 \\in x_1 \\iff \u00ac\u2203x_4.x_4 \\in x_3) \u2227 (\u2200x_3.x_3 \\in x_2 \\iff (\u2200x_4.x_4 \\in x_3 \\iff \u00ac\u2203x_5.x_5 \\in x_4)) \u2192 \u2200x_3.x_3 \\in x_1 + x_1 \\iff x_3 \\in x_2 \\] \\[ A(m, n) = m + n \\] \\[ A(n, 0) = n \\] \\[ S(n) = n + 1 \\] \\[ A(m, S(n)) = S(A(m, n)) \\] \\[ \u2200m.\u2200n.n = S(m) \\iff (\u2200x.x \\in n \\iff x = m) \\] \\[ \u2200m.\u2200n.n = S(m) \\iff (\u2200x_1.x_1 \\in n \\iff (\u2200x_2.x_2 \\in x_1 \\iff x_2 \\in m)) \\] <p>I feel like doing something else, how about Russel's paradox? It states that there is no set that conain only sets that don't contain themselves.</p> \\[ \u00ac\u2203x_1.\u2200x_2.x_2 \\in x_1 \\iff \u00acx_2 \\in x_2 \\] <p>Next: the first axiom of set theory (The Axiom of Extensionality). It states that two sets are equal if they have the same elements, but I think it actually means that if two sets are equal (i.e. they have the same elements), a set cannot contain just one of them, it has to contain either both or neither of the sets.</p> \\[ \u2200x_1.\u2200x_2.x_1 = x_2 \u2192 \u2200x_3.x_1 \\in x_3 \\iff x_2 \\in x_3 \\] \\[ \u2200x_1.\u2200x_2.(\u2200x_3.x_3 \\in x_1 \\iff x_3 \\in x_2) \u2192 \u2200x_3.x_1 \\in x_3 \\iff x_2 \\in x_3 \\] <p>Next: the second axiom of set theory (The Axiom of Foundation). It states that every set must have an element disjoint from itself (i.e. an element where the union of that element and the original set is empty (i.e. they don't have any common elements)).</p> \\[ \u2200x_1.x_1 \\ne \u00d8 \u2192 \u2203x_2.x_2 \\in x_1 \u2227 x_2 \u2229 x_1 = \u00d8 \\] \\[ \u2200x_1.\u00acx_1 = \u00d8 \u2192 \u2203x_2.x_2 \\in x_1 \u2227 \u00ac\u2203x_3.x_3 \\in x_1 \u2227 x_3 \\in x_2 \\] \\[ \u2200x_1.x_1 = \u00d8 \u2228 \u2203x_2.x_2 \\in x_1 \u2227 \u00ac\u2203x_3.x_3 \\in x_1 \u2227 x_3 \\in x_2 \\] \\[ \u2200x_1.\u00ac(\u2203x_2.x_2 \\in x_1) \u2228 \u2203x_2.x_2 \\in x_1 \u2227 \u00ac\u2203x_3.x_3 \\in x_1 \u2227 x_3 \\in x_2 \\] <p>Next: the third axiom of set theory (The Axiom of Pairing). Actually, I'm not going to use the axiom of pairing, I'm going to use the closely related singleton axiom, It states that if you have a set then there exists the set containing that set, as opposed to the axiom of pairing which says that if you have two sets then there is a set containing both of them. These two statements are equivalent, but I prefer the first one.</p> <p>Also I realized that this axiom makes the axiom a regularity redundant. Let's say that \\(S = \\{ S \\}\\). then you would say that \\(S\\) is a set because it is equal to the set containing \\(S\\). So we would also need to assume that \\(S\\) is a set for that to work, so that would mean that we need to prove that \\(S\\) is a set, so that would mean that we need to prove that \\(S\\) is a set, you just never get to the bottom of it and you can never declare that \\(S\\) is a set.</p> <p>Also this is a weird kind of axiom because it doesn't always make the set containing a set into a set, you still have to prove it with the other rules. So we just knocked out two axioms of set theory with one stone.</p> <p>Also, at around this point I would like to tell you something about existence that you can tell someone to cocktail party.</p> <p>First, I have to think of a property. How about even-ness? Okay, now that we have a property, I can tell you what the problem is.</p> <p>The statement \"there does not exist \\(x\\) where \\(x\\) is odd\" is the same as the statement \"for all \\(x\\), \\(x\\) is even\". The statement \"there does not exist \\(x\\) where \\(x\\) is even\" is the same as the statement \"for all \\(x\\), \\(x\\) is odd\". The statement \"there exists \\(x\\) where \\(x\\) is odd\" is just the inversion of the statement \"for all \\(x\\), \\(x\\) is even\". And the statement \"there exists \\(x\\) where \\(x\\) is even\" is just the inversion of the statement \"for all \\(x\\), \\(x\\) is odd\".</p> <p>If you put it that way, the \\(\u2203\\) seems kind of redundant.</p> <p>This for all/their exists thing is a consequence of De Morgan's laws.</p> <p>Didn't I already say this? Anyways, before we continue with the axioms have set theory. I'm goind to do the axioms of proof theory. Of which I'm going to take from this course by bri the math guy (the actual series didn't exist, only the full movie), using the statements \\(x \\in S\\) and \\(y \\in S\\) (yes) instead of \\(p\\) and \\(q\\).</p> <p>Modes Ponens: If \\(p\\) is true and \\(p\\) implies \\(q\\), then \\(q\\) is true. It's like the consequence of an implication.</p> \\[ \u2200S.\u2200x.\u2200y.x \\in S \u2227 (x \\in S \u2192 y \\in S) \u2192 y \\in S \\] <p>Modes Tollens: If \\(q\\) is false and \\(p\\) implies \\(q\\), then \\(p\\) is false. It's like an implication turned on its head.</p> \\[ \u2200S.\u2200x.\u2200y.\u00acy \\in S \u2227 (x \\in S \u2192 y \\in S) \u2192 \u00acx \\in S \\] <p>Hypothetical Syllogism: If \\(p\\) implies \\(q\\) and \\(q\\) implies \\(r\\), then \\(p\\) implies \\(r\\). i.e. you can chain implications.</p> \\[ \u2200S.\u2200x.\u2200y.\u2200z.(x \\in S \u2192 y \\in S) \u2227 (y \\in S \u2192 z \\in S) \u2192 (x \\in S \u2192 z \\in S) \\] <p>Disjunctive Syllogism: If \\(p\\) is false and \\(p\\) or \\(q\\) is true, then \\(q\\) is false. It has to be one or the other! (Or both, but that's irrelevant.)</p> \\[ \u2200S.\u2200x.\u2200y.\u00acx \\in S \u2227 (x \\in S \u2228 y \\in S) \u2192 y \\in S \\] <p>Addition: If \\(p\\) is true, then \\(p\\) or \\(q\\) is true. The more, the marrier!</p> \\[ \u2200S.\u2200x.\u2200y.x \\in S \u2192 (x \\in S \u2228 y \\in S) \\] <p>Simplification: If \\(p\\) and \\(q\\) are true, then \\(p\\) is true. They must both be true!</p> <p>\\(400\\) Lines.</p> \\[ \u2200S.\u2200x.\u2200y.(x \\in S \u2227 y \\in S) \u2192 x \\in S \\] <p>Conjunction: If \\(p\\) is true and \\(q\\) is true, then \\(p\\) and \\(q\\) are true. Litterally just \\(p\\) implies \\(p\\).</p> \\[ \u2200S.\u2200x.\u2200y.(x \\in S \u2227 y \\in S) \u2192 (x \\in S \u2227 y \\in S) \\]"},{"location":"mod.html","title":"Modular arithmetic","text":""},{"location":"mod.html#like-the-title-suggests-what-would-happen-if-you-restart-counting-after-10","title":"Like the title suggests, what would happen if you restart counting after \\(10\\)?","text":"<p>You (supposedly) started counting when you were very young, so starting with counting to the integers, we will rebuild arithmetic from the ground up.</p> \\[ 1 \\] \\[ 2 \\] \\[ 3 \\] \\[ 4 \\] \\[ 5 \\] \\[ 6 \\] \\[ 7 \\] \\[ 8 \\] \\[ 9 \\] \\[ 10 \\] \\[ 1 \\] \\[ 2 \\] <p>Okay, this seems about right, but the whole point (kinda) is for there to be one digit, so instead of \\(10\\), what about \\(0\\)?</p> \\[ 1 \\] \\[ 2 \\] \\[ 3 \\] \\[ 4 \\] \\[ 5 \\] \\[ 6 \\] \\[ 7 \\] \\[ 8 \\] \\[ 9 \\] \\[ 0 \\] \\[ 1 \\] \\[ 2 \\] <p>That's better, just use the last digit of a number. But you might be asking: What about the equals sign? In this case, the equals sign tells if two numbers have the same last digit, I'll use the O-equals sign for this.</p> \\[ n = 10 \\] \\[ k \u229c k + n \\] \\[ k \u229c k + 2n \\] \\[ k \u229c k + 3n \\] \\[ \\vdots \\] \\[ k \u229c k - n \\] \\[ k \u229c k - 2n \\] \\[ k \u229c k - 3n \\] \\[ \\vdots \\] \\[ \\text{Remember, these are only integers so far!} \\] \\[ a \u229c b \\to M(a) = M(b) \\] \\[ a \u229c M(a) \\] <p>A function like \\(M\\) would be useful for converting back and forth, but what should it be? What about \\(M(k)\\) equals the \\(k\\)'th term in, I'll call it the modular counting. Also, instead of the objective of finding what numbers are equal to, you try to find it's \\(M\\). Also, there are only \\(10\\) numbers in modular arithmetic, because</p> \\[ -1 &lt; M(k) &lt; n. \\] \\[ k = k_d n + M(k) \\] \\[ k \u229c M(k) \\text{, So } M(M(k)) = M(k) \\text{.} \\] \\[ M(a + b) = ? \\] \\[ a = a_d n + M(a) \\] \\[ b = b_d n + M(b) \\] \\[ M(a + b) = M(a_d n + M(a) + b_d n + M(b)) = M((a_d + b_d) n + M(a) + M(b)) \\] \\[ M(a + b) = M(M(a) + M(b)) \\] \\[ M(ab) = ? \\] \\[ M(ab) = M((a_d n + M(a))(b_d n + M(b))) = M(a_d n b_d n + a_d n M(b) + M(a) b_d n + M(a) M(b)) = M((a_d b_d n + a_d M(b) + b_d M(a)) n + M(a) M(b)) \\] \\[ M(ab) = M(M(a)M(b)) \\] \\[ M(a - b) = M(M(a) + M(-b)) \\] \\[ M(-b) \u229c -b \u229c -M(b) \\] \\[ M(a - b) = M(M(a) - M(b))) \\]"},{"location":"mod.html#non-integers","title":"Non-integers","text":"\\[ M(\\frac{a}{b}) = M(M(a)M(\\frac{1}{b})) \\] <p>Remember, these are only integers so far, this should be the part where you learn about non-integers, but modular arithmetic actually has only \\(10\\) numbers in it. But first, we need to take a leap of faith, the multiplicative inverse.</p> \\[ 3 \\cdot 7 = 21 \u229c 1 \\] \\[ \\text{Thus} \\] \\[ 7 \u229c \\frac{1}{3} \\] \\[ \\text{and} \\] \\[ 3 \u229c \\frac{1}{7} \\] \\[ \\frac{5}{7} = 5 \\cdot 3 = 15 \u229c 5. \\] <p>But there is a problem, and it's not \\(\\frac{1}{10}\\), but here is the problem with \\(\\frac{1}{10}\\) anyways.</p> \\[ 10 \u229c 0 \\] \\[ \\frac{1}{10} ? \u229c ? \\frac{1}{0} \\] \\[ \\frac{1}{10} \u229c ? \\] \\[ ? \\cdot 10 \u229c 1 \\] \\[ \\text{any number} \\cdot 10 \u229c 0 \\] \\[ ? \u229c \\frac{1}{0} \\] \\[ \\frac{1}{10} \u229c \\frac{1}{0} \\] \\[ \\text{The problem is that } 1 \\text{ divided by a non-} 0 \\text{ number is undefined, say } 2. \\] \\[ 2 \u229c / \u229c 0 \\] \\[ \\frac{1}{2} ? \u229c / \u229c ? \\frac{1}{0} \\] \\[ \\frac{1}{2} \u229c ? \\] \\[ 1 \\cdot 2 \u229c 2 \\ne 1 \\] \\[ ? \\ne 1 \\] \\[ 2 \\cdot 2 \u229c 4 \\ne 1 \\] \\[ ? \\ne 2 \\] \\[ 3 \\cdot 2 \u229c 6 \\ne 1 \\] \\[ ? \\ne 3 \\] \\[ \\vdots \\] \\[ 5 \\cdot 2 \u229c 0 \\text{ again} \\] \\[ \\vdots \\] \\[ 10 \\cdot 2 \u229c 0 \\ne 1 \\] \\[ ? \\ne 3 \\] \\[ \\frac{1}{2} \u229c \\frac{1}{0} \\] <p>So the problem is that you can't divide by some non-0 numbers like I said. This happens when you are dividing by a factor of the base of the modulus (the base of the modulus is the term where you restart counting first) and the numerator does not also have a factor of it. Said simply</p> \\[ \\frac{a}{b} \u229c \\frac{1}{0} \\text{ precisely when } n \\text{ divides } b \\text{, and } a \\text{ does not divide } b. \\] \\[ \\text{The solution, of coarse, is to switch the base from } 10 \\text{ to } 5 \\] \\[ n = 5. \\] \\[ \\text{Side note! } 2 \\text{ and } 3 \\text{ are } i \\text{ and } -i \\text{ respectively. Actually I don't know which is } i \\text{ and which is } -i \\text{, but here's proof!} \\] \\[ i^2 = -1 \\] \\[ (-i)^2 = -1 \\] \\[ 2^2 = 4 = 5 - 1 \u229c -1 \\] \\[ 3^2 = 9 = 2 \\cdot 5 - 1 \u229c -1 \\] \\[ i \\cdot -i = 1 \\] \\[ 2 \\cdot 3 = 6 \u229c 1 \\] <p>(\\(200\\) lines btw)</p> \\[ i + (-i) = 0 \\] \\[ -(i) = -i \\] \\[ -(-i) = i \\] \\[ 2 + 3 = 5 = 5 \\cdot 1 + 0 \u229c 0 \\] \\[ -2 = 5 \\cdot (-1) + 3 \u229c 3 \\] \\[ -3 = 5 \\cdot (-1) + 2 \u229c 2 \\] <p>So, because \\(0\\) \\(1\\) \\(i\\) \\(-i\\) and \\(-1\\) would multiply to get another one of \\(0\\) \\(1\\) \\(i\\) \\(-i\\) and \\(-1\\). This means that if you want to multiply numbers mod \\(5\\), just use the beautiful angle addition of the complex plane. Let's try to find the mod \\(7\\) equivalent using last Monday's work. That took all day. First, the times tables mod \\(7\\)</p> \\[ \\begin{bmatrix} \\times \\quad 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\ 0 \\quad 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 \\quad 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\ 2 \\quad 0 &amp; 2 &amp; 4 &amp; 6 &amp; 1 &amp; 3 &amp; 5 \\\\ 3 \\quad 0 &amp; 3 &amp; 6 &amp; 2 &amp; 5 &amp; 1 &amp; 4 \\\\ 4 \\quad 0 &amp; 4 &amp; 1 &amp; 5 &amp; 2 &amp; 6 &amp; 3 \\\\ 5 \\quad 0 &amp; 5 &amp; 3 &amp; 1 &amp; 6 &amp; 4 &amp; 2 \\\\ 6 \\quad 0 &amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 \\\\ \\end{bmatrix} \\] <p>So, I would want numbers \\(2\\) - \\(5\\) to correspond with letters \\(a\\) - \\(d\\). And no letter for \\(6\\), because it is already equal to \\(-1\\) mod \\(7\\). So the puzzle is to solve for complex numbers \\(a\\) \\(b\\) \\(c\\) and \\(d\\) with the following times table:</p> \\[ \\begin{bmatrix} \\times \\quad 0 &amp; 1 &amp; a &amp; b &amp; c &amp; d &amp; -1 \\\\ 0 \\quad 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 \\quad 0 &amp; 1 &amp; a &amp; b &amp; c &amp; d &amp; -1 \\\\ a \\quad 0 &amp; a &amp; c &amp; -1 &amp; 1 &amp; b &amp; d \\\\ b \\quad 0 &amp; b &amp; -1 &amp; a &amp; d &amp; 1 &amp; c \\\\ c \\quad 0 &amp; c &amp; 1 &amp; d &amp; a &amp; -1 &amp; b \\\\ d \\quad 0 &amp; d &amp; b &amp; 1 &amp; -1 &amp; c &amp; a \\\\ -1 \\quad 0 &amp; -1 &amp; d &amp; c &amp; b &amp; a &amp; 1 \\\\ \\end{bmatrix} \\] \\[ j = \\pm i \\] \\[ k = -j = \\mp i \\] \\[ \\text{So, mod } 5 \\] \\[ 2 = j \\] \\[ 3 = k \\] \\[ \\text{back to mod } 7 \\text{ with a more flattened out times table without multiplication by } 0 \\text{ or } 1 \\text{, because that is pretty self explanatory.} \\] \\[ aa = c \\] \\[ ab = -1 \\] \\[ ac = 1 \\] \\[ ad = b \\] \\[ a (-1) = d \\] \\[ bb = a \\] \\[ bc = d \\] \\[ bd = 1 \\] \\[ b (-1) = c \\] \\[ cc = a \\] \\[ cd = -1 \\] \\[ c (-1) = d \\] \\[ dd = c \\] \\[ d (-1) = a \\] \\[ (-1) (-1) = 1 \\] <p>I'll let you think about it.</p> <p>SPOILERS BELOW</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p>"},{"location":"mod.html#answer","title":"answer","text":"<p>You might have gussed that all of these have absolute value \\(1\\), but you might not have known that these are the roots of unity. But remember to use that \\(j\\) thing. And yes, as opposed to \\(e^{i\\theta} = cos(\\theta) + i \\text{ } sin(\\theta)\\),</p> \\[ e^{j\\theta} = cos(\\theta) + j sin(\\theta). \\] <p>Also, \\(300\\) lines by the way.</p> <p>You might have also noticed that these numbers can circle around in two different ways. (I found the \\(d\\) one before the \\(b\\) one by the way)</p> \\[ 1b = b \\] \\[ bb = a \\] \\[ ab = -1 \\] \\[ (-1) b = c \\] \\[ cb = d \\] \\[ db = 1. \\] <p>and also</p> \\[ 1d = d \\] \\[ dd = c \\] \\[ cd = -1 \\] \\[ (-1) d = a \\] \\[ ad = b \\] \\[ bd = 1. \\] <p>Here's a diagram of both in the complex plane:</p> <p>And for \\(d\\):</p> <p>And also, if two numbers on the complex plane have absolute value \\(1\\), and they multiply to \\(1\\), then they are complex conjugates or as you might know as \\(ccong\\), so the correct picture translating back from letters \\(a\\) - \\(d\\) and \\(-1\\) to numbers \\(2\\) - \\(5\\) and \\(6\\) would be the \\(b\\) - picture with up being \\(j\\) and down being \\(k\\). And here it is:</p> <p>Isn't it beautiful? I can just add the angles and multiply the magnitudes of any two numbers to get their product mod \\(7\\). \\(6\\) is on \\(-1\\) which makes sense because \\(6 = 7 - 1\\). Also, numbers on opposite sides add to \\(7\\), and \"opposite side\" in complex numbers means \"negative\", so that is also just so perfect to me too. Also, if two numbers are complex conjugates on the unit circle, then they are inverses of each other, and I've never seen that in a real desmos graph before! Also, that was a bit of a lie, because I'm actually editing on the repo, so the graph isn\u2019t loading, and I would have to click on the tab with the graph to see it. Alternatively, I could just hit the \"Commit changes\" button on the top-right and wait for \\(2\\) minutes to see it on the website. Yeah, I'll do that.</p> <p>I will add only this line of text to my website, because it would be sad to add anything else to my website on easter.</p> <p>I will add only this line of text to my website, because it would be sad to add anything else to my website on April \\(1\\)'st. Well, no it wouldn't be, but no one would believe something added on April \\(1\\)'st. So I'll take the day off.</p> <p>Okay, you know the drill. Time to copy paste and tweak the previous line, just give me a sec... Wait! April \\(2\\)'nd is not a holiday. But that means that I have to work today. I think I liked the quadruple weekend more.</p> <p>And, would you look at that. I forgot to work at all yesterday, and I have to take the day off for my family. Like I said many times: I'll imbed the desmos graphs tomorrow, or maybe today if I am lucky</p> <p>At this point on April \\(4\\)'th, I'll just stop working on this page and write down below the day that I embedded the desmos graphs.</p> <p>April \\(6\\)'th, I deleted the thing I did the day before, and that's why it doesn't look different.</p>"},{"location":"mod.html#how-to-make-your-own-diagram-for-modular-arithmetic-in-base-p-in-the-desmos-graphing-calculator","title":"how to make your own diagram for modular arithmetic in base \\(p\\) in the desmos graphing calculator","text":"<p>You don't actually need to write out the full times table, you just have to find the smallest primitave root \\(r\\) of the base (a primitave root of a base is a number where it's power's would result in all different numbers (exept \\(0\\)) in the modular base. e.g. \\(2\\) in mod \\(5\\), \\(2^0 \u229c 1\\), \\(2^1 \u229c 2\\), \\(2^2 \u229c 4\\), \\(2^3 \u229c 3\\), \\(2^4 \u229c 1\\). One way to find one is to google it, or use brute force) and then put it a \\(p - 1\\)'th around the unit circle from \\(1\\), which is just the point (\\(1\\), \\(0\\)). Anyways, first boot up the desmos graphing calculator, then I realized that the colors of the functions you draw go red, blue, green, purple, black, and back to red. First, type \\(x\\) and check if it is a good color for your circle,  if it is, remove the letter \\(x\\) and copy this \"x^{2}+y^{2}=1\" and paste it into desmos, if not, press the button on the top-right of the box that you are writing in, which I will call the \\(x\\) button, and repeat until you either find a good color, or choose the least worst color available. In my case, black. Oh, and paste in <code>p=1</code> to make a slider called \\(p\\) and set it to the base that you chose. then do the same thing for the dots, but click on the emptyness underneith the box before pressing the \\(x\\) button. Once you find a good dot color, paste in <code>\\left(0,0\\right)</code>. Then, this is the tricky part, click on the emptyness underneith the box, then press the \\(x\\) button, repeat \\(5\\) times to get the same color, this will be known as \"repeat until same color\". then copy and paste in <code>\\left(1,0\\right)</code>, repeat until same color, than paste in <code>\\left(\\cos\\left(\\frac{2\\pi}{p - 1}\\right),\\sin\\left(\\frac{2\\pi}{p - 1}\\right)\\right)</code>, repeat until same color, than paste in <code>\\left(\\cos\\left(2\\frac{2\\pi}{p - 1}\\right),\\sin\\left(2\\frac{2\\pi}{p - 1}\\right)\\right)</code>, repeat until same color, than paste in <code>\\left(\\cos\\left(3\\frac{2\\pi}{p - 1}\\right),\\sin\\left(3\\frac{2\\pi}{p - 1}\\right)\\right)</code>... For this reason, you should probably choose \\(p\\) to be small like \\(5\\) or \\(7\\), and I probably should've told you that earlier. Then the best part, press \"label\" on each dot and click on the big underscore to start naming, the dot in the center is named \" \\(0\\) \". the one directly to the right and not up or down is named \" \\(1\\) \". The name of the one that is directly counterclockwise to the previous is found by multiplying the previous value by \\(r\\) and subtracting \\(p\\) until you get \\(0\\) or a number less than \\(p\\). And yeah, that's really all there is to it, I'm sorry for those who decided to read through this wall of text (which is the biggest one yet) and didn't just skip to here, but for those people who did read this whole thing, you should now have a beautiful diagram for modular arithmetic.</p> <p>Say goodbye to the modular arithmetic page and say hello to a new page, It's name is the name of the first chapter on the brainstorm page without a question mark... Or just scroll down the dropdown menu or home page.</p> <p>And with that, the page has ended.</p>"},{"location":"polynomial.html","title":"Polynomial","text":""},{"location":"polynomial.html#disclaimer","title":"disclaimer","text":"<p>this page uses terms like \\(dx\\) and \\(e\\), but these have nothing to do with calculus and are just notation, also in the \\(2nd\\) and \\(4th\\) chapters if this is the \\(1st\\), I swapped \\(x\\) for \\(x - \\frac{b}{na}\\) where \\(n\\) is the degree of the polynomial so when plugging things in, make sure to add \\(\\frac{b}{na}\\), also I probably should have swapped the order I did this so that the quadratic came first, the last thing to warn you about is that I re-used letters like \\(a\\), \\(b\\), \\(c\\), \\(p\\), \\(q\\), and \\(z\\), anyways let's start strong with the...</p>"},{"location":"polynomial.html#quartic","title":"quartic","text":"\\[ ax^4 + bx^3 + cx^2 + dx + e = 0 \\] \\[ x \\to x - \\frac{b}{4a} \\] \\[ a (x - \\frac{b}{4a})^4 + b (x - \\frac{b}{4a})^3 + c (x - \\frac{b}{4a})^2 + d (x - \\frac{b}{4a}) + e = 0 \\] \\[ ax^4 - 4ax^3 \\frac{b}{4a} + 6ax^2 \\frac{b^2}{16a^2} - 4ax \\frac{b^3}{64a^3} + a \\frac{b^4}{256a^4} + bx^3 - 3bx^2 \\frac{b}{4a} + 3bx \\frac{b^2}{16a^2} - b \\frac{b^3}{64a^3} + cx^2 - 2cx \\frac{b}{4a} + c \\frac{b^2}{16a^2} + dx - d \\frac{b}{4a} + e = 0 \\] \\[ ax^4 - bx^3 + \\frac{3b^2 x^2}{8a} - \\frac{b^3 x}{16a^2} + \\frac{b^4}{256a^3} + bx^3 - \\frac{3b^2 x^2}{4a} + \\frac{3b^3 x}{16a^2} - \\frac{b^4}{64a^3} + cx^2 - \\frac{bcx}{2a} + \\frac{b^2c}{16a^2} + dx - \\frac{bd}{4a} + e = 0 \\] \\[ ax^4 - \\frac{3b^2 x^2}{8a} - \\frac{b^3 x}{16a^2} + \\frac{b^4}{256a^3} + \\frac{3b^3 x}{16a^2} - \\frac{b^4}{64a^3} + cx^2 - \\frac{bcx}{2a} + \\frac{b^2c}{16a^2} + dx - \\frac{bd}{4a} + e = 0 \\] \\[ (a)x^4 + (c-\\frac{3b^2}{8a})x^2 + (\\frac{2b^3}{16a^2} - \\frac{bc}{2a} + d)x + (\\frac{b^4}{256a^3} - \\frac{b^4}{64a^2} + \\frac{b^2c}{16a^2} - \\frac{bd}{4a} + e) = 0 \\] \\[ x^4 + (c-\\frac{3b^2}{8a^2})x^2 + (\\frac{2b^3}{16a^3} - \\frac{bc}{2a^2} + \\frac{d}{a})x + (\\frac{b^4}{256a^4} - \\frac{b^4}{64a^3} + \\frac{b^2c}{16a^3} - \\frac{bd}{4a^2} + \\frac{e}{a}) = 0 \\] \\[ p = c-\\frac{3b^2}{8a^2} \\] \\[ q = \\frac{2b^3}{16a^3} - \\frac{bc}{2a^2} + \\frac{d}{a} \\] \\[ r = \\frac{b^4}{256a^4} - \\frac{b^4}{64a^3} + \\frac{b^2c}{16a^3} - \\frac{bd}{4a^2} + \\frac{e}{a} \\]"},{"location":"polynomial.html#depressed-quartic","title":"depressed quartic","text":"\\[ x^4 + px^2 + qx + r = 0 \\] \\[ x^4 + px^2 = -qx - r \\] \\[ x^4 + 2px^2 + p^2 = px^2 - qx - r + p^2 \\] \\[ (x^2 + p)^2 = px^2 - qx - r + p^2 \\] \\[ (x^2 + p)^2 + 2(x^2 + p)z + z^2 = (x^2 + p + z)^2 = px^2 - qx - r + p^2 + 2zx^2 + 2pz + z^2 = (p + 2z)x^2 - qx + 2pz + z^2 - r + p^2 \\] <p>Now, define z so that the right side of this equation is a perfect square. But then a quadratic is of the form \\(c(x - ?)(x - ?)\\) or \\((\\something x + \\something)^2\\), so if it is a perfect square, than there is only one solution, which means that the discriminant equals zero, so</p> \\[ (-q)^2 - 4(p + 2z)(2pz + z^2 - r + p^2) = 0 \\] <p>expanding out, we get a cubic polynomial to solve for z. and once we do, we have that \\((x^2 + p + z)^2 = (\\something x + \\something)^2\\), so now we can solve for x with a quadratic</p> <p>yay! but we still have to derive the quadratic discriminant and the cubic formula in the next few chapters.</p>"},{"location":"polynomial.html#cubic","title":"cubic","text":"\\[ ax^3 + bx^2 + cx + d = 0 \\] \\[ x \\to x - \\frac{b}{3a} \\] \\[ a (x - \\frac{b}{3a})^3 + b (x - \\frac{b}{3a})^2 + c (x - \\frac{b}{3a}) + d = 0 \\] \\[ ax^3 - 3ax^2 \\frac{b}{3a} + 3ax \\frac{b^2}{9a^2} - a \\frac{b^3}{27a^3} + bx^2 - 2bx \\frac{b}{3a} + b \\frac{b^2}{9a^2} + cx - c \\frac{b}{3a} + d = 0 \\] \\[ ax^3 - bx^2 + \\frac{b^2x}{3a} - \\frac{b^3}{27a^2} + bx^2 - \\frac{2b^2x}{3a} + \\frac{b^3}{9a^2} + cx - \\frac{bc}{3a} + d = 0 \\] \\[ ax^3 - \\frac{b^2x}{3a} + \\frac{b^3}{6a^2} + cx - \\frac{bc}{3a} + d = 0 \\] \\[ (a)x^3 + (c - \\frac{b^2}{3a})x + (\\frac{b^3}{6a^2} - \\frac{bc}{3a} + d) = 0 \\] \\[ x^3 + (c - \\frac{b^2}{3a^2})x + (\\frac{b^3}{6a^3} - \\frac{bc}{3a^2} + \\frac{d}{a}) = 0 \\] \\[ p = c - \\frac{b^2}{3a^2} \\] \\[ q = \\frac{b^3}{6a^3} - \\frac{bc}{3a^2} + \\frac{d}{a} \\]"},{"location":"polynomial.html#depressed-cubic","title":"depressed cubic","text":"\\[ x^3 + px + q = 0 \\] \\[ x + y = z \\] \\[ z^3 = x^3 + 3x^2 y + 3x y^2 + y^3 = x^3 + 3xyz + y^3 \\] \\[ 3xyz = px \\] \\[ x^3 + 3xyz + q = 0 \\] \\[ x^3 + 3xyz + y^3 = y^3 - q \\] \\[ 3yz = p \\] \\[ z^3 = y^3 - q \\]"},{"location":"polynomial.html#y","title":"y","text":"\\[ z = \\frac{p}{3y} \\] \\[ y^3 - q = \\frac{p^3}{27y^3} \\] \\[ (y^3)^2 - qy^3 - (\\frac{p}{3})^3 = 0 \\]"},{"location":"polynomial.html#z","title":"z","text":"\\[ y = \\frac{p}{3z} \\] \\[ z^3 = \\frac{p^3}{27z^3} - q \\] \\[ (z^3)^2 + qz^3 - (\\frac{p}{3})^3 = 0 \\] <p>okay, so now we have to solve the quadratic, take the cube root, and subtract just to solve for x</p>"},{"location":"polynomial.html#quadratic","title":"quadratic","text":"<p>either this</p> <p>or that</p>"},{"location":"polynomial.html#_1","title":".","text":"\\[ ax^2 + bx + c = 0 \\] \\[ x^2 + \\frac{b}{a} x = x^2 + 2 \\frac{b}{2a} x = - \\frac{c}{a} \\] \\[ x^2 + 2 \\frac{b}{2a} x + \\frac{b^2}{4a^2} = (x + \\frac{b}{2a})^2 = \\frac{b^2}{4a^2} - \\frac{c}{a} = \\frac{b^2 - 4ac}{4a^2} \\] \\[ x + \\frac{b}{2a} = \\pm  = \\pm \\frac{\\sqrt{b^2 - 4ac}}{2a} \\] \\[ x = -\\frac{b}{2a} \\pm \\frac{\\sqrt{b^2 - 4ac}}{2a} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\] \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = -\\frac{b}{2a} \\pm \\sqrt{\\frac{b^2 - 4ac}{4a^2}} \\]"},{"location":"polynomial.html#depressed-cubic-again","title":"depressed cubic (again)","text":"\\[ y^3 = \\frac{q}{2} \\pm \\sqrt{\\frac{q^2 + 4(\\frac{p}{3})^3}{4}} = \\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3} \\] \\[ y = \\sqrt[3]{\\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} \\] \\[ z^3 = -\\frac{q}{2} \\pm \\sqrt{\\frac{q^2 + 4(\\frac{p}{3})^3}{4}} = -\\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3} \\] \\[ z = \\sqrt[3]{-\\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} \\] \\[ x = z - y = \\sqrt[3]{-\\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} - \\sqrt[3]{\\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} = \\sqrt[3]{-\\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} + \\sqrt[3]{-\\frac{q}{2} \\mp \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} \\] <p>but now, that \\(\\pm\\) is the only difference between \\(y\\) and \\(z\\), and if they are the same then \\(x\\) equals zero, and if you saw this mathologer video and stop at timestamp and 19:55, than you know that we might as well say that they are different because we only need one solution, if \\(x \\neq 0\\) than they have to be different. In all cases, one has to be \\(+\\) and one has to be \\(-\\), you can choose which but I will just put the plus one first. In total:</p> \\[ x = \\sqrt[3]{-\\frac{q}{2} + \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} + \\sqrt[3]{-\\frac{q}{2} - \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}} \\] <p>so just plug in for \\(p\\) and \\(q\\) and add \\(\\frac{b}{3a}\\) and you have the cubic formula!</p>"},{"location":"polynomial.html#z-quartic","title":"z (quartic)","text":"<p>So finally, after reading the whole page, you can understand the quadratic discriminant, (either that or you just skipped to this part) but probably not. So this discriminant is the \\(b^2 - 4ac\\) in the quadratic formula, because the square root of a positive number is positive and plus or minus a positive number has two solutions, the square root of a negative number is undefined (unless complex numbers) and plus or minus an undefined number has zero solutions, the square root zero is zero and plus or minus zero has one solution. In total, the sign of the \\(b^2 - 4ac\\) determines how many solutions exist, which is why it is called discriminant, so...</p> \\[ (-q)^2 - 4(p + 2z)(2pz + z^2 - r + p^2) = 0 \\] \\[ q^2 - 8p^2z - 16pz^2 - 4pz^2 - 8z^3 + 4pr + 8rz - 4p^3 - 8p^2z = 0 \\] \\[ (-8)z^3 + (-20p)z^2 + (8(r - 2p^2))z + (q^2 + 4pr - 4p^3) = 0 \\] \\[ (8)z^3 + (20p)z^2 + (16p^2 - 8r)z + (4p(p^2 - r) - q^2) = 0 \\] <p>solve for z! (not \\(\\Gamma\\) (\\(z + 1\\)))</p>"},{"location":"probability.html","title":"Probability","text":""},{"location":"probability.html#to-cheat-or-not-to-cheat","title":"to cheat, or not to cheat?","text":"<p>Let's start with a game! The game is that you flip a coin 10 times and then you add the total number of heads and you want the most heads total, this game is pretty popular in some hypothetical town where there is a rumor going around that some people are using (somehow) using a weighted coin that has a \\(75\\)% chance of landing heads, the notation is \\(\\frac{3}{4}\\) chance, because what else (other than \\(100\\)) are you going to take a percent of? Anyways, this town has nominated you to be the detective and test everyone with a test has all three of the properties:</p> <p>1, false accusation: accuse less than \\(5\\)% of fair players</p> <p>2, true accusation: accuse more than \\(80\\)% of cheaters</p> <p>3, make it short: do as little coin flips as possible because we don't have all day</p> <p>this is usually where I solve the puzzle, but now's the chance to solve it yourself using the tools in the next chapter</p>"},{"location":"probability.html#solution","title":"solution","text":""},{"location":"probability.html#binomial-coefficient","title":"binomial coefficient","text":"<p>do you remember that time when I proved the binomial theorem ? Well, I can count the number of heads and the number of tails and the total number of flips is always the same. For example, no matter if you get \\(1\\) heads and \\(4\\) tails, \\(3\\) heads and \\(2\\) tails, whatever it is, it should  add up to \\(5\\) flips total. The same happens with the probability, because if there always has to be \\(5\\) flips, then there could be \\(0\\) to \\(5\\) heads and \\(5\\) minus that tails, so if you add up the probability of each thing happening, then you should have to get \\(1\\) or \\(100\\)%</p> <p>even if I proved that:</p> \\[ (1 + x)^n = \\sum\\limits_{k = 0}^{\\infty} \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} x^k = \\sum\\limits_{k = 0}^{\\infty} \\frac{n! x^k}{k!(n - k)!} \\] <p>you can use a similar argument that:</p> \\[ (x + y)^n = \\sum\\limits_{k = 0}^{\\infty} \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} x^k y^{n - k} = \\sum\\limits_{k = 0}^{\\infty} \\frac{n! x^k y^{n - k}}{k!(n - k)!} \\] <p>so if I made \\(x\\) into the probability \\(p\\) of landing heads, made \\(y\\) into \\((1 - p)\\), and made \\(n\\) into the number of flips, than on the left, I would get \\(1^n\\), which is just \\(1\\), but on the right, knowing that multiplying probabilities is the probability that they happen in parallel (if independent!), I would get the sum of the probability of any number number of heads times the probability of getting tails \\(5\\) minus the total number of heads times (which is the total number of tails) times each orientation e.g.heads, tails, heads tails, heads is the same as heads, heads, heads, tails, tails. So this is taking the probability of getting some number of heads and tails, adding them up, and saying that it is \\(100\\)%, going one step back, each term is the probability of getting some number of heads which I will call \\(k\\), I will also swap \\(5\\) with \\(n\\).</p> <p>This seems pretty useful and what I did was took apart the binomial theorem and the probability is the probability of getting \\(k\\) heads and \\(n - k\\) tails and each orientation. I will write this probability below saying that there are \\(n\\) flips, \\(k\\) heads, and probability \\(p\\) of landing heads.</p> \\[ P(n, k, p) = \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} p^k (1 - p)^{n - k} \\]"},{"location":"probability.html#binomial-approximation-ish","title":"binomial approximation (ish)","text":"<p>Realistically, the test wouldn't test a specific number of heads or tails, so it would be something more like \\(7\\) or more heads out of \\(10\\) flips, you could say that it is the probability of \\(7\\) heads OR \\(8\\) heads OR \\(9\\) heads OR \\(10\\) heads, but for one, it is hard to define an \"OR\", for example if there are two coins \\(a\\) and \\(b\\), one has probability \\(a\\) of landing heads, one has probability \\(b\\) of landing heads, the probability of \\(a\\) AND \\(b\\) landing heads is simply \\(ab\\), but the probability of \\(a\\) OR \\(b\\) landing heads (or both!) is \\(1 - (1 - a) (1  - b)\\) and that is pretty easy to prove, anyways my point still stands! And I should not use that definition, because it is computationally intensive, hard to read, and most of all, it uses an \"I'll leave this as an exercise for the viewer\". Anyways, I am tired of writing text and just want to prove this new formula.</p> <p>So, now that you know how to manipulate probabilities, I won't use something like the binomial theorem and I will instead just build it from the ground up, so the factor of \\(p^k\\) sounds about right, but if I don't use the \\((1 - p)^{n - k}\\) term, than it wouldn't necessarily mean that the rest are tails, but what about the \\(n\\) choose \\(k\\) term? You might be asking (actually probably not, because I forgot to tell you what that orientation term is called a loooooooooong time ago) well, same as before, just replace the tails with question marks! Actually, as I am writing this, I realized that those question marks need an orientation too, except sometimes they don't if they are all the same then they don't and it is really confusing. Actually, as I was writing that, I realized, no actually I couldn't find a counter counter point, but what I did realize was that the \\(\\frac{n!}{k!(n - k)!}\\) was the thing that was going to change.</p> <p>There are sooooo many counter counter counter counter counter points, and I know that the solution is something something \\(1 - (1 - p)^n\\), so I will just go off of that. \\(1 - (1 - p)^n\\) Is a term that means the probability of \\(p\\) OR \\(p\\) OR \\(p\\) OR \\(p\\) OR \\(p \\dots\\) \\(n\\) times, so the probability of \\(1\\) or more heads out of \\(n\\) total flips, I just made this up, but to add \\(k\\) into the equation, replace \\(p\\) with \\(p^k\\), so now it is the probability of \\(p^k\\) or more heads (if this is wrong, than I will replace it later), anyways it's time to do the thing where I say \"Anyways, I am tired of writing text and just want to prove this new formula.\"</p> \\[ 1 - (1 - p^k)^n \\]"},{"location":"probability.html#_1","title":"Probability","text":"<p>It's been \\(2\\) months since I worked on this page. i'll accept that (hold on, I need to find it somewhere in this page)</p> \\[ 1 - \\prod\\limits_{i = k}^{n} (1 - \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} p^k (1 - p)^{n - k}) \\]"},{"location":"projective_geometry.html","title":"Projective geometry","text":"<p>Credit (even if it is very small): The two points that lie on every circle (???) #SoME3, Putting Algebraic Curves in Perspective, and Extraordinary Conics: The Most Difficult Math Problem I Ever Solved.</p> <p>Let's say that a point \\((a: b)\\) (as opposed to \\((a, b)\\)) is equal to \\((ca: cb)\\) \\((c \\ne 0)\\), so every* (and that's a big asterisk) point \\((a: b)\\) can be scaled onto \\((\\frac{a}{b}: 1)\\), a kind of number line.</p> <p>*unless \\(b = 0\\), then we add this kind of \"point at infinity\" to our number line (it's a single point because the point \\((a: 0)\\) can be scaled to \\((1: 0)\\) (aka the point at infinity), that is, of coarse, unless \\(a = 0\\), but that point isn't really aloud for the same reason as \\(\\frac{0}{0}\\)) making it the real projective line or \\(\u211d \\text{P}^1\\).</p> <p>The reason why it's at infinity is because, if you consider the point \\((1: 1)\\), it falls onto \\(1\\) on the number line, the point \\((1: \\frac{1}{2})\\) falls onto \\(2\\), the point \\((1: \\frac{1}{4})\\) falls onto \\(4\\), the point \\((1: \\frac{1}{8})\\) falls onto \\(8\\), and as the second number gets smaller, the point on the number line gets bigger approaching infinity, hence the name \"point at infinity\". But, if you instead do this from the other direction, it approaches negative infinity. You can imagine a number line that curves down as it goes along, consecutive integers getting closer and closer, and an unsigned infinity at the bottom where the line meets itself.</p> <p>Stepping a dimension up, you get the real projective plane or \\(\u211d \\text{P}^2\\), \\((a: b: c) = (da: db: dc)\\), most numbers going to \\((\\frac{a}{c}: \\frac{b}{c}: 1)\\), some becoming \\((\\frac{a}{b}: 1: 0)\\), less becoming \\((1: 0: 0)\\), the point at infinity becomes a line at infinity (more of a circle, but \\(1\\) degree of freedom, so it's a line), and the number line becomes a space of all points.</p> <p>There is a problem though (that is big enough to be explained on a line by itself), you could imagine the same process that I used to prove the unsigned infinity thing but in \\(\u211d \\text{P}^2\\) to get the unsurprising result of \\((a: b: 0) = (-a: -b: 0)\\). This does mean that, when drawing the regular or affine plane, and drawing a circle around it (to represent the line at infinity of coarse), if you wanted to draw, say, the point \\((1: 1: 0)\\), it would need to be at both the very top right, and the very bottom left of the circle.</p> <p>To see why this double counting thing makes sense, I'll project onto a unit sphere, so, if \\(r = \\sqrt{a^2 + b^2 + c^2}\\), the point \\((a: b: c)\\) maps to \\((\\frac{a}{r}: \\frac{b}{r}: \\frac{c}{r})\\). You might see the problem though, it also maps to \\((-\\frac{a}{r}: -\\frac{b}{r}: -\\frac{c}{r})\\), because it's also on the unit sphere. So, if you just consider the top half of the sphere (including the equator so that points at infinity are accounted for), it counts almost every point once, and points at infinity twice, kinda like the one where we projected onto the plane parallel to and one unit above the \\(xy\\) plane. So, to fix this problem, and give every point the same treatment, you (counterintuitively) count every point twice by using the entire sphere, kinda like giving every line in \\(2d\\) an angle instead of a slope to fix the vertical lines problem, at the cost of there being two angles for every line.</p> <p>Here's a desmos graph.</p> <p>Yes, I know, the plane is placed one unit below the sphere instead of one above, but it's only like that for the sake of demonstration.</p>"},{"location":"projective_geometry.html#homogenization","title":"homogenization","text":"<p>Homogenization is a method of interpolation from equations on the affine plane (non-projective plane) to equations on the projective plane (so, adding the line at infinity), but I think it would be better if I just showed how to do it.</p> <p>Let's say that I have these equations for describing my line:</p> \\[ y = mx + b \\] \\[ z = 1 \\] <p>so, we have this equation:</p> \\[ (x: y: z) = (x: mx + b: 1) \\] \\[ (x, y, z) = (cx, cmx + cb, c) \\] <p>and from those, I have this new equation for describing my line:</p> \\[ y = mx + bz \\] <p>Now, the equation is homogeneous \\(^1\\).</p> <p>In (this \\((x, y, z) = (cx, cmx + cb, c)\\)) equation for a line, \\(z\\) could not equal \\(0\\), but now, \\(z\\) can equal \\(0\\), and if \\(z = 0\\), then it's at the line at infinity, so these \\(z = 0\\) solutions snuck in as a result of homogenization, mission success!</p> <p>\\(^1\\) That is, a polynomial where each term has the same degree. There is a much easier way of doing this called homogenization: you take each term whose degree is not the max, and add factors of \\(z\\) to bring the degree up to the max.</p> <p>But what are these solutions at the line at infinity?</p> \\[ z = 0 \\] \\[ y = mx + bz \\] \\[ y = mx \\] \\[ (x: y: z) = (x: mx: 0) \\] \\[ (1: m: 0) \\] <p>This has some pretty cool implications, but I'll do that tomorrow.</p> <p>Oh, look, it's tomorrow, time to tell you the implications.</p> <p>Y'know how any two distinct points on the affine plane have a line through them? And how (almost) any two distinct lines on the affine plane have a point on both? That is, of course, unless the lines are parallel. Solution: homogenization. A homogenized line with slope \\(m\\) has the point \\((1: m: 0)\\) (and \\((0: 1: 0)\\) if the line is vertical). So, if two lines have the same slope \\(m\\) (and are distinct), then they don't meet normally, and they intersect at \\((1: m: 0)\\). If they have different slopes, then they do meet normally, and they don't intersect at the line at infinity. But what about the \"any two distinct points have a line through them\" rule? If you have a normal point and a point at infinity \\((1: m: 0)\\), they have the line with slope \\(m\\) going through the first one. But what if you have two points on the line at infinity? This (among other things) is why it's called the line at infinity, a line that all points at infinity lie on.</p> <p>Also, I'm gonna switch from \\(y = mx + b\\) to \\(ax + by + c = 0\\), to deal with vertical lines, and because I'm gonna use a variation of this to describe quadratics.</p>"},{"location":"projective_geometry.html#duality","title":"duality","text":"<p>It's hard to explain how points dual to lines, but an example would be that they are both defined by three numbers, they are considered the same if you scale all three by the same amount, throw an error if all three are zero, the origin and the line at infinity are duals, or on the sphere, the equator and the north and south poles (remember, two solutions). The more general definition would be something like this: the two points on a sphere, a point on the dual line, and the point \\(90\u00b0\\) away but still on the dual line are all mutually perpendicular. By the way, points on the plane project to antipodal points on the sphere, and lines on the plane project to great circles on the sphere.</p> <p>Also fun fact: the duals of every point on a line would all pass through the dual point, and the duals of every line that passes through a point would all lie on the dual line.</p> \\[ \\text{Formula: the dual of point } (a: b: c) \\text{ is the homogenized line } $ax + by + cz = 0$ \\text{.} \\]"},{"location":"projective_geometry.html#conics","title":"conics","text":"<p>surprisingly, every conic is a hyperbola or ellipse (circles are just perfect ellipses, and parabolas are right in between ellipses and hyperbolas), and each one has this form:</p> \\[ ax^2 + bxy + cy^2 + dx + ey + f = 0 \\] <p>(Not calculus \\(dx\\) and \\(e\\))</p> <p>Also, if you dual every point on a conic, they are all tangent to another conic, the dual conic.</p>"},{"location":"quadratic.html","title":"Quadratic","text":"<p>Welcome to the hidden page that is not listed on the home page or the dropdown menu. Anyways, let's do the</p>"},{"location":"quadratic.html#quadratic","title":"quadratic","text":"\\[ ax^2 + bx + c = (d) x^2 + (-d(s_1 + s_2)) x + (d s_1 s_2) = d(x - s_1)(x - s_2) \\] <p>now, I will say that if these are equal, than they have the same terms, so I will use the first to simplify, the second to solve for \\(i\\), and the third to solve for \\(k^2\\) (I'll explain later)</p> \\[ a = d \\] \\[ x^2 + \\frac{b}{a} x + \\frac{c}{a} = (x - s_1)(x - s_2) = x^2 + (-(s_1 + s_2)) x + (s_1 s_2) \\] \\[ s_1 = i + j \\] \\[ s_2 = i - j \\] \\[ as_1^2 + bs_1 + c = a(s_1 - s_1)(s_1 - s_2) = a0(s_1 - s_2) = 0 \\] \\[ as_2^2 + bs_2 + c = a(s_2 - s_1)(s_2 - s_2) = a(s_2 - s_1)0 = 0 \\] \\[ as_1^2 + bs_1 + c = 0 \\] \\[ as_2^2 + bs_2 + c = 0 \\] \\[ as^2 + bs + c = 0 \\] \\[ s = i \\pm j = i \\pm \\sqrt{j^2} = i + k \\] \\[ k^2 = j^2 \\] \\[ \\frac{b}{a} = -(s_1 + s_2) \\] \\[ \\frac{b}{2a} = -\\frac{s_1 + s_2}{2} \\] \\[ -\\frac{b}{2a} = \\frac{s_1 + s_2}{2} = i \\] \\[ i = -\\frac{b}{2a} \\] \\[ \\frac{c}{a} = s_1 s_2 = (i + j)(i - j) = i^2 - j^2 = \\frac{b^2}{4a^2} - j^2 \\] \\[ j^2 = \\frac{b^2}{4a^2} - \\frac{c}{a} = \\frac{b^2 - 4ac}{4a^2} \\] \\[ s = k - \\frac{b}{2a} \\] \\[ k^2 = \\frac{b^2 - 4ac}{4a^2} \\] \\[ s = -\\frac{b}{2a} \\pm \\sqrt{\\frac{b^2 - 4ac}{4a^2}} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\] <p>And time to continue like a multiple choice book. If you want to see the other more common proof of the quadratic formula, click here. But if you think you had enough quadratic for the day, click here</p>"},{"location":"rand.html","title":"Non-math","text":"<p>If you were wondering, the first line is always the title.</p> <p>.</p> <p>That one puzzle</p> \\[ \\frac{\\partial (x^2 + y^2)}{\\partial (x + y)} = ? \\] <p>Okay, fine, yes, that one was a joke. But it took me a whole weekend to solve it.</p> <p>Not that every day isn't a weekend for me, but I remember solving it on Sunday and posing it the day before.</p> <p>.</p> <p>I might not work today :(</p> <p>... Because there is a science something, what was it again? Physics festival! So I am sad to say that, today, I will break my at least \\(1\\) edit to my website per day rule... Wait! This text that I am writing counts as something, so I'll add this to my website. Anyways, I need to leave, and everyone is waiting for me.</p> <p>.</p> <p>Or today.</p> <p>... That title and this text will be what I add to my website today.</p> <p>.</p> <p>It's bitcoin halving day today!!!</p> <p>.</p> <p>How to solve a rubix cube</p> <p>I'll leave it to J perm.</p> <p>.</p> <p>.</p> <p>.</p> <p>Hyperbolic arcs</p> \\[ \\text{This line used to be a warning for the below one.} \\] \\[ \\text{This line used to be a GeoGebra file that would always load first even if you're trying to scroll down.} \\] <p>This used to be a page on the website until I deleted it. The page still exists and you can find it by replacing the <code>rand</code> in the URL link above with <code>hyperbolic</code>, I think you can see why I deleted it.</p> <p>.</p> <p>THE NUMBERS</p> \\[ | 1.0 | .01 \\text{ } .1 | 01.0 \\text{ } 1.0 \\text{ } 2.0 | .0011 \\text{ } .02 \\text{ } .1 \\text{ } .2 | 0.01 \\text{ } .0121 \\text{ } 1.0 \\text{ } .13 \\text{ } 3.0 | .001 \\text{ } 0.1 \\text{ } .03 \\text{ } .1 \\text{ } 2.0 \\text{ } .3 | 001.0 \\text{ } .010212 \\text{ } 0.2 \\text{ } 1.0 \\text{ } 13.0 \\text{ } .2 \\text{ } 4.0 | \\] \\[ \\text{To solve this puzzle, you must either find out what comes next, or find the pattern, or both!} \\] <p>.</p> <p>By the way...</p> \\[ \\text{I am trying to build a computer... Within a computer!} \\] <p>.</p> <p>There was a power outage today :(</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>One twig</p> <p>One twig is a unit of distance, but first, y'know how one meter is one light second (the amount light traveles in one second) divided by \\(299,792,458\\)? Well, I was thinking: can't I just make a unit of measurement defined as one light second divided by \\(300,000,000\\)? Well, that would be decimal-centric! (I'd rather use binary.) So, I defined one twig (or one absolute unit) as one light second divided by \\(2^{30}\\), so just under one foot, coming in at \\(10.99\\) inches. Using twig-ic (I just came up with that name) is a lot like using metric (I was too lazy to invent an imperial like system (even though I'm 'murican) because I could only invent one unit), I was gonna use a binary based systen, but octal was better as it is closer to \\(10\\) and is the amount of fingers on a cartoon character. If you divide a twig into \\(8\\), you get \\(1\\) unotwig, after that, \\(1\\) duotwig, then \\(1\\) triotwig, then \\(1\\) quadtwig, then \\(1\\) pentatwig, then \\(1\\) hexatwig, then \\(1\\) heptatwig, then \\(1\\) octotwig, then \\(1\\) nonatwig, then \\(1\\) decatwig, then \\(1\\) elatwig, but at that point, it is smaller than \\(1/10\\)'th of a nanometer, better stop there (also, the prefix is equal to the power of \\(8\\) that you are dividing it by). If you want to get bigger units, add the prefix \"nega\", but when you get to one negaelatwig, it's the size of \\(8\\) lightseconds. So yeah, that is the measurement system that I created last night.</p> <p>.</p> <p>Happy Father's day! (Weekend)</p> <p>.</p> <p>Happy Father's day! (Actual)</p> <p>.</p>"},{"location":"rand.html#vacation","title":"vacation!","text":"<p>update on working every day</p> <p>Because tomorrow starts a month-long vacation, I'm going to go from working every day to working \\(3\\) to \\(4\\) times per week. </p> <p>.</p> <p>\\(100\\) Lines.</p> <p>.</p> <p>vacation to h#suhr#x# has started!</p> <p>If you were wondering, this is one year after the strand puzzle and the arctan puzzle.</p> <p>.</p> <p>shoutout to Dr. Keivan!</p> <p>.</p> <p>I'm gonna see my grandparents!</p> <p>I've still been working every day.</p> <p>.</p> <p>false alarm, that's tomorrow.</p> <p>.</p> <p>I'm gonna see my grandparents!.. For real this time.</p> <p>.</p> <p>Goodbye grandparents.</p> <p>I've still been working every day.</p> <p>.</p> <p>new place today.</p> <p>.</p> <p>I'm gonna see my cousins!</p> <p>I've still been working every day.</p> <p>.</p> <p>By the way...</p> <p>The amount of commits between now and \\(500\\) is (well, I forgot \\(3\\) and \\(2\\)) \\(1\\)... \\(0!\\), this is the \\(500\\)'th commit to this branch (whatever that means).</p> <p>.</p> <p>Goodbye cousins. I'll see them next saturday</p> <p>.</p> <p>I'm going to l#w#do#b!</p> <p>.</p> <p>false alarm, that's... I dunno!</p> <p>.</p> <p>Now, I'm in i#u#co#c.</p> <p>.</p> <p>I'm gonna see my (other) cousins!</p> <p>.</p> <p>By the way...</p> <p>Illuminati</p> <p>Illumination studios.</p> <p>.</p> <p>Actually, yesterday was saturday.</p> <p>.</p> <p>This is the last full day in j#u#bm#c.</p> <p>.</p> <p>Goodbye uncle, \\(2\\) aunts, \\(4\\) cousins, and \\(2\\) grandparents.</p> <p>.</p> <p>Scratch that, I saw them yesterday.</p> <p>.</p> <p>Goin' to e#pw#a#nl.</p> <p>.</p> <p>\\(200\\) Lines.</p> <p>.</p> <p>the car's license plate said \"DNAX\".</p> <p>.</p> <p>Goin' back to texas.</p> <p>.</p> <p>scratch that, I went to atlanta, then to texas.</p> <p>.</p>"},{"location":"rand.html#end-of-vacation","title":"end of vacation.","text":"<p>.</p> <p>(insert work here)</p> <p>.</p> <p>April \\(1\\)'st</p> <p>Aha! I fooled you, by making you think that I fooled you.</p> <p>(P.S. it's not actually April \\(1\\)'st)</p> <p>.</p> <p>I have a fever :(</p> <p>.</p> <p>I no longer have a fever :)</p> <p>.</p> <p>By the way...</p> <p>I am trying to build a computer... Within a computer!.. Again!.. In python (I'll put it on the repo if/when I'm done).</p> <p>.</p> <p>union/labor day</p> <p>(Insert protest here)! (Insert protest here)! (Insert protest here)! (Insert protest here)! (Insert protest here)!</p> <p>.</p> <p>shoutout to Dr. Katz!</p> <p>.</p> <p>it's \\(11:25\\) ish, and I just hit commit. This was the closest call since, well, since this website was created on october \\(31\\)st \\(2023\\)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>By the way...</p> <p>Only ~ \\(2\\) weeks until halloween! (That was the day I created this website.)</p> <p>.</p> <p>i before e except after c...</p> <p>... Unless you're being one of eight weird beige foreign sovereign forfeiting weightlifting, either caffeine or budweiser drinking neighbors, running a feisty heist to seize the height of technology! (See original)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>I am making a slideshow for a talk on the lambda calculus in ~\\(1\\) week.</p> <p>.</p>"},{"location":"rand.html#road-trip","title":"road trip!","text":"<p>First stop: my sister</p> <p>.</p> <p>leaving in \\(15\\) minutes</p> <p>.</p> <p>\\(300\\) Lines.</p> <p>.</p> <p>We're now at our next stop: new orleans.</p> <p>.</p> <p>Next stop: the beach!</p> <p>.</p> <p>Actually, I went to mississippi, then alabama, then florida, then a restauraunt.</p> <p>.</p> <p>We are back in alabama, and we are finaly going to tabconf.</p> <p>.</p> <p>False alarm, that's today. Plus, tabconf is in georgia</p> <p>.</p> <p>I'm doing my first talk in \\(45\\) (ish) minutes</p> <p>.</p> <p>The talk did good, and here it is: that is, when it comes out.</p> <p>.</p> <p>It's the next day and I'm doing my other talk in \\(3\\) (ish) hours</p> <p>.</p> <p>It did good and shorter then expected. (I'll put in the link when the video comes out.)</p> <p>.</p> <p>Announcement!</p> <p>It's the next day again and, because both the talk that I've been waiting \\(1\\) year for (the second one) and the one I've been spending all my free time on for the last week (the first one), I can now come back to working on my website instead of doing daily edits to this page. One of the things that I've wanted to do but can't is set theory.</p> <p>.</p> <p>We are back in alabama</p> <p>.</p> <p>We are going back to florida</p> <p>.</p> <p>We are back in alabama</p> <p>.</p> <p>Next stop: new orleans!</p> <p>.</p> <p>We are heading back on Oct \\(30\\) (today)</p> <p>.</p> <p>yesterday's commit was \\(18\\) mins after midnight</p>"},{"location":"rand.html#end-of-road-trip","title":"end of road trip.","text":"<p>Happy Halloween!</p> <p>Ok, I'm gonna do this first thing in the morning (after texting my dad), I've been planning this since Oct \\(17\\)th (ish). The amount of commits to this website (after I hit commit) between now and \\(800\\) is \\(3\\)... \\(2\\)... \\(1\\)... \\(0\\)! This will be the \\(800\\)th commit on the one year anniversery of my website.</p> <p>.</p> <p>I'm trick or treating in ~\\(1H\\)</p> <p>.</p> <p>The new president of the USA will be (and I'm typing this before I actually know)...</p> <p>Donald Trump! He's gonna be the president of the USA for around \\(8\\) years of my life.</p> <p>.</p> <p>You might've noticed that...</p> <p>I've been working a lot less recently, but I'm gonna try to work every day for all of december and (most of) \\(2025\\)</p> <p>.</p> <p>Happy thanksgiving!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>\\(400\\) Lines.</p> <p>.</p> <p>Happy friday the \\(13\\)th!</p> <p>.</p> <p>I'm \\(11\\) now.</p> <p>.</p> <p>It's my birthday now.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Decorating the tree</p> <p>.</p> <p>I wish you a Merry Christmas eve!.. Eve!</p> <p>.</p> <p>I wish you a Merry Christmas eve!</p> <p>.</p> <p>I wish you a Merry Christmas...</p> <p>.</p> <p>... And a happy New Year!</p> <p>.</p> <p>New Year's resolutions</p> <p>New Year's resolutions I'll make this year:</p> <p>None.</p> <p>New Year's resolutions I made last year:</p> <p>None.</p> <p>New Year's resolutions I made last year of which I have completed:</p> <p>None, so technically all of them.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Here's an analogy for the group \\(\u2124_2\\):</p> ...my friend is... ...my enemy is... The friend of... ...my friend ...my enemy The enemy of... ...my enemy ...my friend <p>.</p> <p>(insert work here)</p> <p>.</p> <p>It's snowing today!</p> <p>Last time it snowed it didn't snow for very long and it didn't really stay. That was \\(2\\) years ago. The last time it snowed before that (where it was actually a place I lived) was about \\(6\\) years ago when I was \\(5\\).</p> <p>.</p> <p>The snow has melted :(</p> <p>.</p> <p>\\(500\\) Lines.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p>"},{"location":"rand.html#a-play-that-me-and-my-friends-made","title":"a play that me and my friends made","text":"<p>I was at a sports party the other day with my sister and some of my friends, and then we decided to make a play together.</p> <p>I don't remember much about the game, I think it was about Kentucky Fried Chicken vs the golden ratio (KC vs PHI)</p> <p>I will refer to everyone involved as me, my sister, my friend (male), my friend (female) 6-year-old #1, and 6-year-old #2.</p> <p>We finished the script for act \\(1\\) scene \\(1\\), but then we are given a 2-minute warning, so we had to improv the rest.</p> <p>We were planning on just showing it to my dad, but all of the adults ended up showing up, and we started.</p> <p>Now that I actually have to recording, let's start with...</p> \\[ \\text{Act } 1 \\text{ Scene } 1 \\] <p>Actors: me and my sister as the parents, the two 6-year-olds as the kids.</p> <p>6-year-old #1: \"Why do we have to go to school? Last time we went to school we had to fight a bear!\"</p> <p>6-year-old #2: \"Plus, it's so windy outside that the wind will pick us up!\"</p> <p>My sister: \"Oh, stop it you! You can stop going to school when you turn \\(18\\) or dinosaurs fly!\"</p> <p>My friend (male): * Throws a dinosaur plush across the backyard *</p> <p>Me: \"Ignore that.\"</p> <p>My sister: \"Well, do you want to know how much it is you need to go to school?\"</p> <p>6-year-old #1: \"Yeah!\"</p> <p>My sister: \"Well, if you don't go to school, you won't get a job, and if you don't get a job, you'll die alone in a ditch!\"</p> <p>Me: * Loudly whispers to her: \"this wasn't in the script!\" *</p> <p>My sister: \"You know what we had to do to get to school? Well, let us show you.\"</p> <p>At this point I went into the audience because I didn't know what their plan was and I didn't think I was involved. But no one was expected what happened next.</p> \\[ \\text{Act } 1 \\text{ Scene } 2 \\] <p>Actors: my friend (male) and my friend (female) as the past versions of the parents.</p> <p>My friend (female): \"I'm not doing that pop test no matter what happens!\"</p> <p>My friend (male): \"Yes.\"</p> <p>\\(600\\) Lines.</p> <p>My friend (female): \"Oh, my god, I've fallen over.\"</p> <p>My friend (male): * Helps her up *</p> <p>My friend (female): \"Tell grandpa I've left the purple toquitos in the freezer!\"</p> <p>*Pause *</p> <p>My friend (female): \"No, I will not march for the queen, I am my own woman!\" She said while marching.</p> <p>*Pause *</p> <p>My friend (male): * Starts pulling her by the arm *</p> <p>My friend (female): \"You're not taking me anywhere, you * unintelligible *!\".</p> <p>*Pause *</p> <p>My friend (female): \"I don't take orders from goldfish!\".</p> <p>My friend (female): * Falls over *</p> <p>My friend (male): * Starts pulling her by the arm again *</p> <p>My friend (male): \"Do you have anything to say to say to us!?\"</p> \\[ \\text{Act } 1 \\text{ Scene } 3 \\] <p>New actor: my sister as the school nurse.</p> <p>My sister: \"What is it?\"</p> <p>My friend (male): \"Am I a mushroom, * unintelligible *!?\"</p> <p>My sister: \"All right, as the school nurse-\"</p> <p>My friend (female): \"Do you know any lava personally\" (?).</p> <p>My sister: \"As I was saying: as the school nurse, I think we need to call \\(911\\) for her.\"</p> <p>My friend (male): * Starts calling \\(911\\) *</p> <p>My friend (female): * Screams and runs away *</p> <p>My sister: \"Stop her!\"</p> <p>My friend (male): * Grabs her by the hand and she starts spinning around him. *</p> <p>My sister: * Grabs her by the other hand as she comes to a stop *</p> \\[ \\text{Act } 1 \\text{ Scene } 4 \\] <p>My friend (female): \"I will not be contained, I am a dinosaur!\"</p> <p>My sister: * Pulls her by the hand and says: \"Sit down.\" *</p> <p>My sister: \"As a dinosaur, it is important for you to rest.\"</p> <p>My friend (male) quietly: \"No, it's not.\"</p> <p>My friend (female): \"I am a dinosaur!\"</p> <p>My friend (male): \"I am going to * unintelligible *!?\"</p> <p>My sister: \"Okay so both of you are a dinosaur.\"</p> <p>My friend (male): \"Well, no, I'm not a-\"</p> <p>My sister: \"What happened to you being a dinosaur? Are you now a panda?\"</p> <p>My friend (male): \"Hmm. Both!\"</p> <p>My sister: \"So you're a dina-panda.\"</p> <p>My friend (male): \"There's no such thing as a-\"</p> <p>My sister: \"There is now.\"</p> <p>My friend (female): \"* unintelligible * !\"</p> <p>My friend (male) and my friend (female): * Start fighting for no reason *</p> <p>My friend (male): \" unintelligible !\"</p> <p>My sister: \"Oh, gosh!\"</p> <p>My friend (male) and my friend (female): * Stop fighting, cross their arms, and start arguing but are interupted by my sister *</p> <p>My sister: * Stands up *</p> <p>My sister: \"You need to go to class.\"</p> <p>My friend (female): \"No, we're not going to class no matter what! We have a pop test today, and y\u2019know, I don't do well with math, so-\"</p> <p>My sister: \"Neither do I.\"</p> <p>My friend (female): * Walks up closer to her *</p> <p>\\(700\\) Lines.</p> <p>My friend (female): \"Neither do I.\"</p> <p>My sister: \"Good.\"</p> <p>*Pause *</p> <p>My friend (female): \"Okay, fine, I'm going to-\"</p> <p>6-year-old #1: * Throws alligator plush at my friend (male)'s face *</p> \\[ \\text{Act } 2 \\] <p>My friend (male): * Starts screaming *</p> <p>My friend (male): \"Get it off!\"</p> <p>My friend (male): * Collapses on the floor *</p> <p>Simultaneously...</p> <p>Alligator: * Latches onto my friend (female)'s hand *</p> <p>My friend (female): \"Get it off me!\"</p> <p>Alligator: * Falls of *</p> <p>My friend (female): * Turns around and sees my friend (male) on the floor *</p> <p>My friend (female): \"Oh, he's dead!\"</p> <p>New actors: 6-year-olds as the police that finally showed up.</p> <p>6-year-olds: * See someone dead on the floor *</p> <p>6-year-olds: * Start doing CPR on him by pushing on his stomach *</p> <p>6-year-old #1: * Leaves *</p> <p>6-year-old #2: * Continues *</p> <p>My friend (male): * Gets up *</p> \\[ \\text{Act } 3 \\] <p>Actors: my friend (male) and my friend (female) as the parents, the two 6-year-olds as the kids.</p> <p>My friend (female) to my friend (male): \"I still can't believe that they aren't going to school!\"</p> <p>6-year-old #1: \"We don't even have school!\"</p> <p>My friend (female): \"Yes you do!\"</p> <p>*Both parents leave *</p> <p>New actor: my sister as probably the older sister.</p> <p>My sister: * Enters *</p> <p>My sister: \"No, summer break hasn't ended yet.\"</p> <p>6-year-old #1: \"When is it gonna be school then?\"</p> <p>My sister: \"Well, it'll be school next week.\"</p> <p>6-year-old #1: \"Yeees!\"</p> <p>6-year-old #2: \"I don't like school.\"</p> <p>6-year-old #1: \"Me neither.\"</p> <p>My friend (female): * Enters *</p> <p>My friend (female): \"Too bad, cuz you have to go, or else the purple toquitos will be soggy!\"</p> <p>*Everyone starts arguing *</p> <p>The end!</p> <p>And then most of us lined up and we all bowed at different times, except for me, I didn't bow.</p>"},{"location":"rand.html#end-of-play","title":"end of play.","text":"<p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Happy Valentine's Day!</p> <p>.</p> <p>\\(800\\) Lines.</p> <p>I think I'm still too young for this. More chocolates for me then!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Why was \\(6\\) afraid of \\(7\\)?</p> <p>Because \\(7\\) ate \\(9\\)</p> <p>But why did \\(7\\) eat \\(9\\)?</p> <p>Because you have to eat \\(3\\) square meals a day.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>I'm sick today :(</p> <p>.</p> <p>A thousand commits have been done, this will be the \\(1001\\)'st</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>I was thinking...</p> <p>If ur mom jokes are about how fat she is, then ur dad jokes should be about how fast he left / how he never got back with the milk.</p> <p>My mom came up with this idea because she was tired of hearing ur mom jokes between me and my sister.</p> <p>Here's one that my sister came up with:</p> <p>The reason why you look into the sky when you miss someone is because ur dad reached escape velocity when he left.</p> <p>.</p> <p>\\(900\\) Lines.</p> <p>.</p> <p>Have a happy- sorry, a sad and existential ash Wednesday.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Happy Rodeo!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Happy Pi Day!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Happy late St. Patrick's Day!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p>"},{"location":"rand.html#the-craziest-work-week","title":"the craziest work week","text":"<p>If you think that you had the craziest work week or busiest schedule, think again! I'll try to add all of these after Monday on the corresponding day. You'll figure out why this is a work week on the \\(7\\)th day. Here's one I found in a book:</p> \\[ \\text{Sunday} \\] <p>Genesis 1:1</p> <p>\\(1000\\) Lines, wow.</p> <p>First, God created the heavens (up to interpretation) and the earth.</p> <p>Genesis 1:2</p> <p>But the Earth was still empty and didn't really have anything interesting on it.</p> <p>Genesis 1:3</p> <p>And then God created light.</p> <p>Genesis 1:4</p> <p>God saw that the light was good, and separated it from the darkness.</p> <p>Genesis 1:5</p> <p>God called the light \"day\" and the darkness \"night\". This concludes the first day.</p> \\[ \\text{Monday} \\] <p>Genesis 1:6</p> <p>And then God said \"let there be an expanse to separate the water from the water\".</p> <p>Genesis 1:7</p> <p>And then just that happened, God created an expanse to separate the water above it (probably rain) from the water below it (ocean).</p> <p>Genesis 1:8</p> <p>God called the expanse \"sky\". This concludes the second day.</p> \\[ \\text{Tuesday} \\] <p>Genesis 1:9</p> <p>But the earth still pretty much looked like a ball of water, so God gathered the water revealing dry ground.</p> <p>Genesis 1:10</p> <p>God called the dry ground \"land\" and the gathered waters he called \"seas\", and God saw that it was good.</p> <p>Genesis 1:11</p> <p>And then, on the land, God created vegetation, i.e. seed bearing plants, trees that produce fruit, etc.</p> <p>Genesis 1:12</p> <p>And then just that happened, and God saw that it was good.</p> <p>Genesis 1:13</p> <p>This concludes the third day.</p> \\[ \\text{Wednesday} \\] <p>Genesis 1:14</p> <p>And then God said \"Let there be lights in the sky to be able to tell day, night, days, weeks, seasons, years, etc...\".</p> <p>Genesis 1:15</p> <p>And then God said \"...Also, they should glow\".</p> <p>Genesis 1:16</p> <p>And then God made just that, two greater lights for the day and night respectively, and and aslo the stars.</p> <p>Genesis 1:17</p> <p>And God set them in the sky to give light on the earth...</p> <p>Genesis 1:18</p> <p>...To tell day from night, large time frames, etc.</p> <p>Genesis 1:19</p> <p>This concludes the fourth day.</p> \\[ \\text{Thursday} \\] <p>Genesis 1:20</p> <p>And then God said \"Let there be fishes in the water and birds in the air\".</p> <p>Genesis 1:21</p> <p>And then God made just that, and God saw that it was good.</p> <p>Genesis 1:22</p> <p>And then God blessed them and told them to be fruitful and increase in numbers.</p> <p>Genesis 1:23</p> <p>This concludes the fifth day.</p> <p>\\(1100\\) Lines.</p> \\[ \\text{Friday} \\] <p>Genesis 1:24</p> <p>And then God said \"Let there be livestock, wild animals, and other such creatures that move across the ground\".</p> <p>Genesis 1:25</p> <p>And then God made just that, and God saw that it was good.</p> <p>Genesis 1:26</p> <p>And then God said \"Let us make man in our own image, in our own likeness, and let them rule over all of the earth\".</p> <p>Genesis 1:27</p> <p>And then God made just that, humans male and female.</p> <p>Genesis 1:28</p> <p>And then God told them that they have permission to rule over the Earth.</p> <p>Genesis 1:29</p> <p>And that they can eat plants and fruit and stuff.</p> <p>Genesis 1:30</p> <p>And that God also gave the same permission to animals to eat plants and fruit.</p> <p>Genesis 1:31</p> <p>And then God looked out at everything he had created in the past \\(6\\) days, and God saw that it was very good. This concludes the sixth day (and chapter \\(1\\)).</p> \\[ \\text{Saturday} \\] <p>Genesis 2:1</p> <p>Now the Earth was finally complete.</p> <p>Genesis 2:2</p> <p>And then God finally took his weekend after all the work he'd been doing.</p> <p>Genesis 2:3</p> <p>And then God blessed saturdays (or really just days of rest) and made them holy.</p>"},{"location":"rand.html#end-of-the-craziest-work-week","title":"end of the craziest work week.","text":"<p>.</p> <p>(insert work here)</p> <p>.</p> <p>The brainstorm page is officially turning \\(1\\) year old!</p> <p>How do I know this? Well, partially GitHub commit history, but also because of a tradition / festival that my family celebrates once a year that happened to coincide with eclipse day last year. According to my memories, during this, I created the brainstorm page. The same thing just happened a couple of days ago (it always happens on the weekend), so that's how I know that the page is turning \\(1\\).</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>\\(1200\\) Lines.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Happy late Maundy Thursday and non-late Good Friday!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Happy Easter!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>\\(1234\\) Lines.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>May the \\(4\\)th...</p> <p>Just finished a new hope.</p> <p>.</p> <p>Day one of road trip my family left in the morning and I'm currently writing this in the night because we were busy packing up the computer.</p> <p>.</p> <p>Day two of road trip and there is nothing worth of note (PS we had ice cream).</p> <p>.</p> <p>Day three and my family finally got to see a Bitcoin related thing. The main thing at the party (other than just talking to people) was there was a pool (known as the mempool) and you had to get as many beach balls as you can onto your side of the pool. Whichever team wins gets to move on to the next round. My family got second place.</p> <p>I was able to explain to one person how to make an 8-bit computer. I explained to someone else what an option is and he said it was the best explanation of the Black Scholes equation he's ever heard (?). And I was able to explain how cubic equations relate to the equation \\((x + y)^3\\) and how they relate to a certain puzzle that my family makes.</p> <p>.</p> <p>Day four and we're already leaving.</p> <p>.</p> <p>Actually, on day four we went to btc++ which was an entire bitcoin conference, and because it happened yesterday I don't even remember what the most interesting thing that happened there was, and I was not informed by my family that we were going there until after I wrote down the message, and the next time I got to use a computer was now.</p> <p>.</p> <p>\\(1300\\) Lines.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Happy Mother's day!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Goin' to the beach!</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Plane flight today.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>Yesterday, among other things, me and my family went to the Valley of Fire.</p> <p>.</p> <p>Yesterday, among other things, me, my mom, and my sister went to Omega Mart, who are making mysterious merchandise, surreptitiously supplementing some supplies using unnoticed unnatural unprecedented Supplements S surely.</p> <p>PS It took me all morning to think of that.</p> <p>.</p> <p>Yesterday, Bitcoin conference.</p> <p>.</p> <p>leaving today.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>\\(1400\\) Lines.</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>I'm sick today :(</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p> <p>(insert work here)</p> <p>.</p>"},{"location":"rand.html#vacation_1","title":"vacation!","text":"<p>Today, me and my family are going to Puery. This is \\(1\\) year after and in the same location as the vacation at the start of the page, the lambda calculus, and binary, and this is \\(2\\) years after I did the strand puzzle and the arctan puzzle. Each line will be written a day after the previous line. You may see the correct pronunciations at the bottom of the chapter. Also, happy (late) Father's Day!</p> <p>Yesterday and today kind of Blended together, but in that time, me and my family went to the city of Syrap in Snarf, then the city of Kinwym in Enumerj where I am writing this.</p> <p>It's the next day again and I am writing this in the city of Garp in Uycech.</p> <p>Bitcoin bar today.</p> <p>Bitcoin conference day \\(1\\).</p> <p>Bitcoin conference day \\(2\\) and summer solstice.</p> <p>I have to write something down for today.</p> <p>Enamel painting today.</p> <p>Road trip!</p> <p>I'm now in the city of Znil in Uyrtso.</p> <p>I'm now back in Kinwym.</p> <p>I'm now in the city of Togtutsh in Enumerj.</p> <p>Happy Tau Day! \\(\\tau &gt; \\pi\\) and I missed Tau Day for the last \\(2\\) years in a row.</p> <p>I'm now in an airbnb in Snarf.</p> <p>Monastery day \\(1\\).</p> <p>Monastery day \\(2\\) and I'm writing this in a McDonald's at \\(3\\) pm because the monastery didn't have internet.</p> <p>Monastery day \\(3\\) and I'm writing this in a book store at \\(2\\) pm.</p> <p>Monastery Day \\(4\\) and I'm writing this on my dad's phone.</p> <p>Monastery day \\(5\\), \\(4\\)'th of July, and I'm writing this in a McDonald's at \\(3\\) pm.</p> <p>Monastery day \\(6\\) and I finally got the internet to work.</p> <p>I'm now in another airbnb in Snarf, this time in Yazel. High ropes / zipline course today.</p> <p>Vineyards today. \\(1500\\) Lines.</p> <p>I'm now in another airbnb in Snarf, this time in Ske, which is the only place that I knew we were going to ahead of time and where my dad's parents used to live.</p> <p>Beach today and I just rewatched  The ALMOST Platonic Solids and  The Sacred Geometry of Tilings while above a hexagonal tiling (also, cool chanel).</p> <p>Beach today on the other side of the lake in Yasrom and extended family tomorrow.</p> <p>Today, me and my family drove through the city-state of Okanam. I'm writing this in a hotel in a city I can't pronounce in Eluti.\u00a0Today I am meeting my mom's older sister's family and my mom's younger brother's family minus my mom's older sister's husband.</p> <p>Today, I lectured my dad on counting systems like the complex numbers, split complex numbers, and dual complex numbers, why you never hear about \\(3\\)d numbers, and how this all relates to ring theory and geometric algebra. Maybe I should make a page about that. Then I ate ice cream in Onawol with the rest of the (extended) family.</p> <p>Beach today. Me and the bros (the three male grandchildren of my mom's parents) dug a hole and then buried my mom's younger brothers son.</p> <p>Waterpark today.</p> <p>Beach today. Me and the bros dug a hole and then buried my mom's older sisters son.</p> <p>Different beach today.</p> <p>Same beach as yesterday and last day of extended family today.</p> <p>I'm now in a hotel in Nalim in Eluti.</p> <p>Today me and my family triped through Omok in Eluti, then Onagul in Dnilerztiws, then Sudav in Nitshnitil, and finaly Mlu in Enumerj where we stayed for the night.</p> <p>I'm now in a hotel in Kerbsfjov in Enumerj.</p> <p>I'm seeing my grandparents today.</p> <p>Same thing as yesterday today.</p> <p>Today I'm meeting with my mom's older sister and her kids.</p> <p>Special day today.</p> <p>Last full day in Kerbsfjov.</p> <p>Leaving Kerbsfjov today.</p> <p>Leaving Puery today.</p> <p>I have now arrived safely back at home the following day. The car's license plate said \"WI HE\".</p>"},{"location":"rand.html#pronunciations","title":"pronunciations","text":"<p>Puery - puh er y</p> <p>Snarf - s n air f</p> <p>Enumerj - ee num er j</p> <p>Uycech - uh ee cech</p> <p>Uyrtso - uh ee er t s ah</p> <p>Eluti - ee luh tih</p> <p>Dnilerztiws - d n ill er z t ih w s</p> <p>Nitshnitil - night sh knit hill</p> <p>Syrap - sih rap</p> <p>Kinwym - kin w ee m</p> <p>Garp - gah r p</p> <p>Znil - z nil</p> <p>Togtutsh - t ah g t * t sh</p> <p>Yazel - yazel</p> <p>Ske - s k eh</p> <p>Yasrom - yes rah m</p> <p>Okanam - oh kuh nom</p> <p>Onawol - oh nah wool</p> <p>Nalim - nah limb</p> <p>Omok - oh m oh k</p> <p>Onagul - oh nah ghoul</p> <p>Sudav - sue d ah v</p> <p>Mlu - m l **</p> <p>Kerbsfjov = curb s f y *** v</p> <ul> <li>= oo as in look or book or nook</li> </ul> <p>** = ow as in bowl or o as in fold</p> <p>\\(1600\\) Lines.</p> <p>*** = the thing before the y sound in the oi in voice or koi fish or oy in boy or toy.</p>"},{"location":"rand.html#end-of-vacation_1","title":"end of vacation.","text":"<p>.</p> <p>I hope you have a good Thursday!</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday!</p> <p>.</p> <p>I hope you have a good Friday and, during the vacation, there have been three new subjects that I was very interested in: Ordinal Numbers while I was in the monastery, \\(3d\\) Geometry while I was in the Ske and near the end in Kerbsfjov, and Ring Theory while I was in Eluti.</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday!</p> <p>.</p> <p>I hope you have a good Friday and today I am going to try to \\(3d\\) model the following Platonic and Archimedean solids: cube, octahedron, truncated cube, truncated octahedron, cuboctahedron, great rhombicuboctahedron, rhombicuboctahedron, and snub cube.</p> <p>P.S. yes, these are just the ones with octahedral symmetry.</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday and Hollow Knight Silksong Release Date!</p> <p>.</p> <p>\\(1700\\) Lines.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday and my tooth finaly fell out.</p> <p>.</p> <p>I hope you have a good Tuesday and today I am going to audit Dr. Katz's class!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday! Only \\(1\\) week until Hollow Knight Silksong and today I am going to Dr. Katz's class again!</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday and labor day! Only \\(3\\) days until Hollow Knight Silksong!</p> <p>.</p> <p>I hope you have a good Tuesday and today I am going to Dr. Katz's class for the third time! Only \\(2\\) days until Hollow Knight Silksong!</p> <p>.</p> <p>I hope you have a good Wednesday! Silksong tomorrow!</p> <p>.</p> <p>SILKSONG IS OUT!!!!!! That is one exclamation point per year the community has been waiting for this! Also, today I am going to Dr. Katz's class for the fourth time! Also, it's the \\(1234\\)th commit. Also, I hope you have a good Thursday! Also, it's exam day.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday! \\(1776\\) Lines.</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday! Also, it's exam day.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday! \\(1800\\) Lines.</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday! Also, it's exam day and a special day today.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday! Also, it's exam day.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday! It's a special day today.</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday and October!</p> <p>.</p> <p>I hope you have a good Thursday! Also, it's exam day.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday! Also, it's MIDTERM exam day.</p> <p>.</p> <p>I hope you have a good Thursday!</p> <p>.</p> <p>I hope you have a good Friday! \\(1900\\) Lines.</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday! Also, it's NOT exam day.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday! It's a special day today.</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday! Also, it's exam day.</p> <p>.</p> <p>I hope you have a good Friday!</p> <p>.</p> <p>I hope you have a good Saturday!</p> <p>.</p> <p>I hope you have a good Sunday!</p> <p>.</p> <p>I hope you have a good Monday!</p> <p>.</p> <p>I hope you have a good Tuesday!</p> <p>.</p> <p>I hope you have a good Wednesday!</p> <p>.</p> <p>I hope you have a good Thursday! Also, it's exam day.</p> <p>.</p> <p>I hope you have a good Friday and a Happy Halloween!</p>"},{"location":"set_theory.html","title":"Set theory","text":""},{"location":"set_theory.html#what-even-is-a-set","title":"What even is a set?","text":"<p>A set is a well defined collection of objects, a set could contain the two shoes on you feet, or the \\(5\\) pieces of cheese on this cutting board (that I'm going to pretend exists), sets can even contain other sets, but sets can not contain themselves, because it would lead to a paradox: would the set that contains every set that doesn't contain itself contain itself?\" This also means that there isn't a set that contains everything.</p> <p>But the thing is, using some symbols, you can describe almost all of math. These symbols can just be pronounced as words, and it would make a sentence, such as \" \\(\u00ac \\exists (x): |x| &lt; 0\\) \" as \" there does not exist \\(x\\) such that the absolute value of \\(x\\) is strictly less than \\(0\\) \". Time to rapidfire through each one's pronunciation and meaning.</p>"},{"location":"set_theory.html#meanings-of-things","title":"meanings of things","text":"\\[ \u2200 \\text{ Is pronounced \"for any\" or \"for all\" (but I prefer \"for any\") and means what it says. It than has an open parentheses, a thing (or sometimes multiple things separated by a comma) (} x, y, z, \\text{ or a set) that I will call } x \\text{ for now, a closed parentheses (parenthese is not a word), a } \\cdot \\text{, a statement that implies something about } x \\text{, a colon, and finish it off with a statement including } x. \\] \\[ \\text{left and right parentheses are not pronounced.} \\] \\[ \\cdot \\text{ Is pronounced \"such that\" and it's only used in two contexts: \"for any } x \\text{ such that...\" and \"there exists } x \\text{ such that...\".} \\] \\[ : \\text{ Is pronounced however a colon is pronounced.} \\] \\[ , \\text{ Is pronounced however a comma is pronounced.} \\] \\[ \\exists \\text{ Is pronounced \"there exists\" and I don't think I need to explain that.} \\] \\[ \u00ac \\text{ Is pronounced \"is not\" or \"does not\" as in \"there does not exist } x \\text{\".} \\] \\[ \\in \\text{ Is pronounced \"is an element of\" where an element of a set is a singular object that is contained in that set.} \\] \\[ x, y, \\text{ And } z \\text{ are pronounced \"} x, y, \\text{ And } z \\text{\" and they all mean \"a thing that could be an element of a set\".} \\] \\[ \\text{capital letters are sets.} \\] \\[ \\iff \\text{ Is pronounced \"if and only if\" as in \"if statement } a \\text{ is true, statement } b \\text{ is true, and if statement } a \\text{ is false, statement } b \\text{ is false\".} \\] \\[ \u2229 \\text{ Is pronounced \"and\" and means \"} a \u2229 b \\text{ is true if and only if statement } a \\text{ is true and } b \\text{ is true\", it can also mean the intersection of two sets, in that case, it is pronounced \"intersectioned with\", but I'll get to its formal meaning in the next chapter.} \\] \\[ = : \\text{ Is pronounced \"equals by definition\" and means \"define the thing on the left as the thing on the right\", or was it the other way around?} \\] \\[ \u2228 \\text{ Is pronounced \"or\" and means \"} a \u2228 b \\text{ is true if statement } a \\text{ is true or } b \\text{ is true... Or both!\", it can also mean the union of two sets, in that case, it is pronounced \"unioned with\", but I'll get to its formal meaning in the next chapter.} \\] \\[ \\text{succ Is pronounced \"the immediate successor of\" and means \"that number } + 1 \\text{\".} \\] \\[ \u2192 \\text{ Is technically called the if then sign, but it is pronounced \"implies\" and means \"statement } a \u2192 b \\text{ is true if statement } a \\text{ being true implies statement } b \\text{ is true\", so } a \u2192 b \\text{ is true if statement } a \\text{ is true and statement } b \\text{ is true, or if statement } a \\text{ is false and statement } b \\text{ is false, or if statement } a \\text{ is true and statement } b \\text{ is false, but not if statement } a \\text{ is false and statement } b \\text{ is true. Also, if there was an element sign two spaces behind, pronounce it \"being an element of\" as opposed to  \"is an element of\".} \\] \\[ \\text{I don't think that I need to explain the } &lt; \\text{sign.} \\] \\[ \u211d \\text{ Is pronounced \"the set of all real numbers\" and means, well, the set of all real numbers.} \\] \\[ \u2115 \\text{ Is pronounced \"the set of all natural numbers\" and means \"the set of all positive integers\", it's debatable weather or not it includes } 0 \\text{.} \\]"},{"location":"set_theory.html#definitions","title":"definitions","text":"\\[ \u00d8 \\text{ Is pronounced \"the empty set\" and means \"the set of which is empty inside\", but here that is in set theory:} \\] \\[ \u00ac \\exists (x) \\cdot x \\in \u00d8 \\] \\[ \u2286 \\text{ Is pronounced \"is a subset of\", and the meaning of that is:} \\] \\[ A \u2286 B \\iff \u2200(x) \\cdot x \\in A: x \\in B \\] \\[ pow \\text{ Is pronounced \"the power set of\" as in \"} pow(S) \\text{\", and the meaning of that is:} \\] \\[ \u2200(P) \\cdot \u2200(U) \\cdot U \u2286 S: U \\in P \u2229 \u2200(T) \\cdot T \u00ac \u2286 S: T \u00ac \\in P: P = : pow(S) \\] \\[ = \\text{ Is pronounced \"is the same as\" or \"is equal to\", and the meaning of that is:} \\] \\[ A = B \\iff A \u2286 B \u2229 B \u2286 A \\] \\[ \u00ac \\exists (S) \\cdot S \\in S \\] \\[ x \\in \\in S \\iff \\exists (U) \\cdot U \\in S \u2229 x \\in U \\] \\[ x \\in \\in S \\text{ can also be written as } \\in^2 \\] \\[ x \\in \\in \\in S \\iff \\exists (U) \\cdot U \\in S \u2229 x \\in^2 U \\] \\[ x \\in \\in \\in S \\text{ can also be written as } \\in^3 \\] \\[ x \\in \\in \\in \\in S \\iff \\exists (U) \\cdot U \\in S \u2229 x \\in^3 U \\] \\[ x \\in \\in \\in \\in S \\text{ can also be written as } \\in^4 \\] \\[ \\vdots \\] \\[ x \\in^{a + b} S \\iff \\exists (U) \\cdot U \\in^a S \u2229 x \\in^b U \\] \\[ x \\in^{a + b} S \\text{ can also be written as } x \\in^a \\in^b S \\] \\[ \\in^S \\text{ Is pronounced \"is a super element of\" (} S \\text{ for super), and the definition is:} \\] \\[ x \\in^S S \\iff x \\in S \u2228 \\exists (U) \\cdot U \\in S \u2229 x \\in^S U \\]"},{"location":"set_theory.html#_1","title":".","text":"<p>Was recursion in the rule book? I guess so.</p> <p>Recursion? Get it?</p>"},{"location":"set_theory.html#numbers","title":"numbers","text":"\\[ 0 = \u00d8 \\] \\[ \\text{succ} (n) \\text{ (Which mathematically equals } n + 1 \\text{) Is how you would usually define numbers, so I'll define numbers that way, I'll say that succ} (n) \\text{ is the set that contains all numbers } 0 \\text{-} n \\text{. But first: the union of two sets, denoted as an } \u2228 \\text{ sign.} \\] \\[ x \\in A \u2228 B \\iff x \\in A \u2228 x \\in B \\] \\[ set \\text{ Is pronounced \"the set containing\" as in \"} set(S) \\text{\", and} \\] \\[ \u2200(S) \\cdot E \\in S \u2229 \u2200(T) \\cdot T \u00ac= E: T \u00ac \\in S: S = : set(E) \\] \\[ \\text{Around } 100 \\text{ lines?? (I might add or remove another definition, but at the time of typing this, this is on } 95 \\text{ lines.)} \\] \\[ \\text{succ} (n) = : set(n) \u2228 n \\]"},{"location":"set_theory.html#back-to-definitions","title":"back to definitions","text":"\\[ \u2229 \\text{ Is pronounced \"and\" and means \"} a \u2229 b \\text{ is true if and only if statement } a \\text{ is true and } b \\text{ is true\", it can also mean the intersection of two sets, in that case, it is pronounced \"intersectioned with\", but} \\] \\[ x \\in A \u2229 B \\iff x \\in A \u2229 x \\in B \\] \\[ \\text{Here's another definition of the subset: } \u2200(A, B) \\cdot \u00ac \\exists (x) \\cdot x \\in A \u00ac \u2192 x \\in B: A \u2286 B. \\] \\[ \\text{And another one! } A \u2286 B \\iff \u00ac \\exists (x) \\cdot x \\in A \u2229 x \u00ac \\in B \\] \\[ n_1 &lt; n_2 \\iff n_1 \\in n_2 \\] \\[ \\text{WARNING! The next statement is the axiom of choice, kinda controversial.} \\] \\[ \u2200(S) \\cdot S \u00ac= \u00d8: \\exists (x) \\cdot x \\in S \\]"},{"location":"set_theory.html#group-theory","title":"group theory","text":"<p>A group (call it \\(G\\)) is a certain type of set, including an addition like thing represented with a \\(+\\) sign (this addition like thing could also be multiplication), let's start with the set of numbers \\(0\\) - \\(4\\) under modular addition. To be a group, it has to follow \\(4\\) different rules.</p> \\[ 1 \\text{, Closure} \\] \\[ \u2200(a, b) \\cdot a \\in G \u2229 b \\in G: a + b \\in G \\] <p>Because it is modular, this holds true for modular addition.</p> \\[ 2 \\text{, Associativity} \\] \\[ \u2200(a, b, c) \\cdot a \\in G \u2229 b \\in G \u2229 c \\in G: (a + b) + c = a + (b + c) \\] <p>Because addition is associative, this holds true for modular addition.</p> \\[ 3 \\text{, Identity} \\] \\[ \\exists (e) \\cdot e \\in G \u2229 \u2200(a) \\cdot a \\in G: a + e = a \u2229 \u2200(b) \\cdot b \\in G: e + b = b \\] <p>Because of \\(0\\), this holds true for modular addition.</p> \\[ 4 \\text{, Inverses} \\] \\[ \u2200(a) \\cdot a \\in G: \\exists (b) \\cdot b \\in G \u2229 a + b = e \u2229 b + a = e \\] <p>Because of negatives and them looping back around, this holds true for modular addition. (Also, modular multiplication almost works, but it fails at this step because there is no \\(\\frac{1}{0}\\).)</p> <p>Thus, the set of numbers \\(0\\) - \\(4\\) under modular addition is a group.</p>"},{"location":"set_theory.html#limits","title":"limits","text":"\\[ \\lim_{n \\to \\infty} f(n) = x \\iff \u2200(y) \\cdot y \\in \u211d \u2229 y \\ne x: \\exists (n) \\cdot n \\in \u2115 \u2229 \u2200(k) \\cdot k \\in \u2115 \u2229 k \u2265 n: |f(k) - x| &lt; |f(k) - y| \\] \\[ \\lim_{n \\to \\infty} f(n) \\to \\infty \\iff \u2200(y) \\cdot y \\in \u211d: \\exists (n) \\cdot n \\in \u2115 \u2229 \u2200(k) \\cdot k \\in \u2115 \u2229 k \u2265 n: f(k) &gt; y \\] \\[ \\lim_{n \\to \\infty} f(n) \\to -\\infty \\iff \u2200(y) \\cdot y \\in \u211d: \\exists (n) \\cdot n \\in \u2115 \u2229 \u2200(k) \\cdot k \\in \u2115 \u2229 k \u2265 n: f(k) &lt; y \\] \\[ \\lim_{n \\to \\infty} f(n) = 0 \\iff \u2200(y) \\cdot y \\in \u211d \u2229 y \\ne 0: \\exists (n) \\cdot n \\in \u2115 \u2229 \u2200(k) \\cdot k \\in \u2115 \u2229 k \u2265 n: |f(k)| &lt; |y| \\] \\[ \\lim_{n \\to \\infty} f(n) = x^{+} \\iff \u2200(y) \\cdot y \\in \u211d \u2229 y \\ne x: \\exists (n) \\cdot n \\in \u2115 \u2229 \u2200(k) \\cdot k \\in \u2115 \u2229 k \u2265 n: |f(k) - x| &lt; |f(k) - y| \u2229 f(k) &gt; x \\] \\[ \\lim_{n \\to \\infty} f(n) = x^{-} \\iff \u2200(y) \\cdot y \\in \u211d \u2229 y \\ne x: \\exists (n) \\cdot n \\in \u2115 \u2229 \u2200(k) \\cdot k \\in \u2115 \u2229 k \u2265 n: |f(k) - x| &lt; |f(k) - y| \u2229 f(k) &lt; x \\]"},{"location":"set_theory.html#set-theory-proofs","title":"set theory proofs?","text":"<p>No.</p>"},{"location":"some4.html","title":"Summer of Math Exposition 4","text":"<p>First, the things on the left of the screen are not part of the submission, because it said in the rules \"you can't make your submission ahead of time\", this page exists, I'll be summarizing the website as of today, and finally, I'll only show original math that I did.</p>"},{"location":"some4.html#calculus","title":"Calculus","text":"\\[ \\frac{f(x + dx) - f(x)}{dx} = \\frac{df}{dx} = f\\prime (x) \\]"},{"location":"some4.html#sum-rule","title":"sum rule","text":"\\[ \\vdots \\] \\[ (f + g)\\prime = f\\prime + g\\prime \\]"},{"location":"some4.html#product-rule","title":"product rule","text":"\\[ \\vdots \\] \\[ (fg)\\prime = fg\\prime + f\\prime g \\]"},{"location":"some4.html#chain-rule","title":"chain rule","text":"\\[ \\vdots \\] \\[ (f(g))\\prime = f\\prime(g) g\\prime \\]"},{"location":"some4.html#mbc-rule","title":"mbc rule","text":"\\[ \\vdots \\] \\[ (cf)\\prime = c f\\prime \\]"},{"location":"some4.html#exponent-rule","title":"exponent rule","text":"\\[ \\vdots \\] \\[ \\frac{d(a^x)}{dx} = a^x \\frac{a^{dx} - 1}{dx} \\] \\[ \\text{(lets figure this out later!)} \\]"},{"location":"some4.html#e","title":"e","text":"\\[ \\frac{de^x}{dx} = e^x \\] \\[ \\vdots \\] \\[ e = (1 + dx)^{\\frac{1}{dx}} \\] \\[ log_e (x) = : ln(x) \\]"},{"location":"some4.html#logarithmic-derivitave","title":"logarithmic derivitave","text":"\\[ \\vdots \\] \\[ (ln(f))\\prime = \\frac{f\\prime}{f} \\] \\[ f(x) = a^x \\] \\[ \\vdots \\] \\[ (a^x)\\prime = a^x ln(a) \\]"},{"location":"some4.html#power-rule","title":"power rule","text":"\\[ f(x) = x^n \\] \\[ \\vdots \\] \\[ (x^n)\\prime = n x^{n - 1} \\]"},{"location":"some4.html#polynomial","title":"Polynomial","text":"<p>Skipping this one, it's just Mathologer</p>"},{"location":"some4.html#calculus-ii","title":"Calculus II","text":""},{"location":"some4.html#more-about-e","title":"More about \\(e\\)","text":"\\[ e = \\lim_{\\Delta x \\to 0} (1 + \\Delta x)^{\\frac{1}{\\Delta x}} = \\lim_{N \\to \\infty} (1 + \\frac{1}{N})^N \\] \\[ \\vdots \\] \\[ e^x = (1 + x dx)^{\\frac{1}{dx}} \\] <p>you will see why this is usefull here.</p>"},{"location":"some4.html#more-about-e-to-the-x","title":"More about \\(e\\) to the \\(x\\)","text":"\\[ \\sum\\limits_{n=0}^{\\infty} C_n x^n = e^x \\] \\[ \\vdots \\] \\[ \\sum\\limits_{n=0}^{\\infty} C_n n x^{n - 1} = \\sum\\limits_{n=0}^{\\infty} C_n x^n \\] \\[ \\vdots \\] \\[ C_n = \\frac{1}{n!} \\] \\[ e^x = \\sum\\limits_{n=0}^{\\infty}\\frac{x^n}{n!} \\] \\[ e = \\sum\\limits_{n=0}^{\\infty}\\frac{1}{n!} \\]"},{"location":"some4.html#quoteint-rule","title":"quoteint rule","text":"\\[ \\vdots \\] \\[ (\\frac{f}{g})\\prime = \\frac{f\\prime g - f g\\prime}{g^2} \\]"},{"location":"some4.html#lhopitals-rule","title":"L'Hopital's rule","text":"\\[ f(c) = 0 \\] \\[ g(c) = 0 \\] \\[ \\lim_{x \\to c} \\frac{f(x)}{g(x)} = ? \\] \\[ \\vdots \\] \\[ \\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\lim_{x \\to c} \\frac{\\frac{d}{dx} f(x)}{\\frac{d}{dx} g(x)} \\]"},{"location":"some4.html#complex-numbers","title":"Complex numbers","text":"\\[ i = \\sqrt{-1} \\] \\[ a + bi = c + di \\iff c = a, d = b \\] \\[ (a + bi)(c + di) = ac + adi + bci + bdii = ac + adi + bci - bd = (ac - bd) + (ad + bc)i \\] \\[ (a + bi)^2 = (aa - bb) + (ab + ab)i = (a^2 - b^2) + 2abi \\] \\[ (a + bi)r = ar + bri \\] \\[ \\text{ccong} (a + bi) = a - bi \\] <p>the vertical line is the definition of \\(sin(\\theta)\\)</p> <p>the horizontal line is the definition of \\(cos(\\theta)\\)</p> <p>\\(\\alpha\\) is the angle from the right of the \\(x\\) axis to the point.</p> <p>I will define \\(f(x)\\) as a function that takes in a real number and outputs a complex number that is \\(x\\) radians around the unit circle. Because of how circles work, \\(f(x) = cos(x) + isin(x)\\). Fun fact! \\(z\\) equals \\(rf(\\theta)\\) for any complex \\(z\\). Eccept pepole don't use \\(f(\\theta)\\) because there are other complex functions \\(f(z)\\), and \\(rcos(x) + irsin(x)\\) dosen't reach the high bar for perfection set by mathematicians.</p> <p>So, now the question is: solve for \\(f(x)\\).</p> <p>I'll start by multiplying \\(f(x) \\cdot f(y)\\)</p> \\[ \\vdots \\] \\[ \\vdots \\] \\[ f(x)f(y) = f(x + y) \\] <p>hmmm \\(f(x)f(y) = f(x + y)\\) sounds faniliar... Oh right! This is an exponential, but what's the base?</p> <p>while, a common proof that I once used that if the derivitave of \\(g(x)\\) is \\(g(x)\\), then \\(g(x) = ce^x\\), it has a method of:</p> \\[ \\vdots \\] \\[ g(x) = g(0)(1 + dx)^{\\frac{x}{dx}} \\] <p>and then, using facts from Calculus part 2, \\(g(x) = g(0)e^x = ce^x\\).</p> <p>So I will use a proof like that, the proof will go like this:</p>"},{"location":"some4.html#proof","title":"proof","text":"\\[ f(x + dx) = f(x)f(dx) \\] \\[ \\vdots \\] \\[ f(dx) = (cos(0) + dx \\text{ } cos'(0)) + (sin(0) + dx \\text{ } sin'(0))i \\] <p>\\(cos(x)\\) reaches a peak at \\(0\\), so \\(cos'(0) = 0\\)</p> <p>while as you zoom into the right of the unit circle, the hight is the distance travled and the sine of a small angle is that angle, so \\(sin'(0) = 1\\)</p> \\[ \\vdots \\] \\[ f(x + ndx) = f(x)(1 + dxi)^n \\] <p>and if you saw Calculus part 2, you know that...</p> \\[ f(0 + \\frac{x}{dx}dx) = f(x) = f(0)((1 + dxi)^{\\frac{1}{dx}})^x = f(0)(e^i)^x \\] <p>so, surprisingly, the base of the exponential is \\(e^i\\), eccept many of you would of guessed that because of eueler's identity, which I was trying to derive, anyways there is one more step (more like three but you get the point)</p> \\[ f(0) = 1 \\] \\[ f(x) = e^{ix} \\] <p>thus:</p> \\[ cos(x) + isin(x) = e^{ix} \\] <p>proof complete!</p>"},{"location":"some4.html#the-holy-grail-of-complex-numbers-i-forgot-to-do-this-3-months-ago","title":"the Holy Grail of complex numbers (I forgot to do this \\(3\\) months ago.)","text":"\\[ cos(\\pi) + isin(\\pi) = e^{i \\pi} \\] \\[ e^{i \\pi} = -1 + i0 \\] \\[ e^{i \\pi} = -1 + i0 = -1 \\] \\[ e^{i \\pi} + 1 = 0 \\]"},{"location":"some4.html#jacobian-matrices","title":"Jacobian matrices","text":"<p>Learn more here, and here.</p> \\[ \\begin{bmatrix}  a \\\\   b\\end{bmatrix} = a + bi \\] \\[ \\hat{I} = \\begin{bmatrix}  1 \\\\   0\\end{bmatrix} = 1 \\] \\[ \\hat{j} = \\begin{bmatrix}  0 \\\\   1\\end{bmatrix} = i \\] \\[ a + bi = \\begin{bmatrix}  ? \\quad ? \\\\ ? \\quad ?\\end{bmatrix} \\] \\[ \\vdots \\] \\[ a + bi = \\begin{bmatrix} a &amp; -b \\\\ b &amp; a\\end{bmatrix} \\]"},{"location":"some4.html#complex-functions","title":"complex functions","text":"\\[ z = x + yi =\\begin{bmatrix}  x \\\\ y\\end{bmatrix} \\] \\[ f(z) = u + vi = \\begin{bmatrix}  u \\\\ v\\end{bmatrix} \\] \\[ dz = dx + dyi = \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ df = du + dvi = \\begin{bmatrix}  du \\\\ dv\\end{bmatrix} \\] \\[ \\vdots \\] \\[ \\begin{bmatrix}  du \\\\ dv\\end{bmatrix} = f\\prime (z) \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ \\text{And remember that!} \\] \\[ df = f(z + dz) - f(z) \\] \\[ \\frac{\\partial}{\\partial x} f(x, t) = : \\frac{f(x + dx, t) - f(x, t)}{dx} \\] \\[ \\partial f(x, t)_x = : f(x + dx, t) - f(x, t) \\] \\[ f(z + dx \\hat{I}) = f(z) + \\begin{bmatrix}  \\partial u_x \\\\ \\partial v_x\\end{bmatrix} \\] \\[ f(z + dy \\hat{j}) = f(z) + \\begin{bmatrix}  \\partial u_y \\\\ \\partial v_y\\end{bmatrix} \\] \\[ f(z + dx + dyi) = f(z + dz) = f(z) + \\begin{bmatrix}  \\partial u_x \\\\ \\partial v_x\\end{bmatrix} + \\begin{bmatrix}  \\partial u_y \\\\ \\partial v_y\\end{bmatrix} \\] \\[ f(z + dz) - f(z) = df = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} dx + \\frac{\\partial u}{\\partial y} dy  \\\\ \\frac{\\partial v}{\\partial x} dx + \\frac{\\partial v}{\\partial y} dy\\end{bmatrix} = \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ \\begin{bmatrix}  du \\\\ dv\\end{bmatrix} = \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\begin{bmatrix}  dx \\\\ dy\\end{bmatrix} \\] \\[ f\\prime (z) = \\begin{bmatrix}  \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\]"},{"location":"some4.html#cauchy-reimann-equations","title":"cauchy-reimann equations","text":"<p>the equation or jacobian matrix is a matrix, but as you know, evry complex number has a corrasponding matrix, but not every matrix has a corrasponding complex number, so to find out if the jacobian matrix is a complex number or just a matrix, or said another way, if complex function \\(f(z)\\) has a derivitave, we need the cauchy-reimann equations, lets go derive them!</p> <p>so, if the jacobian matrix is a complex number \\(a + bi\\) (which I have been saving for something like this), than the corrasponding matrix is:</p> \\[ \\begin{bmatrix} a &amp; -b \\\\ b &amp; a\\end{bmatrix} \\] <p>so, that means that</p> \\[ \\begin{bmatrix} \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} = \\begin{bmatrix} a &amp; -b \\\\ b &amp; a\\end{bmatrix} \\] <p>well, what makes a matrix of that form?</p> <p>for one, the top left equals the bottom right equals the real part</p> <p>and for another, the top right equals the negatave of the bottom left equals the negatave of the imaginary part</p>"},{"location":"some4.html#in-conclusion","title":"in conclusion...","text":"\\[ f\\prime (z) = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} \\quad \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} \\quad \\frac{\\partial v}{\\partial y}\\end{bmatrix} \\] <p>and to test if this is a matrix or a complex number</p> \\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y} \\] \\[ -\\frac{\\partial u}{\\partial y} = \\frac{\\partial v}{\\partial x} \\]"},{"location":"some4.html#examples","title":"examples","text":"\\[ f(z) = z^2 \\] \\[ f(x + yi) = (x^2 - y^2) + (2xy)i \\] \\[ u = x^2 - y^2 \\] \\[ \\vdots \\] \\[ 2x = 2x \\] \\[ 2y = 2y \\] \\[ f\\prime (z) = 2x + 2yi = 2z \\] <p>another one!</p> \\[ f(z) = \\text{ccong} (z) \\] \\[ f(x + yi) = x - yi \\] \\[ \\vdots \\] \\[ 1 \\ne -1 \\] \\[ 0 = 0 \\] \\[ f\\prime (z) = \\begin{bmatrix}  1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\] <p>in conclution, \\(z^2\\) has a derivitave, and \\(\\text{ccong} (z)\\) does not.</p>"},{"location":"some4.html#gamma","title":"Gamma","text":"<p>Skipping this one, it's just BriTheMathGuy</p>"},{"location":"streak.html","title":"NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO","text":"<p>let me explain, you know how I work on my website every day? I skipped yesterday. Other than that, I worked on the website every day for around \\(6\\) months. Other than that, I'll use the \"last time you hit commit\" button to tell how long it's been.</p> <p>It happened again, but this time, it was because I thought that I pushed the \"commit button\", when I did not. To be fair,  I was on vacation, but at least, now I have an excuse to fix the typo where I spelled \"every\" as \"everey\".</p> <p>It happened again.</p> <p>It happened again on Oct \\(2\\)nd, but it's Oct \\(5\\) as I am writing this.</p> <p>It happened again.</p> <p>It happened again.</p> <p>I forgot to hit the button yesterday.</p> <p>It happened again.</p> <p>I forgot to hit the button yesterday.</p> <p>It happened again.</p> <p>It happened again.</p> <p>It happened again.</p> <p>It happened again.</p> <p>It happened again.</p> <p>It happened twice in a row.</p> <p>Scratch that, thrice.</p>"},{"location":"the_strand_puzzle.html","title":"The strand puzzle","text":""},{"location":"the_strand_puzzle.html#house-numbers","title":"house numbers","text":"<p>First things first, I have to copy the strand puzzle word for word, having the same punctuation, same capitalization, and same timing for pressing enter, and here it is</p> <p>\"There is a man who lives on a long street, numbered on</p> <p>his side one, two, three, and so on, and that all the</p> <p>numbers on one side of him added up exactly the same</p> <p>as all the numbers on the other side of him.</p> <p>.</p> <p>He said he knew there was more than</p> <p>fifty houses on that side of the street, but not so many</p> <p>as five hundred.\"</p> <p>Just to clarify, there are no houses on the other side of the street, and the house numbers don't skip any. To solve the puzzle, you find the number of his house, and the total number of houses on the street / number of the last house on the street, and that one is between \\(50\\) and \\(500\\). For future reference, I will call those unknowns \\(x\\) and \\(y\\) respectively. When someone gave ramanujan (I'm a big fan of him by the way), he (supposedly) solved it in under \\(10\\) seconds.</p>"},{"location":"the_strand_puzzle.html#calculation","title":"calculation","text":"<p>also, because it is against MY rules to use something that I haven't derived (which is why the polynomial page has more than \\(10\\) lines), and why most of the contents of this website are derivations), first I will derive the formula for the sums of integers using discrete calculus</p>"},{"location":"the_strand_puzzle.html#the-formula-for-the-sums-of-integers-i-couldnt-find-a-better-name","title":"the formula for the sums of integers (I couldn't find a better name)","text":"\\[ \\sum\\limits_{k = 0}^{n} k = f(n) \\] \\[ \\Delta f(n) = \\sum\\limits_{k = 0}^{n + 1} (k) - \\sum\\limits_{k = 0}^{n} (k) = n + 1 + \\sum\\limits_{k = 0}^{n} (k) - \\sum\\limits_{k = 0}^{n} (k) \\] \\[ \\Delta f(n) = n + 1 \\] \\[ f(n - 1) + n = f(n) \\] \\[ f(n - 1) = f(n) - n \\] \\[ f(1) = 1 \\] \\[ f(1 - 1) = f(0) = f(1) - 1 \\] \\[ f(0) = 0 \\] \\[ \\Delta^2 g(n) = : \\Delta (\\Delta g(n)) \\] \\[ \\Delta^2 f(n) = \\Delta (n + 1) = n + 1 + 1 - n - 1 = 1 \\] \\[ \\Delta f(0) = 0 + 1 = 1 \\] \\[ \\Delta^2 f(0) = 1 \\] \\[ \\text{for the rest of this page, until I say \"ao!\", the things about } f(n) \\text{ and } g(n) \\text{ work for any } f \\text{ and } g \\] \\[ \\Delta (f(n) + g(n)) = f(n + 1) + g(n + 1) - f(n) - g(n) = f(n + 1) - f(n) + g(n + 1) - g(n) = \\Delta f(n) + \\Delta g(n) \\] \\[ f(n + 1) = f(n) + \\Delta f(n) \\] \\[ \\Delta (c f(n)) \\text{(in respect to n)} = c f(n + 1) - c f(n) = c (f(n) + \\Delta f(n)) - c f(n) = c f(n) + c \\Delta f(n) - c f(n) = c \\Delta f(n) \\]"},{"location":"the_strand_puzzle.html#putting-it-all-together","title":"putting it all together","text":"\\[ \\Delta \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} \\text{(in respect to n)} = \\begin{pmatrix} n \\\\ k - 1 \\\\ \\end{pmatrix} \\] \\[ \\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} = \\frac{n^{\\frac{k}{}}}{k!} \\] \\[ \\Delta f(n) = 5 \\begin{pmatrix} n \\\\ 0 \\\\ \\end{pmatrix} + 1 \\begin{pmatrix} n \\\\ 1 \\\\ \\end{pmatrix} + 4 \\begin{pmatrix} n \\\\ 2 \\\\ \\end{pmatrix} + 2 \\begin{pmatrix} n \\\\ 3 \\\\ \\end{pmatrix} \\] \\[ f(n) = f(0) + 5 \\begin{pmatrix} n \\\\ 1 \\\\ \\end{pmatrix} + 1 \\begin{pmatrix} n \\\\ 2 \\\\ \\end{pmatrix} + 4 \\begin{pmatrix} n \\\\ 3 \\\\ \\end{pmatrix} + 2 \\begin{pmatrix} n \\\\ 4 \\\\ \\end{pmatrix} = f(0) \\begin{pmatrix} n \\\\ 0 \\\\ \\end{pmatrix} + 5 \\begin{pmatrix} n \\\\ 1 \\\\ \\end{pmatrix} + 1 \\begin{pmatrix} n \\\\ 2 \\\\ \\end{pmatrix} + 4 \\begin{pmatrix} n \\\\ 3 \\\\ \\end{pmatrix} + 2 \\begin{pmatrix} n \\\\ 4 \\\\ \\end{pmatrix} \\] \\[ \\text{ao!} \\] \\[ \\Delta^2 f(n) = \\begin{pmatrix} n \\\\ 0 \\\\ \\end{pmatrix} \\] \\[ \\Delta f(0) = 1 \\] \\[ \\Delta f(n) = \\begin{pmatrix} n \\\\ 0 \\\\ \\end{pmatrix} + \\begin{pmatrix} n \\\\ 1 \\\\ \\end{pmatrix} \\] \\[ f(0) = 0 \\] \\[ f(n) = \\begin{pmatrix} n \\\\ 1 \\\\ \\end{pmatrix} + \\begin{pmatrix} n \\\\ 2 \\\\ \\end{pmatrix} \\] \\[ \\begin{pmatrix} n \\\\ 0 \\\\ \\end{pmatrix} = 1 \\] \\[ \\begin{pmatrix} n \\\\ 1 \\\\ \\end{pmatrix} = n \\] \\[ \\begin{pmatrix} n \\\\ 2 \\\\ \\end{pmatrix} = \\frac{n (n - 1)}{2} \\] \\[ f(n) = \\frac{n (n - 1)}{2} + n = \\frac{n (n - 1)}{2} + \\frac{2n}{2} = \\frac{(n - 1) n + (2) n}{2} = \\frac{n (n - 1 + 2)}{2} \\] \\[ \\sum\\limits_{k = 0}^{n} k = \\frac{n (n + 1)}{2} \\]"},{"location":"the_strand_puzzle.html#back-to-the-strand-puzzle-or-whatever-i-was-doing-because-i-dont-remember","title":"back to the strand puzzle or whatever I was doing because I don't remember","text":"<p>Now I can find formulas for the sums of house numbers and say that they are equal. The first starts at \\(1\\) and stops at \\(x - 1\\), so the formula is:</p> \\[ \\frac{(x - 1) x}{2} \\] <p>Now, for the second formula, there are two methods. If I say \"ao!\", then forget the last \\(2\\) lines, if I say \"oa!\", then remember them. With that being said...</p> <p>one method is to pull out an \\(x\\) at each of the \\(y - x\\) steps, which amounts to an added \\(x(y - x)\\), so it is:</p> \\[ \\frac{(y - x) (y - x + 1)}{2} + x(y - x) = \\frac{1}{2}((y - x) (y - x + 1) + 2x(y - x)) = \\frac{1}{2}(y^2 - xy + y - xy + x^2 - x + 2xy - 2x^2) = \\frac{1}{2}(y^2 + y - x^2 - x) = \\frac{y (y + 1)}{2} - \\frac{x (x + 1)}{2} \\] <p>ao!</p> <p>one method is to realize that this is the difference between the first \\(y\\) houses and the first \\(x\\) houses, so it is:</p> \\[ \\frac{y (y + 1)}{2} - \\frac{x (x + 1)}{2} \\] <p>oa!</p> <p>you can probably tell that the first was made by me, but time for some algebra!</p> \\[ \\frac{(x - 1) x}{2} = \\frac{y (y + 1)}{2} - \\frac{x (x + 1)}{2} \\] \\[ (x - 1) x = y (y + 1) - x (x + 1) \\] \\[ x^2 - x = y^2 + y - x^2 - x \\] \\[ 2x^2 = y^2 + y \\] <p>Bit more!</p> \\[ 2 (4x^2) = 4y^2 + 4y \\] \\[ 2 (2x)^2 = 4y^2 + 4y + 1 - 1 = (4y^2 + 4y + 1) - 1 = (2y + 1)^2 - 1 \\] \\[ (2y + 1)^2 - 2 (2x)^2 - 1 = 0 \\] \\[ (2y + 1)^2 - 2 (2x)^2 = 1 \\] \\[ X = : 2x \\] \\[ Y = : 2y + 1 \\] \\[ Y^2 - 2X^2 = 1 \\] <p>Which you might (but probably don't) know as the...</p>"},{"location":"the_strand_puzzle.html#pell-equation","title":"pell equation","text":"\\[ Y^2 - 2X^2 \\approx 0 \\] \\[ Y^2 \\approx 2X^2 \\] \\[ \\frac{Y^2}{X^2} \\approx 2 \\] \\[ (\\frac{Y}{X})^2 \\approx 2 \\] \\[ \\frac{Y}{X} \\approx \\sqrt{2} \\] <p>But that is besides the point. The pell equation can be rearranged into \\(Y = \\sqrt{2X^2 + 1}\\), then plug in values for \\(X\\) until \\(Y\\) is an integer. Doing this, (and remembering to divide by \\(2\\) and take the floor), I'll list the solutions that we get.</p> \\[ 6 \\quad 8 \\] \\[ 35 \\quad 49 \\] <p>Just barely misses!</p> \\[ 204 \\quad 288 \\] <p>There it is!</p>"},{"location":"the_strand_puzzle.html#in-general","title":"in general","text":"<p>The real solution would be a formula for the \\(n\\)'th solution to the strand puzzle dropping the second rule, it is a bit tedious to check using this method and someone probably would have used the formula and this wouldn't work. I usually don't tell you the video that this page is built on, but this time, you would notice that it takes a completely different method. So yes, I built the rest of this page from the ground up.</p> <p>Something that is easy to notice but hard to prove is that the bigger the \\(X\\) and \\(Y\\), the better the approximation of \\(\\sqrt{2}\\), so I will conjecture the following: Any ratio \\(\\frac{Y}{X}\\) for \\(X\\) and \\(Y\\) solving the pell equation*, is closer to \\(\\sqrt{2}\\) than any other fraction with denominator less than or equal to \\(X\\).</p> <p>*The numbers \\(X\\) and \\(Y\\) could solve the alternative \\(Y^2 - 2X^2 = - 1\\) that alternates with the pell equation as \\(X\\) and \\(Y\\) get bigger, so instead of the pell equation, from now on, it is more like \\(|Y^2 - 2X^2| = 1\\).</p> <p>By the way, the conjecture was an \"I'll leave this as an exercise for the viewer\", and I made up the proof as I went.</p> <p>Proof (by contradiction): Let's say that there is a fraction \\(\\frac{b}{a}\\) that is a better approximation of \\(\\sqrt{2}\\) than \\(\\frac{Y}{X}\\) with \\(a\\) strictly less than \\(X\\). \\(X\\) and \\(Y\\) solve \\(|Y^2 - 2X^2| = 1\\), and \\(X\\) is the next biggest solution than \\(a\\). Well, what is \\(|b^2 - 2a^2|\\)? By the thing that is easy to notice but hard to prove, even if \\(|b^2 - 2a^2| = 1\\), it still doesn't work, so \\(|b^2 - 2a^2| \\neq 1\\), and by the fact that \\(\\sqrt{2}\\) is irrational* (I'll prove that later), \\(|b^2 - 2a^2| \\neq 0\\), and by the fact that \\(a\\) and \\(b\\) are integers, \\(|b^2 - 2a^2| = an\\) \\(integer\\). So \\(|b^2 - 2a^2|\\) is an integer, does not equal \\(0\\), and does not equal \\(1\\). But the closer to \\(1\\), the better the approximation (unless the numbers are bigger). Thus, the proof is complete!</p> <p>If \\(a = X\\), than \\(|b^2 - 2a^2| = |b^2 - 2X^2|\\). If \\(b = Y\\), than I can ignore that case because \\(\\frac{Y}{X}\\) cannot be a better approximation of \\(\\sqrt{2}\\) than \\(\\frac{Y}{X}\\). So if \\(b &lt; Y\\), \\(b^2 &lt; Y^2\\) * , so \\(|b^2 - 2X^2|\\) also is not \\(1\\).</p> <p>( ) If \\(a\\), \\(b\\), \\(X\\), and \\(Y\\) are all negative, then \\(b^2 &gt; Y^2\\). But for one, if they are both negative, then they are not in reduced fractions. And for another, if one is negative, then the whole thing is negative, and \\(\\sqrt{2}\\) is not. Also if \\(b &gt; Y\\) (yes, it was only a constraint on \\(a\\) and not \\(b\\)), then it still wouldn't work.</p> <p>** Warning! I will swap \\(a\\) and \\(b\\) in this proof: Note that an even number is two times an integer, if the square of a number is even, than the number that is being squared is even, and a ratio of two even numbers is not in reduced form. You can derive these because I am too busy writing this proof down. Let's say that a fraction \\(\\frac{a}{b}\\) in lowest terms (that will be important in the contradiction part of proof by contradiction) that equals \\(\\sqrt{2}\\). Going back, this means that \\(a^2 = 2b^2\\). But \\(b^2\\) is an integer, so \\(a^2\\) is even, so \\(a\\) is even. if \\(a\\) is even, than let's say that \\(a = 2m\\) for some integer \\(m\\), \\(a^2 = 4m^2\\), so \\(4m^2 = 2b^2\\), so \\(2m^2 = b^2\\). But \\(m^2\\) is an integer, so \\(b^2\\) is even, so \\(b\\) is even, so \\(a\\) and \\(b\\) are both even, but if they are both even, (and if you remembered), this is a contradiction!</p> <p>Side note! With this proof, you can prove the thing that is easy to notice but hard to prove, but this would be a circular argument, because we need it to be true for it to be true. To this day, I haven't found any proof of this fact. But if I do, then you know the drill by now, I'll write the proof here and put this in quotation marks.</p> <p>Now, the proof is finally complete! Where was I again?</p>"},{"location":"the_strand_puzzle.html#time-to-keep-going","title":"Time to keep going","text":"\\[ Y^2 - 2X^2 = 1 \\] \\[ \\hat{Y} = 2X + Y \\] \\[ \\hat{X} = X + Y \\] \\[ \\hat{Y}^2 - 2 \\hat{X}^2 = (2X + Y)^2 - 2(X + Y)^2 = 4X^2 + 4XY + Y^2 - 2X^2 - 4XY - 2Y^2 = 2X^2 - Y^2 = -(Y^2 - 2X^2) \\] \\[ \\hat{Y}^2 - 2 \\hat{X}^2 = - 1 \\] \\[ Y_N = 2 \\hat{X} + \\hat{Y} \\] \\[ \\text{(N for next)} \\] \\[ X_N = \\hat{X} + \\hat{Y} \\] <p>But the calculations that I did earlier still work, so</p> \\[ Y_N^2 - 2X_N^2 = 1 \\] <p>So like I said, solutions for the pell equation and inverse pell equation alternate.</p> <p>Okay, this actually happened: I was solving the strand puzzle and got up to this point. I met someone new, and naturally, I opened with how I got here and where I was with the strand puzzle. She said \"can you prove that this is the next solution to the pell equation\", and I said \"I'm too tired for that\". My point is that you should just run with it.</p> <p>From now on, \\(X\\) will be replaced with \\(X_n\\), \\(Y\\) will be replaced with \\(Y_n\\), \\(\\hat{X}\\) will be replaced with \\(X_{n + 1}\\), \\(\\hat{Y}\\) will be replaced with \\(Y_{n + 1}\\), \\(X_N\\) will be replaced with \\(X_{n + 2}\\), and \\(Y_N\\) will be replaced with \\(Y_{n + 2}\\). this is because it will look more like a formula on \\(n\\) when you solve for \\(Y_n\\) as opposed to \\(Y\\). Also these can be solutions to the inverse pell equation. For the pell equation, I want to have a \\(2n\\), so it has to be the inverse pell equation first, an easy answer is</p> \\[ X_1 = 1 \\] \\[ Y_1 = 1 \\] <p>and the first solution to the strand puzzle is \\(1 \\quad 1\\), so \\(2 \\quad 3\\) falls out of that, you can verify this solution yourself (you might recognize \\(\\frac{17}{12}\\) from the mathologer video, it translates to \\(6 \\quad 8\\) considered as the first real solution, but we\u2019ll get there), so</p> \\[ X_2 = 2 \\] \\[ Y_2 = 3 \\] <p>and because of the recurrence...</p> \\[ X_{n + 1} = X_n + Y_n \\] \\[ Y_{n + 1} = 2X_n + Y_n \\] \\[ X_{n + 2} = X_{n + 1} + Y_{n + 1} = X_n + Y_n + 2X_n + Y_n = 3X_n + 2Y_2 = 2(X_n + Y_n) + X_n = 2X_{n + 1} + X_n \\] \\[ Y_{n + 2} = 2X_{n + 1} + Y_{n + 1} = X_n + X_n + Y_n + Y_n + 2X_n + Y_n = 4X_n + 3Y_n = 2(2X_n + Y_n) + Y_n = 2Y_{n + 1} + Y_n \\] \\[ X_n = 2X_{n - 1} + X_{n - 2} \\] \\[ Y_n = 2Y_{n - 1} + Y_{n - 2} \\]"},{"location":"the_strand_puzzle.html#the-final-dash-in-the-silaspe-marathon-yes-that-is-a-reference-to-mathologer-marathon","title":"The final dash in the silaspe marathon (yes, that is a reference to mathologer marathon)","text":"<p>For now, I will just copy some text from the fibonacci page, and manipulate it.</p> \\[ z^2 = z + 1 \\] \\[ z = \\begin{Bmatrix} 1 + \\sqrt{2} = c \\\\ 1 - \\sqrt{2} = d \\\\ \\end{Bmatrix} \\] \\[ c^2 = 2c + 1 \\] \\[ c^3 = 2c^2 + c = 5c + 2 \\] \\[ c^4 = 5c^2 + 2c = 12c + 5 \\] \\[ \\vdots \\] \\[ c^n = c_{n, n} c + c_{n, n - 1} \\] \\[ c^{n + 1} = c_{n + 1, n + 1}c + c_{n + 1, n} = c^n c = c_{n, n} c^2 + c_{n, n - 1} c = 2c_{n, n}c + c_{n, n} + c_{n, n - 1} c \\] \\[ c_{n + 1, n} = c_{n, n} \\] \\[ c_{n + 2, n} = c_{n + 1, n} = c_{n, n} \\] \\[ \\vdots \\] \\[ c_{n + k, n} = c_{n, n} \\] \\[ C_n = : c_{n, n} \\] \\[ c^n = C_n c + C_{n - 1} \\] \\[ c^{n + 1} = C_{n + 1}c + C_n = c^n c = C_n c^2 + C_{n - 1} c = 2C_n c + C_n + C_{n - 1} c \\] \\[ C_{n + 1} = C_n + C_{n - 1} \\] \\[ C_n = 2C_{n - 1} + C_{n - 2} \\] \\[ c^2 = 2c + 1 = c C_2 + C_1 \\] \\[ C_1 = 1 \\] \\[ C_2 = 2 \\] \\[ C_n = X_n \\] \\[ c^n = X_n c + X_{n - 1} \\] \\[ \\text{Yay, now I could solve for $X_n$ if it weren't for the second term, so how can I solve that?} \\] \\[ \\text{Well, this is only because $c^2 = 2c + 1$, but same goes for $d$, so...} \\] \\[ d^n = X_n d + X_{n - 1} \\] \\[ \\text{and subtracting, we get...} \\] \\[ c^n - d^n = X_n c + X_{n - 1} - X_n d - X_{n - 1} = (c - d) X_n = (1 + \\sqrt{2} - 1 + \\sqrt{2}) X_n = 2\\sqrt{2} X_n \\] \\[ X_n = \\frac{c^n - d^n}{2\\sqrt{2}} = \\frac{c^n - d^n}{c - d} \\] \\[ \\text{On a different note, rearranging the recurrence, we get} \\] \\[ X_{n - 2} = X_n - 2X_{n - 1} \\] \\[ Y_{n - 2} = Y_n - 2Y_{n - 1} \\] \\[ X_2 = 2 = 2X_1 + X_0 = 2 + X_0 \\] \\[ Y_2 = 3 = 2Y_1 + Y_0 = 2 + Y_0 \\] \\[ X_0 = 0 \\] \\[ Y_0 = 1 \\] \\[ \\text{but that is besides the point, what I want is a formula for } Y \\text{. Skipping through a lot of trial and error, you might try this:} \\] \\[ X_n = 2X_{n - 1} + X_{n - 2} \\] \\[ X_{n - 1} = 2X_{n - 2} + X_{n - 3} \\] \\[ X_n + X_{n - 1} = 2X_{n - 1} + X_{n - 2} + 2X_{n - 2} + X_{n - 3} \\] \\[ (X_{(n)} + X_{(n) - 1}) = 2(X_{(n - 1)} + X_{(n - 1) - 1}) + (X_{(n - 2)} + X_{(n - 2) - 1}) \\] \\[ X_1 + X_{1 - 1} = X_1 + X_0 = 1 + 0 \\] \\[ X_1 + X_{1 - 1} = 1 \\] \\[ X_2 + X_{2 - 1} = X_2 + X_1 = 2 + 1 \\] \\[ X_2 + X_{2 - 1} = 3 \\] \\[ \\text{Sound familiar? If not, I'll write down the formulas that we have just derived along with the formulas for } Y. \\] \\[ (X_{(n)} + X_{(n) - 1}) = 2(X_{(n - 1)} + X_{(n - 1) - 1}) + (X_{(n - 2)} + X_{(n - 2) - 1}) \\] \\[ X_1 + X_{1 - 1} = 1 \\] \\[ X_2 + X_{2 - 1} = 3 \\] \\[ \\text{Now for } Y. \\] \\[ Y_n = 2Y_{n - 1} + Y_{n - 2} \\] \\[ Y_1 = 1 \\] \\[ Y_2 = 3 \\] \\[ \\text{Now, it should click.} \\] \\[ Y_n = X_n + X_{n - 1} \\] \\[ Y_n = \\frac{c^n - d^n}{2\\sqrt{2}} + \\frac{c^{n - 1} - d^{n - 1}}{2\\sqrt{2}} = \\frac{c^n - d^n + c^{n - 1} - d^{n - 1}}{2\\sqrt{2}} = \\frac{c^n - d^n + \\frac{1}{c} c^n - \\frac{1}{d} d^n}{2\\sqrt{2}} \\] \\[ cd = (1 + \\sqrt{2})(1 - \\sqrt{2}) = 1 - \\sqrt{2} + \\sqrt{2} - 2 = -1 \\] \\[ Y_n = -\\frac{d^n - c^n + \\frac{1}{d} d^n - \\frac{1}{c} c^n}{2\\sqrt{2}} = \\frac{cd d^n - cd c^n + cd \\frac{1}{d} d^n - cd \\frac{1}{c} c^n}{2\\sqrt{2}} = \\frac{cd d^n - cd c^n + cd^n - dc^n}{2\\sqrt{2}} = \\frac{cd^n (d + 1) - dc^n (c + 1)}{2\\sqrt{2}} \\] \\[ c + 1 = 1 + \\sqrt{2} + 1 = \\sqrt{2}(1 + \\sqrt{2}) = c\\sqrt{2} \\] \\[ d + 1 = 1 - \\sqrt{2} + 1 = \\sqrt{2}(\\sqrt{2} - 1) = -d\\sqrt{2} \\] \\[ Y_n = \\frac{-cdd^n\\sqrt{2} - cdc^n\\sqrt{2}}{2\\sqrt{2}} = \\frac{-cdc^n - cdd^n}{2} \\] \\[ -cd = 1 \\] \\[ Y_n = \\frac{c^n + d^n}{2} \\]"},{"location":"the_strand_puzzle.html#the-final-step-in-the-silaspe-marathon","title":"The final step in the silaspe marathon","text":"<p>You might recognize these two formulas from the mathologer video that I showed you earlier. Around half of the page after and including the \"in general\" chapter was improv, including things like \"the proof\". The only things that weren't improv were the result for \\(X_n\\), the proof was using*, the other thing was the recurrence relation, I used a recurrence relation that went backwards, but it still counts. You can use these formulas (that I'll write down) for solutions for \\(x_n\\) and \\(y_n\\) (I bet that you forgot about them). I forgot to tell you that \\(x_n\\) and \\(y_n\\) are the \\(n\\)'th solutions to the strand puzzle respectively. Another thing to notice is that on the github repo, this is the \\(395\\)'th line, this is already the longest page so far, but from now on, if I reach \\(400\\) lines, I'll write down \"\\(400\\) lines!!\".</p> <p>*Bunnymatics, a study of the population of immortal bunnies. They can be a child, or an adult. Each day, the child bunnies turn into adult bunnies, and the adult bunnies asexually reproduce and then clone themselves (probably an adult thing). Day one, there is one adult bunny. if you give the bunnies price of \\(1\\) for child bunnies and price \\(c\\) for adult bunnies, one more day is multiplying the price by \\(c\\)</p> <p>\\(400\\) lines!!!!!!</p> \\[ X_n = \\frac{(1 + \\sqrt{2})^n - (1 - \\sqrt{2})^n}{2\\sqrt{2}} \\] \\[ Y_n = \\frac{(1 + \\sqrt{2})^n + (1 - \\sqrt{2})^n}{2} \\] \\[ X_n \\approx 2x_n \\] \\[ Y_n \\approx 2y_n + 1 \\] \\[ x_n = \\frac{X_{2n}}{2} \\] \\[ y_n = \\frac{Y_{2n}}{2} - \\frac{1}{2} \\] \\[ x_n = \\frac{(1 + \\sqrt{2})^{2n} - (1 - \\sqrt{2})^{2n}}{4\\sqrt{2}} \\] \\[ y_n = \\frac{(1 + \\sqrt{2})^n + (1 - \\sqrt{2})^n}{4} - \\frac{1}{2} \\]"},{"location":"trigonometry.html","title":"Trigonometry","text":""},{"location":"trigonometry.html#angle-addition","title":"Angle addition","text":"<p>By the way, the opposite is the sine times the hypotenuse, and the adjacent is the cosine times the hypotenuse</p> <p>In other words, \\(opp = sin \\cdot hyp\\), and \\(adj = cos \\cdot hyp\\).</p> <p>The black triangle in its bottom-left corner has angle \\(\\alpha\\) and the blue triangle in its bottom-left corner has angle \\(\\beta\\), they are both right triangles, and the length of the top of the blue triangle is 1, and of coarse the combined angle is \\(\\alpha + \\beta\\).</p> <p>The question is: what is the \\(sin(\\alpha + \\beta)\\) and \\(cos(\\alpha + \\beta)\\)?</p> <p>I'll start with the \\(sin\\), which is the length of the purple line, which is the length of the purple line below the upper-black line plus the length of the purple line above the upper-black line.</p> <p>The length below is the same as the height of the right of the black triangle with hypotenuse \\(cos(\\beta)\\) which is the \\(opp\\) so \\(sin(\\alpha)cos(\\beta)\\).</p> <p>The length above is in the top right mini blue triangle with hypotenuse \\(sin(\\beta)\\), and I'll leave this as an exercise for the viewer, but the angle in the top-left is \\(\\alpha\\) and it's the \\(adj\\) so \\(cos(\\alpha)sin(\\beta)\\).</p> <p>In total, \\(sin(\\alpha + \\beta) = cos(\\alpha)sin(\\beta) + sin(\\alpha)cos(\\beta)\\).</p> <p>Now with the \\(cos\\), which is the length of the green line, which is the length of the upper-black line minus the length of the mini blue triangle bottom.</p> <p>The first length is the same as the length of the bottom of the black triangle with hypotenuse \\(cos(\\beta)\\) which is the \\(adj\\), so \\(cos(\\alpha)cos(\\beta)\\)</p> <p>the length of second line is in the bottom of the mini blue triangle with hypotenuse \\(sin(\\beta)\\), and as you know, the angle in the top-left is \\(\\alpha\\) and it's the \\(opp\\) so \\(sin(\\alpha)sin(\\beta)\\)</p> <p>in total, \\(cos(\\alpha + \\beta) = cos(\\alpha)cos(\\beta) - sin(\\alpha)sin(\\beta)\\)</p> \\[ cos(\\alpha + \\beta) = cos(\\alpha)cos(\\beta) - sin(\\alpha)sin(\\beta) \\] \\[ sin(\\alpha + \\beta) = cos(\\alpha)sin(\\beta) + sin(\\alpha)cos(\\beta) \\]"}]}